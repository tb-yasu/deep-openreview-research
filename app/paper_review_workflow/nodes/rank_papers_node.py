"""Node for ranking evaluated papers."""

import asyncio
import re
from typing import Any

from langchain_openai import ChatOpenAI
from loguru import logger

from app.paper_review_workflow.models.state import (
    PaperReviewAgentState,
    EvaluatedPaper,
    EvaluationCriteria,
)
from app.paper_review_workflow.utils import convert_papers_to_dict_list
from app.paper_review_workflow.constants import (
    MAX_DISPLAY_PAPERS,
    PRELIMINARY_LLM_MAX_TOKENS,
    ABSTRACT_SHORT_LENGTH,
    MAX_KEYWORDS_DISPLAY,
)
from app.paper_review_workflow.config import LLMConfig


class RankPapersNode:
    """è©•ä¾¡æ¸ˆã¿è«–æ–‡ã‚’ã‚¹ã‚³ã‚¢é †ã«ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ãƒãƒ¼ãƒ‰."""
    
    def __init__(self, llm_config: LLMConfig | None = None) -> None:
        """RankPapersNodeã‚’åˆæœŸåŒ–.
        
        Args:
        ----
            llm_config: LLMè¨­å®šï¼ˆä¸¦åˆ—æ•°ãªã©ã‚’å«ã‚€ï¼‰
        """
        self.llm = None  # å¿…è¦æ™‚ã«åˆæœŸåŒ–ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
        self.llm_config = llm_config
    
    def __call__(self, state: PaperReviewAgentState) -> dict[str, Any]:
        """è«–æ–‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè¡Œ.
        
        Args:
        ----
            state: ç¾åœ¨ã®çŠ¶æ…‹
            
        Returns:
        -------
            æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã®è¾æ›¸
        """
        logger.info(f"Ranking {len(state.evaluated_papers)} evaluated papers...")
        
        # è©•ä¾¡åŸºæº–ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        criteria = state.evaluation_criteria
        filtered_papers = [
            paper for paper in state.evaluated_papers
            if self._meets_criteria(paper, criteria)
        ]
        
        logger.info(
            f"After filtering: {len(filtered_papers)}/{len(state.evaluated_papers)} papers "
            f"meet the criteria"
        )
        
        # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        ranked_papers = sorted(
            filtered_papers,
            key=lambda p: p.overall_score or 0.0,
            reverse=True,
        )
        
        # ç°¡æ˜“LLMãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if criteria.enable_preliminary_llm_filter and len(ranked_papers) > 0:
            logger.info("ğŸ” Preliminary LLM filter enabled - evaluating top candidates...")
            ranked_papers = self._apply_preliminary_llm_filter(
                ranked_papers, 
                criteria
            )
        
        # top_kãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä¸Šä½kä»¶ã®ã¿é¸æŠ
        if criteria.top_k_papers is not None:
            selected_papers = ranked_papers[:criteria.top_k_papers]
            logger.info(
                f"Selected top {criteria.top_k_papers} papers from {len(ranked_papers)} ranked papers "
                f"(actual: {len(selected_papers)})"
            )
        else:
            selected_papers = ranked_papers
            logger.info(f"All {len(ranked_papers)} papers selected (no top_k limit)")
        
        # ä¸Šä½è«–æ–‡ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        top_papers = convert_papers_to_dict_list(
            selected_papers,
            max_count=MAX_DISPLAY_PAPERS,
            include_llm_scores=False,
        )
        
        if top_papers:
            logger.success(f"Top paper: {top_papers[0]['title'][:50]} (Score: {top_papers[0]['overall_score']:.3f})")
        
        return {
            "ranked_papers": selected_papers,  # LLMè©•ä¾¡ã«æ¸¡ã™è«–æ–‡ãƒªã‚¹ãƒˆ
            "top_papers": top_papers,
        }
    
    def _meets_criteria(self, paper: EvaluatedPaper, criteria: EvaluationCriteria) -> bool:
        """è«–æ–‡ãŒè©•ä¾¡åŸºæº–ã‚’æº€ãŸã™ã‹ãƒã‚§ãƒƒã‚¯.
        
        Args:
        ----
            paper: è©•ä¾¡æ¸ˆã¿è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            åŸºæº–ã‚’æº€ãŸã™å ´åˆTrue
        """
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®æœ€å°å€¤ãƒã‚§ãƒƒã‚¯
        if paper.relevance_score is not None:
            if paper.relevance_score < criteria.min_relevance_score:
                return False
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢ã®æœ€å°å€¤ãƒã‚§ãƒƒã‚¯
        if criteria.min_rating is not None and paper.rating_avg is not None:
            if paper.rating_avg < criteria.min_rating:
                return False
        
        return True
    
    def _apply_preliminary_llm_filter(
        self, 
        ranked_papers: list[EvaluatedPaper], 
        criteria: EvaluationCriteria,
    ) -> list[EvaluatedPaper]:
        """ç°¡æ˜“LLMè©•ä¾¡ã§relevance_scoreã‚’å†è¨ˆç®—ã—ã€å†ã‚½ãƒ¼ãƒˆï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰.
        
        Args:
        ----
            ranked_papers: ã‚½ãƒ¼ãƒˆæ¸ˆã¿è«–æ–‡ãƒªã‚¹ãƒˆ
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            relevance_scoreã‚’æ›´æ–°ã—ã¦å†ã‚½ãƒ¼ãƒˆã—ãŸè«–æ–‡ãƒªã‚¹ãƒˆ
        """
        # è©•ä¾¡å¯¾è±¡æ•°ã‚’æ±ºå®š
        filter_count = min(
            criteria.preliminary_llm_filter_count,
            len(ranked_papers)
        )
        
        # ä¸¦åˆ—æ•°ã‚’æ±ºå®šï¼ˆllm_configãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ï¼‰
        max_concurrent = self.llm_config.max_concurrent if self.llm_config else 10
        
        logger.info(f"Evaluating top {filter_count} papers with LLM for better relevance scoring...")
        logger.info(f"âš¡ Parallel execution with max {max_concurrent} concurrent requests")
        
        # LLMåˆæœŸåŒ–
        if self.llm is None:
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.0,
                max_tokens=PRELIMINARY_LLM_MAX_TOKENS,
            )
        
        # ä¸Šä½Nä»¶ã‚’ä¸¦åˆ—LLMè©•ä¾¡
        target_papers = ranked_papers[:filter_count]
        updated_papers = asyncio.run(
            self._evaluate_relevance_parallel(target_papers, criteria, max_concurrent=max_concurrent)
        )
        
        # æ®‹ã‚Šã®è«–æ–‡ï¼ˆLLMè©•ä¾¡ã—ãªã„ï¼‰ã‚’è¿½åŠ 
        remaining_papers = ranked_papers[filter_count:]
        all_papers = updated_papers + remaining_papers
        
        # relevance_scoreã§å†ã‚½ãƒ¼ãƒˆï¼ˆoverall_scoreã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€overall_scoreã§ã‚½ãƒ¼ãƒˆï¼‰
        re_ranked_papers = sorted(
            all_papers,
            key=lambda p: p.overall_score or 0.0,
            reverse=True,
        )
        
        # æˆåŠŸæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢ã§ãªã„è«–æ–‡ï¼‰
        success_count = sum(1 for p in updated_papers if p.relevance_score != (ranked_papers[0].relevance_score or 0.0))
        
        logger.success(
            f"âœ“ Preliminary LLM filter completed: {success_count}/{filter_count} papers re-scored (parallel)"
        )
        
        return re_ranked_papers
    
    async def _evaluate_relevance_parallel(
        self,
        papers: list[EvaluatedPaper],
        criteria: EvaluationCriteria,
        max_concurrent: int = 10,
    ) -> list[EvaluatedPaper]:
        """è¤‡æ•°è«–æ–‡ã®é–¢é€£æ€§ã‚’ä¸¦åˆ—è©•ä¾¡.
        
        Args:
        ----
            papers: è©•ä¾¡å¯¾è±¡è«–æ–‡ãƒªã‚¹ãƒˆ
            criteria: è©•ä¾¡åŸºæº–
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
            
        Returns:
        -------
            æ›´æ–°ã•ã‚ŒãŸè«–æ–‡ãƒªã‚¹ãƒˆ
        """
        # Semaphoreã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def evaluate_with_semaphore(paper, index, total):
            async with semaphore:
                return await self._evaluate_single_relevance_async(paper, criteria, index, total)
        
        # å…¨è«–æ–‡ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        tasks = [
            evaluate_with_semaphore(paper, i + 1, len(papers))
            for i, paper in enumerate(papers)
        ]
        
        # å…¨ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ã‚¿ã‚¹ã‚¯ã¯ç¶™ç¶šï¼‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ­£å¸¸ã«å®Œäº†ã—ãŸè«–æ–‡ã®ã¿ã‚’è¿”ã™ï¼ˆExceptionã¯é™¤å¤–ï¼‰
        updated_papers = [
            result for result in results
            if not isinstance(result, Exception)
        ]
        
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸè«–æ–‡æ•°ã‚’ãƒ­ã‚°
        error_count = len(results) - len(updated_papers)
        if error_count > 0:
            logger.warning(f"âš  {error_count}/{len(results)} papers failed during relevance evaluation")
        
        return updated_papers
    
    async def _evaluate_single_relevance_async(
        self,
        paper: EvaluatedPaper,
        criteria: EvaluationCriteria,
        index: int,
        total: int,
    ) -> EvaluatedPaper:
        """å˜ä¸€è«–æ–‡ã®é–¢é€£æ€§ã‚’éåŒæœŸã§è©•ä¾¡.
        
        Args:
        ----
            paper: è©•ä¾¡å¯¾è±¡è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            index: è«–æ–‡ç•ªå·ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            total: ç·è«–æ–‡æ•°ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            
        Returns:
        -------
            æ›´æ–°ã•ã‚ŒãŸè«–æ–‡
        """
        try:
            # LLMã§é–¢é€£æ€§ã‚’è©•ä¾¡ï¼ˆéåŒæœŸï¼‰
            llm_relevance = await self._evaluate_relevance_with_llm_async(paper, criteria)
            
            # relevance_scoreã‚’æ›´æ–°
            updated_paper = paper.model_copy(deep=True)
            old_score = paper.relevance_score or 0.0
            updated_paper.relevance_score = llm_relevance
            
            # overall_scoreã‚‚æ›´æ–°ï¼ˆrelevance_weightã‚’è€ƒæ…®ï¼‰
            score_diff = llm_relevance - old_score
            updated_paper.overall_score = (paper.overall_score or 0.0) + score_diff * 0.4  # relevance_weight=0.4
            
            if index % 50 == 0:
                logger.info(f"  Progress: {index}/{total} papers evaluated")
            
            return updated_paper
            
        except Exception as e:
            logger.warning(f"Failed to LLM evaluate paper {paper.id}: {e}")
            # å¤±æ•—æ™‚ã¯å…ƒã®ã‚¹ã‚³ã‚¢ã‚’ä¿æŒ
            return paper
    
    async def _evaluate_relevance_with_llm_async(
        self,
        paper: EvaluatedPaper,
        criteria: EvaluationCriteria,
    ) -> float:
        """LLMã§è«–æ–‡ã®é–¢é€£æ€§ã‚’ç°¡æ˜“è©•ä¾¡ï¼ˆéåŒæœŸç‰ˆï¼‰.
        
        Args:
        ----
            paper: è©•ä¾¡å¯¾è±¡è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            é–¢é€£æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
        """
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’çŸ­ç¸®
        abstract_short = (
            paper.abstract[:ABSTRACT_SHORT_LENGTH] + 
            ("..." if len(paper.abstract) > ABSTRACT_SHORT_LENGTH else "")
        )
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’æ–‡å­—åˆ—åŒ–
        interests_str = ", ".join(criteria.research_interests)
        user_description = criteria.research_description or f"Keywords: {interests_str}"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = f"""
Evaluate how relevant the following paper is to the user's research interests on a scale of 0.0-1.0.

# Paper Information

**Title**: {paper.title}

**Keywords**: {', '.join(paper.keywords[:MAX_KEYWORDS_DISPLAY])}

**Abstract**:
{abstract_short}

# User's Research Interests

{user_description}

# Output Format

Output only the score in the range of 0.0-1.0 (e.g., 0.85)
"""
        
        # LLMã«éåŒæœŸã§å•ã„åˆã‚ã›
        response = await self.llm.ainvoke(prompt)
        response_text = response.content.strip()
        
        # ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
        score_match = re.search(r'(0\.\d+|1\.0|0|1)', response_text)
        if score_match:
            score = float(score_match.group(1))
            return max(0.0, min(1.0, score))
        else:
            logger.warning(f"Failed to parse relevance score from: {response_text[:100]}")
            return paper.relevance_score or 0.5
    
    def _evaluate_relevance_with_llm(
        self, 
        paper: EvaluatedPaper, 
        criteria: EvaluationCriteria,
    ) -> float:
        """LLMã§è«–æ–‡ã®é–¢é€£æ€§ã‚’ç°¡æ˜“è©•ä¾¡.
        
        Args:
        ----
            paper: è©•ä¾¡å¯¾è±¡è«–æ–‡
            criteria: è©•ä¾¡åŸºæº–
            
        Returns:
        -------
            é–¢é€£æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
        """
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’çŸ­ç¸®
        abstract_short = (
            paper.abstract[:ABSTRACT_SHORT_LENGTH] + 
            ("..." if len(paper.abstract) > ABSTRACT_SHORT_LENGTH else "")
        )
        keywords_str = ", ".join(paper.keywords[:MAX_KEYWORDS_DISPLAY])
        
        # research_description ãŒãªã„å ´åˆã¯ research_interests ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        research_interests_str = ", ".join(criteria.research_interests)
        user_interests = criteria.research_description or f"Keywords: {research_interests_str}"
        
        prompt = f"""Rate the relevance of this paper to the user's research interests.

User's Research Interests:
{user_interests}

Paper:
Title: {paper.title}
Keywords: {keywords_str}
Abstract: {abstract_short}

Rate the relevance on a scale of 0.0 to 1.0:
- 1.0: Highly relevant, directly addresses the research interests
- 0.7-0.9: Very relevant, closely related
- 0.4-0.6: Moderately relevant, some overlap
- 0.1-0.3: Slightly relevant, tangential connection
- 0.0: Not relevant

Return ONLY a single number between 0.0 and 1.0 (e.g., "0.85"). No other text.
"""
        
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content.strip()
            
            # æ•°å€¤ã‚’æŠ½å‡º
            # "0.85"ã®ã‚ˆã†ãªå½¢å¼ã€ã¾ãŸã¯"The relevance is 0.85"ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œ
            match = re.search(r'(\d+\.?\d*)', response_text)
            if match:
                score = float(match.group(1))
                # 0-1ã®ç¯„å›²ã«åˆ¶é™
                score = max(0.0, min(1.0, score))
                return score
            else:
                logger.warning(f"Could not parse LLM response: {response_text[:50]}")
                return paper.relevance_score or 0.5
                
        except Exception as e:
            logger.warning(f"LLM evaluation failed: {e}")
            return paper.relevance_score or 0.5

