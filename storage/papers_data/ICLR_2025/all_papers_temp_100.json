[
  {
    "id": "5sRnsubyAK",
    "title": "Neuroacoustic Patterns: Constant Q Cepstral Coefficients for the Classification of Neurodegenerative Disorders",
    "authors": [
      "Aastha Kachhi",
      "Shashank Ojha",
      "Megha Pandey",
      "Ajay Kumar Sharma",
      "Anurag Pandey"
    ],
    "abstract": "Early identification of neurodegenerative diseases is crucial for effective diagnosis in neurological disorders. However, the quasi-periodic nature of vocal tract sampling often results in inadequate spectral resolution in traditional spectral features, such as Mel Frequency Cepstral Coefficients (MFCC), thereby limiting their classification effectiveness. In this study, we propose the use of Constant Q Cepstral Coefficients (CQCC), which leverage geometrically spaced frequency bins to provide superior spectrotemporal resolution, particularly for capturing the fundamental frequency and its harmonics in speech signals associated with neurodegenerative disorders. Our results demonstrate that CQCC, when integrated with Random Forest and Support Vector Machine classifiers, significantly outperform MFCC, achieving absolute improvements of 5.6 % and 7.7 %, respectively. Furthermore, CQCC show enhanced performance over traditional acoustic measures, such as Jitter, Shimmer, and Teager Energy. The effectiveness of CQCC is underpinned by the form-invariance property of the Constant Q Transform (CQT), which ensures consistent feature representation across varying pitch and tonal conditions, thereby enhancing classification robustness. Furthermore, the robustness of CQCC features against MFCC features are validated using LDA plots. These findings are validated using the Italian Parkinson’s database and the Minsk2019 database of Amyotrophic Lateral Sclerosis, underscoring the potential of CQCC to advance the classification of neurodegenerative disorders.",
    "keywords": [
      "Neurodegenerative Disorder",
      "Constant Q Cepstral Coefficient",
      "Form Invariance",
      "Random Forest",
      "SVM."
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=5sRnsubyAK",
    "forum_url": "https://openreview.net/forum?id=5sRnsubyAK",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper introduces a new feature extraction method that leverages the form-invariance property of the Constant Q Transform (CQT). It is applied for the classification of neurodegenerative disorders, specifically Parkinson's Disease (PD) and Amyotrophic Lateral Sclerosis (ALS). The authors propose that CQCC, which leverages geometrically spaced frequency bins, provides superior spectrotemporal resolution compared to traditional Mel Frequency Cepstral Coefficients (MFCC). The study demonstrates that CQCC, when integrated with Random Forest and Support Vector Machine classifiers, significantly outperforms MFCC, achieving absolute improvements of 5.6% and 7.7%, respectively. The effectiveness of CQCC is validated using the Italian Parkinson’s database and the Minsk2019 database of ALS",
        "strengths": "This paper leverages the form-invariance property of the Constant Q Transform (CQT) to achieve superior spectrotemporal resolution compared to traditional Mel Frequency Cepstral Coefficients (MFCC). The authors demonstrate significant improvements in classification accuracy using Random Forest and Support Vector Machine classifiers, validated across multiple datasets. While the technical complexity may pose challenges for some readers, the research is of high quality and holds significant potential for advancing early diagnosis and treatment of neurodegenerative diseases. The paper's contributions are a valuable addition to the niche application area of disease diagnosis via audio.",
        "weaknesses": "1. Rigor in experimentation \n1a. Error Analysis/ Literature comparison \n Although the paper includes spectrographic analysis and Linear Discriminant Analysis (LDA) plots to visualize feature separability, It is necessary to Conduct a detailed literature analysis of the SoTA models used for this task such as Deep feature extractors. A useful analysis would be to Identify common patterns or features that contribute to the errors and provide insights into the specific cases where the proposed method fails. This would help in understanding the contribution of CQCC as an efficient feature extractor. \n\n1a. Cross validation/ 10 fold CV-\nThe paper lacks external validation of the proposed CQCC method. While simple accuracy score results are promising, additional validation using independent datasets not used in the training phase would provide stronger evidence of the method's effectiveness. This could involve cross-validation with other publicly available datasets or non intersecting splits from current dataset to achieve error estimates or uncertainty scores. *(Ref see Uncertainty Quantification of Deep Learning Models)\n\n\n2. Dataset Diversity:\nThe study primarily uses the Italian Parkinson’s Voice and Speech dataset and the Minsk2019 ALS database. While these datasets are well-established, the paper could benefit from including more diverse datasets to ensure the generalizability of the findings. It would strengthen the validity of the results and demonstrate the robustness of the proposed method across various contexts\n\n3 Lack of Comparison with Other Deep models/ deep feature extractors:\nThe paper compares CQCC primarily with traditional acoustic features like MFCC, Jitter, Shimmer, and Teager Energy. However, it does not provide a comparison with other advanced feature extraction methods or machine learning techniques that have go-to in most cases of such applications. Including such comparisons would provide a more comprehensive evaluation of the proposed method's performance and highlight its relative strengths and weaknesses"
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors have proposed Constant Q Cepstral Coefficients (CQCC) as a measure to identify neurodegenerative diseases  (like Parkinson’s and Amyotrophic lateral sclerosis). The proposed measure is compared against basic acoustic features Jitter Shimmer Teager Energy and MFCC using traditional machine learning classifiers like random forest and Support vector machines. The discriminator power of CQCC is demonstrated using two different datasets i.e. Italian Parkinson’s Voice and Speech dataset and Minsk2019 ALS database.",
        "strengths": "1. The paper contributes towards developing interpretable features representation for neurodegenerative diseases. \n2. A comparison with mostly commonly used features like MFCC\n3. evaluation on two different languages and diseases. \n4. achievement of significant performance over widely used feature sets by neurodegenerative research community. \n5. demonstration of improved class separation of CQCC over MFCC using LDA plots.",
        "weaknesses": "1. hyperparameter optimization is not performed.\n2. it is not clear what is the dimensionality of each feature set, consider adding a table or paragraph in the methodology section detailing the dimensionality of each feature set used.  \n3. Have you considered discussing the trade-offs between your approach and deep learning methods like wav2vec or BERT? This could help contextualize your choice of method and highlight any advantages in terms of interpretability or computational efficiency.\n4. As the research field lacks large amount of datasets. In the limitations section, could you discuss how the scarcity of large datasets in this field might impact the generalizability of the findings, and what implications this has for future research?   \n5. Could you provide more context in the methodology section about why these specific traditional feature sets were chosen for comparison? Are there particular characteristics of these features that make them relevant benchmarks for neurodegenerative disease detection?\n6. consider adding more references"
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The article investigates constant-Q cepstral coefficients (CQCC) to perform classification of neuro-degenarative disorder from speech, and compared the results with respect to standard mel-frequency cepstral coefficients (MFCCs) and other low-level acoustic features, such as jitter, shimmer, teager-energy etc. The results presented in the paper indicate sufficient performance improvement compared to the MFCC baseline. \n\nWhile this is a well motivated work that has the potential to impact detection of neuro-degenerative diseases using speech as the input modality, however it is not clear completely what the main novelty of the paper is. The authors did specify that the constant-Q cepstra is the main novelty presented in this work, however that is fairly incremental as such features have been used in speech technologies, perhaps not in the same application area as this article.",
        "strengths": "The paper focuses on speech based detection of neuro-degenerative disease, specifically Parkinson's disease and Amyotrophic lateral sclerosis (ALS). The paper is well motivated, clearly outlines the prior work that has been done and the contribution of the paper. Results presented in the article shows a strong performance demonstrated by the proposed approach as compared with MFCC-based system.",
        "weaknesses": "This is an interesting and relevant work focusing on detection/recognition of Parkinson's disease and Amyotrophic lateral sclerosis (ALS) from speech data, consisting of sustained vowels, specifically focusing on constant-Q cepstral coefficients (CQCC) as acoustic features. There are certain aspects that needs to be addressed - \n(1) Given the findings are primarily based on sustained vowels, how do the observations generalize to spontaneous speech? Is it absolutely needed to have speech containing sustained vowel to be able to detect/recognize the condition investigated in this work?\n(2) Table 2 in the dataset section, introduces three datasets: D1, D2 and D3. However it is not clear which one of these correspond to the datasets detailed in section 4.1. Also, in section 4.1, there are two datasets that are introduced: (a) Italian Parkinson’s Voice and Speech dataset, and (b) Minsk2019 ALS database. Table 2 is confusing as it introduces three datasets, and it is not clear what is the 3rd dataset, and which datasets correspond to D1, D2 and D3.\n(3) Section 4.3 introduces MFCCs as state-of-the-art: I wonder about the rationale behind stating that MFCCs are state-of-the-art. Is there any prior work that established MFCCs as the state-of-the-art feature for this specific application? \n(4) There are some typing errors that can be addressed by proof-reading the paper: \n(a) page 2, section 2, line 094: \"•Furthermore, no studies...\" >> \"• Furthermore, no studies... \"\n(b) page 2, section 2, line 097: \"this is the first study of it;s kind ... \" >> \"this is the first study of it's kind ... \"\n(c) page 5, section 4.1, line 264: \"..sustained sounds of all vowel sounds .. \" > please rephrase this line, \"sounds\" is repeated twice and it makes the sentence a bit confusing."
      },
      {
        "rating": "1",
        "confidence": "5",
        "summary": "The paper explores the discriminatory ability of Constant Q Cepstral Coefficients (CQCC) to classify neurodegenerative disorders based on the utterance of sustained vowels. The experimental setup includes samples from patients suffering from Parkinson-s Disease (PD) and Amyotrophic Lateral Sclerosis. The proposed pipeline includes using SMOTE to compensate for class-inbalance problems and two classical ML classification models: SVM and RF. The results show a comparison between CQCC against the well-known MFCC and classical acoustic parameters related to the fundamental frequency variability.",
        "strengths": "The paper presents results demonstrating that the use of CQCC enhances classification accuracy in detecting neurodegenerative disorders compared to traditional MFCC and acoustic parameters.",
        "weaknesses": "The main drawback of the paper is its novelty. Moreover, I consider it entirely out of the Conference's scope since it does not introduce any approach incorporating the idea of a \"learning representation.\" The components related to Machine Learning used in the paper are traditional ML models. Regarding its novelty, the set of features analysed in the paper was introduced back in 2017 and has been tested before in several voice/speech processing applications, so its contribution would be more focused on the academic community interested in the specific area of neurodegenerative disorders classification from speech signals. However, even considering the potential contribution in the area of neurodegenerative disorders detection, the comparison proposed in the paper is pretty limited since some previous works have shown that, in the context of PD detection, Rasta-PLP coefficients have better performance than MFCC but more importantly, that sustained vowels lack articulatory information which is critical to the PD detection. Indeed, there are not many datasets available out there, but the Italian Dataset used in the experiments has pretty low recording quality, and many works have shown that classifying PD vs. Control in that dataset is not a difficult task, so it should not be used as a benchmark."
      }
    ],
    "rating_avg": 3.75,
    "confidence_avg": 4.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "J1SGf2lyr6",
    "title": "A Feature-Aware Federated Learning Framework for Unsupervised Anomaly Detection in 5G Networks",
    "authors": [
      "Saeid Sheikhi"
    ],
    "abstract": "The expansion of 5G networks has led to remarkable data volume and complexity, introducing significant security challenges that require the implementation of robust and scalable anomaly detection mechanisms. Traditional centralized approaches pose privacy risks and scalability challenges due to the distributed nature of 5G infrastructures. Federated Learning (FL) offers a decentralized solution but often overlooks the importance of feature relevance and privacy preservation during model aggregation. This paper introduces a novel Feature-Aware Federated framework that integrates feature importance into the aggregation process while ensuring differential privacy. We employ integrated gradients to compute feature importance for each client, aggregate them globally with differential privacy noise, and use these insights to weight model parameters during aggregation. Additionally, we propose Dynamic Feature Importance Adaptation (DFIA) to update feature importance occasionally, enhancing the model's adaptability to evolving data distributions. Experimental results demonstrate that our framework outperforms traditional federated approaches like FedAvg and FedProx in unsupervised anomaly detection tasks within 5G networks, achieving higher accuracy and robustness while preserving data privacy.",
    "keywords": [
      "Federated Learning",
      "Anomaly Detection",
      "5G Networks",
      "Privacy-Preserving"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=J1SGf2lyr6",
    "forum_url": "https://openreview.net/forum?id=J1SGf2lyr6",
    "reviews": [
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper presents a Feature-Aware Federated Learning framework for anomaly detection in 5G networks, addressing challenges of data privacy and scalability. The framework incorporates feature importance into model aggregation using integrated gradients to compute feature relevance, with differential privacy to protect client data. Additionally, a Dynamic Feature Importance Adaptation (DFIA) mechanism periodically updates feature importance to adapt to changing data distributions. Experiments show that this approach outperforms traditional federated methods like FedAvg and FedProx in unsupervised anomaly detection tasks, achieving improved accuracy, robustness, and privacy preservation.",
        "strengths": "Strengths:\n+ This work offers a Feature-Aware Federated Learning framework for anomaly detection in 5G networks, addressing challenges of data privacy and scalability.",
        "weaknesses": "Weaknesses:\n- The novelty of this paper needs to be further improved.\n- The experimental results of this paper are not convincing.\n- More STOA baselines need to be included.\n- The writing quality of this paper needs another round of polishing."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "Federated learning provides a decentralised solution for anomaly detection in 5G networks, but neglects the importance of feature relevance and privacy preservation during model aggregation. This paper presents a federated learning framework for 5G networks, incorporating feature importance to improve anomaly detection. It uses integrated gradients to determine feature relevance and dynamically adapts to changing data distributions, integrating differential privacy to secure data. Experiments on real-world 5G network datasets show the advantages of the FAFL framework in terms of anomaly detection accuracy and robustness.",
        "strengths": "1、The proposed framework effectively addresses conditional joint learning methods in the context of data heterogeneity, lack of labelling, privacy issues and dynamic environment challenges faced in 5G network scenarios. \n2、The paper considers the protection of customer privacy by applying differential privacy when aggregating cross-customer feature importance to prevent leakage of sensitive information.\n3、The paper utilises a 5G testbed environment containing two 5G cores to collect datasets and conduct experiments with a certain degree of authenticity and credibility that are relevant.",
        "weaknesses": "1、The applicable scenario of the proposed method is unclear. Further clarification is necessary. \n2、The paper lacks innovation, appearing as a fusion of multiple existing technologies rather than presenting original contributions.\n3、This paper compares a limited number of methods, necessitating the inclusion of additional federated anomaly detection techniques to enhance its persuasive ability. Furthermore, while the paper discusses differential privacy, it fails to conduct privacy attack experiments to substantiate the need for this mechanism. Additionally, the paper lacks a comprehensive experimental analysis.\n4、This paper resembles a scientific report more than an academic publication. It lacks standardization in language, contains several writing errors ( line 126 ), and fails to provide a detailed description of the proposed framework.\n5、The paper does not align well with the conference theme and fails to address the domains of representation learning and deep learning."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a Feature-Aware Federated Learning (FAFL) framework, which incorporates feature importance into FL process. The framework utilizes integrated gradients to compute feature relevance and adapts dynamically to changing data distributions, aiming to handle the heterogeneity of client data.",
        "strengths": "The modeling of the 5G scenario in the paper is commendable, and the proposed method is an interesting attempt to handle heterogeneous features.",
        "weaknesses": "1. The background research in this paper is insufficient. There is already a substantial body of work focused on heterogeneous data and features in FL, but the authors only mention the classic approaches, FedAvg and FedProx.\n\n2. The experiments are lacking. The authors only use the two aforementioned methods as baselines and conduct a limited number of experiments. A significant portion of the paper is devoted to details about model architecture.\n\n3. The paper lacks motivation. Why do we need a feature weighting method instead of letting the model learn it? It would be better if the necessity of this method in certain scenarios could be clarified.\n\n4. In the introduction, it’s stated: \"this (FedAvg’s) assumption overlooks the heterogeneity inherent in client data,\" which suggests that the 5G scenario involves non-i.i.d data. However, the authors' method directly averages the 'feature importance vectors' from different clients (Eq.5). The rationale behind this step doesn’t seem intuitive.\n\n5. Eq.10 seems incorrect. Since \\theta_k is a vector representing the model parameters, this implies that w_k should also be a vector. Meanwhile, \\Sigma a_i doesn’t appear to make sense. The unclear nature of this formula makes it difficult to fully understand the proposed method."
      },
      {
        "rating": "1",
        "confidence": "5",
        "summary": "The authors propose an explainable AI-based algorithm, integrated gradients, to employ the feature importance of each client in the aggregation of the FL system, apply it to the 5G-related tasks, and process the importance estimation in each round to adopt the dynamic feature of 5G networks. Results compared with traditional aggregation algorithms, such as FedAvg and FedProx show some performance gain on one standard dataset.",
        "strengths": "1. The method of employing integrated gradients to estimate the contribution/importance of each client is interesting.",
        "weaknesses": "1. The proposed scheme seems standard and simply applied to the FL system. Details on the key steps are largely omitted. For example, it is not clear how the local client calculates the feature importance of its own data. According to Eq. 3 and my basic understanding of integrated gradients, this step needs a basis from other clients or the global model. However, this detail has not been fully explored in the provided content.\n\n2. To adopt the dynamic characters of the environment, the authors just let the importance calculation happen in every T round, which is kind of straight and trivial. In addition, the advantages of this step are not investigated in the experiments.\n\n3. Wrong use of DP. Adding random noise on the feature importance will not protect the raw data of clients, and the related experiments are also confusing as many details are not provided."
      }
    ],
    "rating_avg": 2.5,
    "confidence_avg": 4.5,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "49ti6LOUw5",
    "title": "UnoLoRA: Single Low-Rank Adaptation for Efficient Multitask Fine-tuning",
    "authors": [
      "Anirudh Lakhotia",
      "Akash Kamalesh",
      "Prerana Sanjay Kulkarni",
      "Nischal H S",
      "Gowri Srinivasa"
    ],
    "abstract": "Recent advances in Parameter-Efficient Fine-Tuning (PEFT) have shown Low- Rank Adaptation (LoRA) to be an effective implicit regularizer for large language models. Building on these findings, we propose UnoLoRA, a novel approach that leverages a single shared LoRA module for efficient multi-task learning. While existing methods typically use separate LoRA adaptations for each task, our approach demonstrates that a single shared adapter can effectively capture both task-specific and task-agnostic knowledge. We further introduce UnoLoRA*, an enhanced variant that employs a shared hypernetwork to generate task-specific embeddings, improving convergence and task adaptation. Our method significantly reduces trainable parameters to just 0.05% per task while maintaining competitive performance on the GLUE benchmark. Our analysis reveals that the A and B matrices in our shared LoRA adapter naturally develop complementary roles: A matrices capture generalizable features across tasks, while B matrices specialize in task-specific representations. Our results show that sharing a single LoRA adapter can achieve efficient multi-task learning while significantly reducing memory requirements, making it particularly valuable for resource-constrained applications.",
    "keywords": [
      "lora",
      "multi-task learning",
      "peft"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=49ti6LOUw5",
    "forum_url": "https://openreview.net/forum?id=49ti6LOUw5",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper presents UnoLoRA, an approach for parameter-efficient multitask learning in large language models (LLMs) using a single Low-Rank Adaptation (LoRA) module shared across multiple tasks. Building upon LoRA as an implicit regularizer, the authors explore its application in a multitasking context, aiming to reduce the number of trainable parameters while maintaining competitive performance. The paper introduces an architecture, UnoLoRA, which integrates a shared hypernetwork that generates task-specific scaling factors.",
        "strengths": "- The paper conducts comprehensive experiments and analysis to verify the proposed method.\n- The paper is well structured, proposing an architecture, UnoLoRA, which integrates a shared hypernetwork that generates task-specific scaling factors.",
        "weaknesses": "- The experiments are conducted on T5-series models, which are from 4 years ago. Using a more recent model doesn't necessarily mean aiming for the current SOTA (state-of-the-art), but rather that the behaviors of stronger models might differ, making experiments on T5 impractical. For instance, current models, after instruction tuning, demonstrate strong zero-shot generalization across tasks, making multi-task learning less important.\n- In the first table, the method proposed in this paper does not outperform HyperFormer++, even though they have different amounts of training parameters, the average effectiveness is also quite lacking. Therefore, the experimental results of this paper are not very convincing."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper introduces UnoLoRA, a method for parameter-efficient multitask fine-tuning of large language models (LLMs) through a shared Low-Rank Adaptation (LoRA) module. UnoLoRA leverages LoRA's implicit regularization properties to facilitate multitask learning by using a single adapter shared across all tasks, instead of separate adapters for each task. This approach drastically reduces trainable parameters to 0.05% per task while maintaining competitive performance with existing multitask methods. The model is evaluated on the GLUE benchmark and demonstrates parameter efficiency and improved generalization by capturing both shared and task-specific information. The authors further refine their method with UnoLoRA⋆, which converges faster and performs better in early training stages compared to the initial UnoLoRA.",
        "strengths": "- The authors conduct in-depth analyses of LoRA matrices in both single-task and multitask settings, highlighting distinctions in their properties (like effective rank and Frobenius norm) and the roles of A and B matrices. Visualizations like PCA further illustrate how UnoLoRA efficiently manages task-shared and task-specific information.\n- The study’s experiments on the GLUE benchmark provide extensive evidence of UnoLoRA's effectiveness and competitive performance.",
        "weaknesses": "- For the experiments on the GLUE benchmark, no repeated experiments with different random seeds were performed, and the experimental results are not completely convincing due to the randomness.\n- Only the T5-base model was used for the experiment. The effectiveness of the method was not verified on larger or smaller models, nor on decoder-only models."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents a method called UnoLoRA, a procedure for constructing\nlow-rank Transformer adapters in a multi-task setting by training a network to\napply task-specific transformations to a shared adapter. In particular, while\nstandard LoRA parameterizes weight matrices as $W + AB^\\top$ for low-rank\n$A$ and $B$, UnoLoRA parameterizes them as $W + A ~\\mathrm{diag}(H(t)) ~ B^\\top$,\nwhere $t$ is a task representation that includes both a discrete identifier\nand example data and positional embeddings, and $H$ is a hypernetwork. A similar recipe was previously\nexplored by Karimi Mahabadi et al. (2021) under the name of \"HyperFormers\"; as\nfar as I can tell, the main differences are that:\n\n- HyperFormers condition only on task IDs, while UnoLoRA conditions on example\n  input data\n\n- HyperFormers also modulate LayerNorm parameters, and not just adapters\n\n- HyperFormers use a slightly different adapter parameterization from the modern LoRA recipe, with\n  a nonlinearity in the middle",
        "strengths": "- Simple and seemingly effective way of parameterizing low-rank adapters in the multitask setting. The idea is timely---there have been a lot of improvements in LoRA and related schemes in the last couple of years, and revisiting conditional computation + adapter combinations seems like a promising direction.",
        "weaknesses": "- Comparatively minor tweak of an existing idea. This wouldn't be an issue on\n  its own, except for the fact that the various changes are not evaluated in\n  a way that enables direct comparison to HyperFormers, as described below.\n\n- Inconsistencies and missing details in the description of the method. Fig 1\n  makes reference to a \"Task-specific A\" parameter that is not mentioned\n  anywhere in the formal description of the method---is it used, and if so,\n  where? Additionally, the experiments make reference to a method called\n  UnoLoRA$^*$, which achieves slightly better performance than the base method\n  but does not appear to be described anywhere.\n\n- Major issues in evaluation. The paper's main results are summarized in Fig\n  6(a), which show that UnoLoRA and HyperFormers both pareto-dominate training\n  separate adapters for each task---UnoLoRA involves fewer parameters at the\n  same level of performance, while HyperFormers give increased accuracy but are\n  slightly less parameter-efficient than UnoLoRA. I have two concerns here.\n\n    - First, the individual differences between UnoLoRA and HyperFormers are\n      never individually evaluated, making it impossible to figure which (if any)\n      are responsible for the performance differences.\n\n    - Second, and more fundamentally---the whole point of adapter-based methods\n      is that they provide a tunable parameter (the adapter rank) that trades\n      off between accuracy and parameter count. So what we really need to see\n      is the entire accuracy / efficiency curve for both model classes, rather\n      than an arbitrary point on each. In fact, if I understand correctly,\n      even the size of the adapter is totally incomparable between the two\n      models being compared: this paper trains UnoLoRA with a rank of 8, while\n      the results copied from the HyperFormers paper appear to use a rank of 24.\n\n  Without a minimal comparison (or a complete frontier from each model), it is\n  possible that all observed differences between methods result from\n  incomparable hyperparameter choices.\n\n- Major formatting issues: nearly every citation in the paper is incorrectly formatted (using \\citet instead of \\citep). It seems likely that this paper didn't receive even a single round of proofreading, and should not have been submitted to ICLR in its current form."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This article proposes a new method called UNOLORA, which utilizes shared low-rank adaptation (LoRA) modules to achieve efficient multi-task learning for large language models, and has achieved outstanding performance on the GLUE benchmark.",
        "strengths": "- The method proposed by the authors is simple but effective.",
        "weaknesses": "- The writing and presentation is not good, for example, the caption and figure of Figure 1 seems confusing. Also the font size in the figure is too small to understand.\n\n- The training of Shared Hypernetwork will introduce additional training cost.\n\n- The method is only evaluated on one model, without scaling up the model size/architecture."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.5,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "zkNCWtw2fd",
    "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
    "authors": [
      "Adel Elmahdy",
      "Sheng-Chieh Lin",
      "Amin Ahmad"
    ],
    "abstract": "Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingual pre-trained language models have achieved remarkable success, yet they often optimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias. The approach fine-tunes multilingual language models using a mix of monolingual and cross-lingual question-answer pair batches sampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets show that the proposed method consistently achieves comparable or superior results in zero-shot retrieval across various languages and retrieval tasks compared to monolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingual training. These results demonstrate the effectiveness of the proposed approach for learning language-agnostic representations that enable strong zero-shot retrieval performance across diverse languages.",
    "keywords": [
      "Information Retrieval",
      "Multilingualism and Cross-Lingual NLP",
      "Question Answering"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=zkNCWtw2fd",
    "forum_url": "https://openreview.net/forum?id=zkNCWtw2fd",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a hybrid batch training approach for multilingual information retrieval by combining monolingual and cross-lingual training data. The core methodology relies on mixing different types of training data using probability weights α and β. While the implementation is straightforward, the novelty of the contribution is limited.",
        "strengths": "1. Addresses a relevant challenge in multilingual information retrieval.\n2. Provides comprehensive experimental validation across multiple benchmark datasets (XQuAD-R, MLQA-R, MIRACL).",
        "weaknesses": "1. The primary contribution merely combines two existing training approaches with probability weights, presenting a straightforward and obvious solution.\n2. The paper employs translated QA pairs as data augmentation, creating an unfair comparison with baseline methods that do not utilize this advantage."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper studies information retrieval tasks where monolingual, cross-lingual, and multilingual setups are examined. The paper studies different batch sampling approaches at the training time without modifying existing training loss (e.g., contrastive learning loss) or model architectures. Specifically, the paper argues that existing approaches either use (i) monolingual batching where the languages of query and documents are matched, but they can be of different languages, or (ii) cross-lingual batching where the languages of query and documents are different. Based on this, the paper proposes hybrid batching, which is the mixing of these two batching methods.\n\nExperiments are conducted on two base models (XLM-R and LaBSE) and evaluated on two tasks (XQuAD-R, MLQA-R, MIRACL). To train systems with data in various languages, the paper employs in-house machine translation to translate existing training corpora (described in Section 3.1). The experimental results show that hybrid batching, generally, outperforms monolingual-only and cross-lingual-only in a range of setups, including monolingual, cross-lingual, and multilingual.",
        "strengths": "The paper shows that two standard batching strategies are complementary for information retrieval tasks, as the combination of them shows improvements.",
        "weaknesses": "1. Limited evaluations are only QA datasets (e.g., the main text only shows XLM-R and LaBSE). Also, the main text consists of many large tables where each does not present as much information as the space it takes, e.g., the authors could summarize how many languages/scenarios the proposed method shows improvements instead of providing large tables like Table 3, Table 4, Table 5, etc.\n\n2. It is not clear if the proposed method is actually effective. In many cases, the improvements appear rather small. For example, in Table 1, on XQuAD-R for XLM-R (0.792 vs 0.798; 0.705 vs 0.700; 0.593 vs 0.593). Are they even statistically significant?\n\n3. As this paper mainly provides empirical observations, it would be stronger if the paper provides insights on which scenario (e.g., what kind of base model or dataset) where hybrid batching is expected to show significant improvements and when it does not. The current paper pretty much reports experimental findings which could limit its usefulness. Several questions remain, for example, what is the size and mixed of training data does one need to see the impact of this hybrid batching? I expect that if there is limited training data, the impact would be marginal."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a simple method called hybrid batch training, which involves translating to obtain parallel data in multiple languages, and sampling these data to construct a multilingual training dataset. The model is trained by inputting monolingual or multilingual training data with a certain probability, thereby balancing its performance in both scenarios.",
        "strengths": "1.  This paper proposes a hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias.\n2.  The hybrid batch training strategy simply modifies the training data batches without necessitating the introduction of loss functions or new architectural components.",
        "weaknesses": "1. The proposed hybrid batch training strategy only modifies the input training data, which lacks novelty.\n2. This paper lacks sufficient analysis to the field of multilingual information retrieval. It does not adequately demonstrate the shortcomings of existing work nor the importance and necessity of this study.\n3. The experiments only compare the performance of different input strategies but not various multilingual information retrieval methods."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.6666666666666665,
    "decision": "Reject",
    "meta_review": "Although reviewers emphasized the interesting hybrid batch sampling approach, then all agree on the shortcomings:\n\n- limited contributions and experimentation: the paper only combines the previous existing approaches and show improvements in some cases (not always significant) but little has been done to dive deeper to into finding an optimal recipe (e.g., per language sampling, comparing to different cross-lingual batch samplings with different up/downsampling, etc.). Also one interesting direction is how this approach should be aligned with the capabilities of the pretrained model? For example, if the pretrained model is bad at language X, is it possible to improve that with that approach during fine-tuning? Should this hybrid sampling approach focus more monolingual batches in language X for example?\n- limited scope: the scope is limited to retrieval for QA but it is important to see what is the effect on other cross-lingual benchmarks (XGLUE, Xtreme). Also it may also be interesting to do some \"scaling\" analysis on the approach to see how much of this matter at scale.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "viQ1bLqKY0",
    "title": "EXecution-Eval: Can language models execute real-world code?",
    "authors": [
      "Rob Kopel"
    ],
    "abstract": "As Large Language Models (LLMs) advance, traditional benchmarks face challenges of dataset saturation and disconnection from real-world performance, limiting our understanding of true model capabilities. We introduce EXecution-Eval (EXE), a benchmark designed to assess LLMs' ability to execute code and predict program states. EXE attempts to address key limitations in existing evaluations: difficulty scaling, task diversity, training data contamination, and cost-effective scalability.\nComprising over 30,000 tasks derived from 1,000 popular Python repositories on GitHub, EXE spans a range of context lengths and algorithmic complexities. Tasks require models to execute code, necessitating various operations including mathematical reasoning, logical inference, bit manipulation, string operations, loop execution, and maintaining multiple internal variable states during computation. Our methodology involves: (a) selecting and preprocessing GitHub repositories, (b) generating diverse inputs for functions, (c) executing code to obtain ground truth outputs, and (d) formulating tasks that require models to reason about code execution. This approach allows for continuous new task generation for as few as 1,200 tokens, significantly reducing the risk of models \"training on the test set.\"\nWe evaluate several state-of-the-art LLMs on EXE, revealing insights into their code comprehension and execution capabilities. Our results show that even the best-performing models struggle with complex, multi-step execution tasks, highlighting specific computational concepts that pose the greatest challenges for today's LLMs. Furthermore, we review EXE's potential for finding and predicting errors to aid in assessing a model's cybersecurity capabilities. We propose EXE as a sustainable and challenging testbed for evaluating frontier models, offering potential insights into their internal mechanistic advancement",
    "keywords": [
      "large language model",
      "evaluation",
      "benchmark",
      "code execution"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=viQ1bLqKY0",
    "forum_url": "https://openreview.net/forum?id=viQ1bLqKY0",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces EXE, a new benchmark designed to evaluate language models (LLMs) on their ability to execute Python code sourced from real-world applications. This benchmark aims to address several limitations of existing evaluations, particularly the issues of scalability, task diversity, training data contamination, and benchmarking costs.  \n\nThe benchmark comprises over 30,000 tasks drawn from 1,000 popular GitHub repositories, spanning different complexities and computational operations like logical inference, mathematical reasoning, and state management. \n\nTo construct this benchmark, the authors first select the top 1,000 most popular pypi packages and collate the corresponding github repos, after that, the authores perform a static ast analysis to filter to functions with LLM generatable argument and return type annotations. Finally, the authors apply LLM to generate test cases.\n\nThe evaluation with GPT-4 model demonstrate the limitation of existing code models.",
        "strengths": "### 1. This paper is well-written and easy to follow.\n\n### 2. Benchmarking code LLM is an important problem.\n\n### 3.  The findings are interesting.",
        "weaknesses": "## 1. The motivation for this work is not clearly articulated. \n\nThe paper proposes benchmarking the code execution capabilities of LLMs, but it is unclear why such a capability is needed given the existing roles of compilers and interpreters. A possible motivation might be that LLMs are more lightweight and could predict execution outcomes without running the code. However, I did not see any evaluation results to support this assumption.\n\n## 2. The paper suggests that the proposed dataset can guard against data contamination [1, 2], but lacks a detailed explanation of how this is achieved. \n\nThe authors claim that the dataset is dynamically collected from GitHub, which could help mitigate contamination. However, since the benchmark is built from popular GitHub repositories that do not frequently change, the dataset may not be as dynamic as implied. Additionally, because the test inputs are generated by LLMs, it is unclear how this setup effectively prevents data contamination.\n\n## 3. Certain methodological details are missing. \n\nFirst, in \"Function Selection and Dependency Collation,\" the authors mention using static AST analysis, but it is not clear how this process is performed. Second, regarding the error metric, the authors state that they \"compare the type and message (excluding stacktrace) using a language model comparison,\" which is described too vaguely to understand how this metric is actually computed.\n\n## 4. This work lacks soundness in the following areas: \n\n(1) The authors claim the benchmark is diverse; however, there is no diversity evaluation regarding the prompts and solutions. (2) Since all test cases are generated by an LLM, there is no guarantee that the test cases are sound or appropriate for the programs. Given that some test cases result in errors during execution, this raises soundness concerns.\n\n## 5. Minor: Some figures are of low resolution and unclear.\n\n\n[1] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n\n[2] PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models"
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The authors present a new benchmark to evaluate LLMs' capability in executing real-world code. To collect a set of executable code from the real world, they built a pipeline to collect repos from GitHub to construct self-contained, deterministic code. They performed static analysis to inline the dependencies to make it self-contained, and then generated inputs using LLMs. The benchmark includes 30,000 tasks across 1,000 popular Python repos. They evaluated GPT-4o and GPT-4o mini and showed that these strong models still struggle with more complex tasks.",
        "strengths": "* The benchmark addresses the issue in the prior work, i.e. CruxEval, by collecting real-world Python functions, instead of synthetically generated ones from LLMs.\n* The benchmark includes diverse tasks and spans across 1000 repos\n* The pipeline is mostly automatic and can be updated to include newer repos to address the benchmark contamination problem\n* They provide analysis regarding the relationship between performance and line count, number of function calls, execution time, etc. to better understand what affects performance",
        "weaknesses": "* The main issue with the work is that it lacks certain insights as to how this benchmark would shed light. For example, many people use CruxEval because it correlates well with model's code generation/understanding ability. Does evaluating on this benchmark instead of CruxEval serve as a better predictor of such capability?\n* The paper evaluates on two models: GPT4o and GPT4o-mini. It would be better to also evaluate some open source models to compare against the closed API-only ones, especially the StarCoder model which explicitly provides training data, so one can check whether the code in the training data affects the execution prediction or not\n* The input test cases are LLM generated. Since the work emphasizes real-world scenarios, it would be good to assess whether the LLM-generated test cases are of reasonable quality, and whether it gives an advantage to the LLM that generated the test cases in performing the task"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The authors introduce a dataset of executable python functions mined from Github. The functions chosen have certain type annotations for which test cases can be generated. The task consists in providing a code snippet as well as the input arguments into an LLM and asking the LLM to predict the output (this task has been referred to as \"program induction\" in some literature of the past, and I will refer to it as \"program understanding\")\n\nThe authors argue that this is a non-trivial benchmark and that the methodology allows the benchmark to evolve over time to include test cases or functions that are not in the training set. The authors also argue that this program understanding task could be an useful gauge of LLMs performance for coding tasks. \n\nThe authors evaluate GPT4o and GPT4o-mini on this task and provide some analysis on performance by certain proxies for ``difficulty\" such as lines of code, number of function calls, etc.",
        "strengths": "The authors deserve credit for their creative use of open-source software on Github. I believe that more executable coding benchmarks will be beneficial to the community and the authors have elements to create something very interesting! The steps taken to create the dataset seem non-trivial and the scale of the dataset is notable (>30K functions). There is preliminary evidence that the task is non-trivial, and the authors also have interesting analysis on factors that lead to more difficult program understanding on this task. I think there is potential for the authors to leverage their ingenuity in constructing this dataset for interesting applications. After skimming CruxEval which seems to propose a similar approach, my judgment is that the underlying dataset scale and difficulty of EXE is more noteworthy.",
        "weaknesses": "I think the motivation of this paper is great, and the creativity to create an executable programming benchmark is excellent! I think there is great potential in this work! I would recommend the authors try to focus on some of the following facets. \n\n1. Clarification of Test case generation methodology\n\nI may have missed it, but I tried to look for details on methodology of test case input generation. The authors are clear on the accepted types are allowed for inputs/outpus, but it is unclear how generation is done. The best I could find is: \"Based on the type definition (used for setting the function calling schema) inputs/ output pairs have been generated with the goal of maximising diversity of control flow paths within the function.\" and \"Using the argument type annotations we construct a LLM function calling schema that generates a diverse set of inputs.\" The paper requires more details and clarification on this, and depending on the methodology chosen, this could affect the merits of the approach. \n\n2. Experiments / Lack of Models Considered\n\nBecause this is a datasets and benchmarks paper and the paper's motivation emphasizes \"difficulty\" of the task, not enough is done to substantiate this claim. My expectation for a dataset/benchmark paper should be at least to evaluate numerous open source models (e.g. CodeLLama, LLama3 family, CodeT5, etc) of varying sizes in addition to commercial models. Additionally, only 2 commercial models from OpenAI are used. Performing wider evaluation will strengthen these claims and the analysis, otherwise, it is an open question on how other models would perform on this task. \n\n3. The framing of experiments + context of other works (a potential lack of novelty)\n\nThe authors do not distinguish their approach or experiments from a dataset like CodeNet. The code understanding experiments provided here can also be done with CodeNet. If the authors could show that LLM performance or the nature of LLM performance is different on their task vs. CodeNet, this would substantiate the contribution. Of course the code on github is more diverse in nature, but on the other hand, the input/output types are still limited, and a dataset like CodeNet is multi-lingual. \n\nMy recommendation would be to consider other creative uses of this dataset besides the ones you currently have. \n\n\n4. Polished Writing\n\nA paper for this venue should have a higher standard of polishing. For example, the term AST should be introduced as an Abstract Syntax Tree (AST) and referred to as AST. At one point the authors colloquially refer to evaluation benchmarks as \"evals.\" These are minor points and easy to fix, but are nevertheless are standards. \n\n5. Clarification on Licensing, Copyright, etc. \n\nI did not see clarification if the authors filtered code for permissively licensed software and if the dataset falls under acceptable use of the software."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper introduces a new benchmark EXE, focusing on testing the capability of LLMs to simulate code execution. EXE is made up of over 30000 tasks derived from 1,000 popular Python repositories on GitHub. In this scenario, LLMs need to execute code, involving operations like mathematical reasoning, logical inference, loop execution, and maintaining internal variable states. This paper provides a shallow breakdown on this. The pipeline to create EXE involves selecting and preprocessing GitHub repositories, synthesizing inputs based on function signatures, and then creating test cases (unit tests, and potentially, chaining functions tests) with the inputs. The authors claim their pipeline is automatic and capable of continuous new task generation with newest repositories to avoid test set contamination.\n\n\n————after rebuttal————\n\nThe substantial revisions made during the rebuttal period addressed some of my concerns regarding model evaluation and test set contamination. The clarifications on dependency solving are also convincing. However, as the rest of my concerns are not fully addressed and the substantial revisions are making it a new paper with too many raw details without careful organizations. I think it would be better to be revised and submitted to follow-up venues like ICML. Besides, knowing the capabilities of LLMs in code executions should be the outcome of this paper, and I think current easy subset(see weakness 8) evaluation somehow weakens it.\n\nNonetheless, I have raised my score to 5 and presentation to 2 to praise for the authors’ efforts.\n\nGood luck,\n\nReviewer o62o",
        "strengths": "1.Provide a benchmark of real-world Python code for testing LLM execution, the test cases are significantly harder and more representative for real-world usage, therefore providing a more realistic assessment of model capabilities, \n\n2.Establish an automatic pipeline to create a real-world dataset for LLM-based code execution tasks.\n\n3.Cover a wide range of programming concepts and can be potentially scaled up or updated with new tasks.\n\n4.The unit-test based evaluation is correct, the authors also mention the potential to create more complicated test cases like using chaining functions.",
        "weaknesses": "## Major weaknesses:\n1.Only GPT-4o and GPT-4o-mini are evaluated, contrary to the claim of evaluating **\"several state-of-the-art LLMs.\"** Additional evaluation with different LLMs are recommended, like Claude, Gemini, Deepseek, Phi, Qwen, etc.\n\n2.The claim of **\"avoiding training on the test set\"** relies heavily on the quality and effectiveness of the pipeline's ability to generate new test cases, which is not thoroughly demonstrated in the paper, no supplementary materials provided either. The Lack of supportive materials (either the benchmark itself or its creating code) to support claims about the framework's capabilities, weakens the contribution of a dataset paper.\n\n3.The handling of import dependencies and the process of inlining required elements are not clearly explained. It's technically important here. Need clarification.\n\n4.A bit limited to Python code, which may not represent the full spectrum of programming challenges across different languages. Since LLMs are pretrained on various programming languages, it's worth to know the execution capability on other programming languages.\n\n5.Poor quality of figures in the paper, with low-precision images that are difficult to see clearly, the authors should use vector figures instead of jpgs or pngs, \n\n6.The appendix uses 8 pages to show an example, which is excessive and poorly organized, besides, it's still not intuitive for understanding. This needs significant revision for clarity and conciseness.\n\n## Minor weaknesses:\n\n7.A bit limited evaluation metrics, using only Pass@1 accuracy. Considering more evaluations on Pass@k, or try some self-correction mechanism with LLM.\n\n8.Filtering on limited acceptable types and functions seems to make EXE an **easy subset of the real real-world programs**, although it is a fair design choice for a benchmark to avoid environment configuration issues. I think it's more interesting to know the capabilities and limitations of LLMs when executing harder cases, containing real-world types like numpy.array, torch.tensor for example. Can the authors add some discussions about their findings here?\n\n## Typos and Presentation Issues:\n\nLine 294: tense issues, ...**increase** task difficulty, however bit manipulation and boolean operations only **showed**...  Should use unified tense throughout a paragraph.\n\nLine 297: however for loops **on (73 Pass@1) on** average did not have a significant impact.\n\nLine 303: Incorrect spacing on the title of the rightmost subfigure.\n\nFigure 7: Examining only on LLM really executed code makes the accuracy normal now. However, it seems the results are not clearly illsutrated (only a small part of the figure is valid now, which is not clear). Consider to use some new figures.\n\nAppendix A.2: These are important part of your paper, since current version only uses 8 pages, consider to move this section to the main page and explain them with more details."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "Reject by reviewer consensus.\n\nThe paper introduces EXE, a benchmark designed to evaluate language models (LLMs) on their ability to execute Python code sourced from github and with LLM generated test cases. Reviewers generally understood this work and liked the motivation as well as how it represents real-world Python code.\n\nSeveral reviewers think that only evaluating on GPT-* models is insufficient and urged inclusion of at least some open models. Reviewers also pointed out various issues such as diversity (only Python), and whether model generated test cases are trustworthy.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "vdUYa7N8Mt",
    "title": "The Rate-Distortion-Perception Trade-Off with Algorithmic Realism",
    "authors": [
      "Yassine Hamdi",
      "Aaron B. Wagner",
      "Deniz Gunduz"
    ],
    "abstract": "Realism constraints (or constraints on perceptual quality) have received considerable recent attention within the context of lossy compression, particularly of images. Theoretical studies of lossy compression indicate that high-rate common randomness between the compressor and the decompressor is a valuable resource for achieving realism. On the other hand, the utility of significant amounts of common randomness at test time has not been noted in practice. We offer an explanation for this discrepancy by considering a realism constraint that requires satisfying a universal critic that inspects realizations of individual compressed images, or batches thereof. We characterize the optimal rate-distortion-perception trade-off under such a realism constraint, and show that it is asymptotically achievable without any common randomness, unless the batch size is impractically large.",
    "keywords": [
      "lossy compression",
      "perceptual quality",
      "rate-distortion-perception trade-off",
      "randomization",
      "universal critics"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=vdUYa7N8Mt",
    "forum_url": "https://openreview.net/forum?id=vdUYa7N8Mt",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper concerns with the rate-distortion-perception tradeoff (RDP) in the context of lossy compression and argues that previous theoretical results, which suggest that common randomness between the encoder and the decoder is crucial for good performance, do not accurately reflect how humans perceive realism. To address this, the authors reformulate the RDP with reaslim constraints by adopting the concept of universal critic that generalizes no-reference metrics and divergences and insecpt batches of samples. Under this framework, they prove that near-perfect realism is achievable without common randomness unless the batch size is impractically large and the proposed realism measure reduces to a divergence.",
        "strengths": "* The paper provides a novel perspective on the rate-distortion-perception tradeoff by adopting the concept of universal critics.\n* The paper presents rigorous theoretical analysis and proofs to support its claims.\n* The theoretical finding that near-perfect realism is achievable without common randomness has significant practical implications for lossy compression.",
        "weaknesses": "While the paper presents a novel and potentially impactful contribution, its clarity and accessibility are hindered by a dense presentation style. The heavy use of technical notation and the lack of illustrative examples make it challenging to grasp the core concepts and implications of the proposed framework.\n\nSpecifically, the paper would benefit from:\n\n* More explanatory discussions: For instance, a concise discussion following Definition 3.3 would clarify the meaning and significance of the new formulation in comparison to the original RDP framework.\n\n* Illustrative examples: Simple case studies or visual examples would help readers understand the practical implications of the theoretical results. The authors could consider drawing inspiration from the original RDP paper by Blau & Michaeli, which effectively uses examples to convey its ideas.\n\nAddressing these issues would make the paper more accessible to a wider audience and increase its impact. While the core contribution merits acceptance, I strongly encourage the authors to revise the paper with a focus on clarity and illustrative examples."
      },
      {
        "rating": "5",
        "confidence": "2",
        "summary": "The study addresses a core issue in lossy image compression: achieving high perceptual quality in the decompressed images while minimizing distortion and compression rate. A unique aspect of this paper is its focus on algorithmic realism — a concept that considers human perception and aims to create compressed images that appear realistic to a critic. This builds on prior work on the rate-distortion-perception (RDP) trade-off, but instead of relying heavily on common randomness, it introduces a framework that reduces or eliminates the need for shared randomness between encoder and decoder in practical settings.",
        "strengths": "1. Interesting Insight into Realism Constraints: By redefining perceptual realism through an algorithmic lens, the paper provides a fresh perspective on the RDP trade-off and its practical applications in lossy compression.\n2. Reduced Dependency on Common Randomness: The finding that common randomness is only needed in impractically large batches addresses a significant gap in previous theoretical predictions versus experimental observations.\n3. Good Theoretical Foundation: The study provides rigorous proof and aligns well with information theory, making it a valuable resource for researchers interested in theoretical advances in compression.",
        "weaknesses": "While the paper provides rigorous theoretical derivations and proofs, one significant limitation is the lack of practical illustrations or implementations that could help readers appreciate the impact and contributions of the proposed framework in real-world applications. The authors claim that algorithmic realism simplifies the practical attainment of the rate-distortion-perception (RDP) trade-off by reducing the dependency on common randomness between encoder and decoder. However, without practical visualizations or demonstration attempts, it becomes challenging for readers to intuitively evaluate the work’s contributions.\n\nThough I acknowledge the value of theoretical derivations, the paper appears incomplete and, consequently, less persuasive without empirical validation. I strongly recommend that the authors complement their theoretical results with practical experiments, such as specific implementations, visual examples, or a demonstration. This would significantly enhance the paper’s credibility and provide readers with a tangible understanding of the theory’s implications.\n\nTo make these points more specific, I propose the following questions:\n\n **1. Evaluation of Practical Applicability**\n\nThe paper offers extensive theoretical proofs, yet there is no concrete implementation provided to demonstrate how this framework could be integrated into real-world image compression tasks. Could the authors consider validating the proposed approach on an actual compression system to illustrate its practical efficacy?\n\n **2. Feasibility of Reducing Common Randomness.**\n\nWhile the theory is sound, it would benefit from an empirical investigation to verify that reducing common randomness does not detract from visual quality. Without experimental validation, how can readers assess the applicability of these theoretical findings to practical compression systems?\n\n   **3. Experimental Support for Theory-Practice Connection**\n\n The paper’s theoretical framework is detailed but lacks experimental applications or use cases. Could the authors consider providing experiments to demonstrate the balance of visual quality and compression rate achieved by the proposed approach?\n\n   **4. Inclusion of Visual Case Studies**\n\n Given the claims of practical feasibility, is it possible to provide specific examples of compressed and decompressed images to offer readers a more direct perception of the quality improvement achieved by the proposed approach?\n\nThese additions would substantially enhance the paper by bridging the gap between theoretical results and their practical impact, allowing readers to more fully appreciate the contributions of this work."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper propose a new rate-distortion-perception function and proves its achievabiliy and converse, in both zero-shot and asymptotical setting. The proposed RDP function replace the P from divergence to a realism measure defined by authors. The propose RDP function is achievable without common randomness.",
        "strengths": "It is great to have a RDP which is achievable without randomness. Afterall, the human eye distinguishs images in a per-image setting without randomness. The proposed RDP is better aligned to human perception in this sense. I have not went through the details of proofs due to the complex notation. However, I am in general glad to see a new RDP function with achievability & converse, zero-shot & asymptotic.",
        "weaknesses": "The reason why I am not willing to give this paper a higher rating is that the authors have not shown how the proposed RDP can guide perceptual compression / super-resolution, not even a toy example.\n\nThe RDP function in [Blau & Michaeli 2019] has many disadvantages, which this paper does not have:\n* [Blau & Michaeli 2019] does not prove the converse.\n* [Blau & Michaeli 2019] does not distinguish zero-shot and asymptotic function.\nThose issues have not been fixed until [A coding theorem for the rate-distortion-perception function].\n\nHowever, those weakness does not stop [Blau & Michaeli 2019] being popular. This is because [Blau & Michaeli 2019] has clear application in perceptual compression / super-resolution. It explains why previous work using GAN for perceptual compression; It aligns very well with the practically used \"real vs. fake\" test; It even guides later works in diffusion based image compression.\n\nICLR is a machine learning venue, not a pure information theory venue such as ISIT / TIT. It is better to have numerical examples (even toy size) and suggestions for later application works, so that the later works in ICLR can benefits from this paper more.\n\n(minor) It is better to move the converse to the main paper, as this year we have 10 page budget. It is strange to have a 8 page paper and 20 page appendix. At least for me, the converse is as important as achievability."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper proposes a new mathematical formulation for the rate-perception-distortion tradeoff. Specifically, in the previous rate-perception-distortion formulation, the perceptual quality constraint is a constraint on the statistical divergence between the distribution of the decoded images and that of the clean images. In theory, this typically leads to randomized decoders, which produce many different decoded images given an encoded one. However, in practice, high-perceptual-quality compression-decompression algorithms rarely incorporate such randomness.\nTo explain this phenomenon, the authors replace the perceptual quality constraint with a new interesting concept called the \"universal critic\", which poses a perceptual quality constraint on individual images (or on a batch of images).\nThe new rate-perception-distortion formulation leads to solutions which do not incorporate randomness. This is a sensible result given the fact that now there is no constraint on the *distribution* of the decoded images.",
        "strengths": "This paper is incredibly interesting, and written very well. The theoretical results are interesting and serve a highly important contribution to the community of information theorists.",
        "weaknesses": "1. There are no experiments, demonstrations, simulations, presented evidence, etc. This paper contains only theoretical results, which is not necessarily a bad thing, but I am not sure whether it's a fit for the ICLR community (most of which are practitioners). I would expect to see this paper in a theoretical journal.\n\n2. There is no discussion/limitation section discussing the possible future continuation of this work."
      }
    ],
    "rating_avg": 5.5,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "Reviewers find the theoretical work interesting, but they agree that the paper lacks significant experimental demonstration of their results and also missing motivation for the utility of the presented work.\nFor that, the paper is recommended for rejection at this current state, but authors are encouraged to perform the experiments and resubmit.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "PwxYoMvmvy",
    "title": "Beyond Random Masking: When Dropout meets Graph Convolutional Networks",
    "authors": [
      "Yuankai Luo",
      "Xiao-Ming Wu",
      "Hao Zhu"
    ],
    "abstract": "Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs fundamentally from standard neural networks - preventing oversmoothing rather than co-adaptation. We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a form of structural regularization not present in standard neural networks. Our analysis shows that dropout effects are inherently degree-dependent, resulting in adaptive regularization that considers the topological importance of nodes. We provide new insights into dropout's role in mitigating oversmoothing and derive novel generalization bounds that account for graph-specific dropout effects. Furthermore, we analyze the synergistic interaction between dropout and batch normalization in GCNs, uncovering a mechanism that enhances overall regularization. Our theoretical findings are validated through extensive experiments on both node-level and graph-level tasks across 14 datasets. Notably, GCN with dropout and batch normalization outperforms state-of-the-art methods on several benchmarks, demonstrating the practical impact of our theoretical insights.",
    "keywords": [
      "Graph neural networks",
      "Dropout"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=PwxYoMvmvy",
    "forum_url": "https://openreview.net/forum?id=PwxYoMvmvy",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper investigates the role of dropout in GCNs, addressing a gap in understanding how dropout interacts with graph structure in these models. The authors provide a theoretical analysis that dropout in GCNs generates dimension-specific stochastic subgraphs, which introduces a unique form of structural regularization that doesn’t appear in traditional neural networks. The study highlights that dropout’s effects vary based on node degree, leading to adaptive regularization that leverages topological node importance. The paper also discuss dropout’s capacity to reduce oversmoothing and presents generalization bounds tailored to graph-specific dropout effects. Additionally, it explores the combined effect of dropout and batch normalization in GCNs, identifying a mechanism that enhances overall regularization.",
        "strengths": "- The paper focuses on the role of dropout in GCNs, specifically analyzing its unique interactions with graph structure. This originality is meaningful to the community.\n- The work presents a well-developed theoretical framework, introducing concepts like dimension-specific stochastic subgraphs, adaptive regularization based on node degree, and graph-specific generalization bounds.\n- Including comprehensive experiments across 16 datasets for both node-level and graph-level tasks is encouraging.",
        "weaknesses": "- The authors provide generalization bounds for graph neural networks with dropout. However, further clarification is needed on how this finding offers insights into understanding and designing graph neural networks, or any specific guidance on selecting dropout rates. With this theory, is it possible to get the best dropout rate with a specific graph structure and GNN? This would help demonstrate the practical relevance of the theory. Additionally, can the experiments provide corresponding analyses regarding this theory? For example, whether the change in performance at different dropout rates is consistent with the change in generalization bounds can be analyzed from the theory.\n- The use of dropout or similar strategies designed specifically for graphs is also widely applied in GNNs, like DropNode, DropEdge, DropMeassge, etc [1, 2, 3]. The authors may need to discuss its relevance to this study, including whether the proposed theory can analyze these methods and the essential difference and connection between dropout and these methods. Compared to traditional dropout, does dropout on the graph structure more directly enhance the performance of graph neural networks?\n\n[1] Dropedge: Towards deep graph convolutional networks on node classification\n\n[2] Dropmessage: Unifying random dropping for graph neural networks\n\n[3] Graph random neural networks for semi-supervised learning on graphs"
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper focuses on the theoretical analysis of dropout in Graph Convolutional Networks (GCNs) and its impact on regularization and model performance.\n\nThis paper establishes a mathematical framework to analyze dropout's behavior in GCNs. \nIt shows the dropout in GCN is similar to adaptive regularization that considers the topological importance of nodes, and is effective in mitigating over-smoothing in GCNs. And the dropout has synergy with batch normalization in GCNs for enhanced regularization.",
        "strengths": "1. The paper provides a mathematical framework that deepens the understanding of dropout in Graph Convolutional Networks (GCNs), addressing its relation to adaptive regularization and batch normalization.\n2. The empirical analysis is extensive, including empirical observation of theorems and evaluation results on various datasets.\n3. The idea of using active path subgraphs to understand graph feature dropout is interesting.",
        "weaknesses": "1. Dropout is a general and well-known technique, to achieve performance gain via dropout, the question can be how to tune the parameter. Can the theoretical analysis of dropout in GCNs provide insights on how to select the dropout hyperparameter?\n2.  The paper primarily focuses on dropout in GCNs, but it may not sufficiently compare the method with other graph learning regularization techniques, e.g. [1], [2]\n\n[1] Tackling Over-Smoothing for General Graph Convolutional Networks. Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, Junzhou Huang.  (extension of DropEdge) Arxiv 2020\n\n[2] Rethinking Graph Regularization for Graph Neural Networks. Han Yang, Kaili Ma, James Cheng. AAAI 2021"
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper develops a comprehensive theoretical framework analyzing how dropout uniquely interacts with Graph Convolutional Networks (GCNs), revealing that it creates dimension-specific stochastic sub-graphs and provides degree-dependent adaptive regularization. The research provides new theoretical insights into dropout's role in mitigating oversmoothing and its synergistic interaction with batch normalization, deriving novel generalization bounds specific to graph structures. These theoretical findings are validated through extensive experiments across 16 datasets, demonstrating improved performance on benchmark datasets like Cora, CiteSeer, and PubMed.",
        "strengths": "The paper demonstrates rigorous theoretical analysis with a comprehensive mathematical framework for understanding dropout in GCNs, introducing well-defined concepts like dimension-specific sub-graphs and feature-topology coupling matrices.\n\nThe research reveals novel insights about unique interactions between dropout and graph structure, particularly showing how dropout creates dimension-specific stochastic sub-graphs and exhibits degree-dependent effects leading to adaptive regularization.\n\nThe analysis is thorough and multi-faceted, examining structural regularization, oversmoothing mitigation, and interaction with batch normalization, supported by extensive experiments across 16 datasets for both node-level and graph-level tasks.\n\nThe work successfully bridges theory and practice, providing actionable insights for GCN design and training while demonstrating improved performance on benchmark datasets like Cora, CiteSeer, and PubMed.",
        "weaknesses": "The experimental validation lacks detailed information about the 16 datasets used, and the comparative analysis with state-of-the-art methods could be more comprehensive. Some experimental results mentioned in figures are truncated in the provided content.\n\nThe theoretical framework makes limiting assumptions about undirected graphs, and doesn't adequately address the extension to directed graphs. The interaction between dropout and different activation functions, as well as the impact of graph density on dropout effectiveness, need more exploration.\n\nThe paper lacks clear guidelines for selecting optimal dropout rates based on graph properties, analysis of scalability to very large graphs, and discussion of computational overhead for implementing the theoretical framework."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper performs a comprehensive theoretical analysis of dropout in the case of Graph Convolution Networks (GCN) from multiple perspectives: dimension-specific graph structure modification during training, degree-dependent effect on nodes, impact on over-smoothing, and it’s combined effect with batch normalization.",
        "strengths": "- The theoretical analyses are detailed and sound. \n- Mathematics and the overall logic of the paper is easy to follow.\n- The paper attempts to better understand the internal workings of dropout in GCNs.",
        "weaknesses": "While I like the theoretical analyses presented in the paper, I think the experiments do not quite align to support the theoretical claims. Below are my concerns with the paper.\n\n- The authors referring to the dropout and batch normalization as “our approach” in lines 409-410 and 466 is misleading since the techniques have been well-established in deep learning for improving performance. The contribution of the authors lies in the detailed theoretical analysis of these techniques within the context of a GCN. While it is a valuable contribution, the techniques should not be claimed as their approaches.\n- The major concern is with the conclusions drawn from the experiments. It is already established in deep learning that dropout and batch normalization enhance performance through regularization. Therefore, only comparing the performance in Tables 1, 2, and 3 does not provide sufficient evidence that the observed improvements are specifically due to the additional effects of dropout in graph neural networks, as analyzed in the theorems. The authors need to design experiments that can directly validate their theoretical analysis.\n- Section 3.4 describes an interesting connection between dropout, the number of GCN layers, and over-smoothing. However, the authors fail to provide experimental evidence to support this relationship. Demonstrating how dropout affects over-smoothing in GCN with varying layer depths would strengthen the paper.\n- Line 472 (regarding Table 1) and line 483 (regarding Table 2) draw contrasting conclusions about the effect of dropout on Dirichlet Energy. What is the reason behind this difference in the behavior of dropout?\n- Minor: Repeated use of variable 'd' for denoting degree (line 133) and node feature dimensionality (line 141).\n\nWith a sound experimental design that can directly validate the theoretical claims, I believe the paper would be a good contribution to the GCN community."
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.0,
    "decision": "Accept (Poster)",
    "meta_review": "This paper introduces a novel theoretical analysis of the dropout in GCNs. It shows that dropout creates a degree-dependent dimension-specific stochastic sub-graph, which performs an adaptive structural regularization and prevents the over-smoothing issue. Besides, a deep understanding of the interplay between dropout and batch normalization is presented. The theoretical analysis of this paper is rigorous and the experimental evaluations are sufficient to justify the statements.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "YaRzuMaubS",
    "title": "Defining Deception in Decision Making",
    "authors": [
      "Marwa Abdulhai",
      "Micah Carroll",
      "Justin Svegliato",
      "Aryansh Shrivastava",
      "Anca Dragan",
      "Sergey Levine"
    ],
    "abstract": "With the growing capabilities of machine learning systems, particularly those that interact with humans, there is an increased risk of systems that can easily deceive and manipulate people. Preventing unintended behaviors therefore represents an important challenge for creating aligned AI systems. To approach this challenge in a principled way, we first need to define deception formally. In this work, we present a concrete definition of deception under the formalism of rational decision making in partially observed Markov decision processes. Specifically, we propose a general regret theory of deception under which the degree of deception can be quantified in terms of the actor's beliefs, actions, and utility. To evaluate our definition, we study the degree to which our definition aligns with human judgments about deception. We hope that our work will constitute a step toward both systems that aim to avoid deception, and detection mechanisms to identify deceptive agents.",
    "keywords": [
      "deception",
      "AI safety"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=YaRzuMaubS",
    "forum_url": "https://openreview.net/forum?id=YaRzuMaubS",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents a new quantitative definition of deception in agent-to-agent dialogue interactions modelled as partially observable Markov decision processes (POMDPs). One of the agents is the speaker (S), and the other agent is the listener (L). The listener L is modelled as a POMDP over a certain set of world states. The set of observations of the POMDP representing L is the same as the set of actions available to S. The speaker S is modelled as a POMDP over a state space that contains the world states plus belief states over L. Essentially, S has access to the ground truth of the world and the actions provided by L, and has a belief model over the beliefs and policies of L, while L has no access to the ground truth, and has a belief model over the ground truth guided by the actions of S. \n\nEach agent (S and L) has its own reward function.\n\nDegree of deception is defined as the difference between the expected reward of L if it listens to (and therefore updates its beliefs over) S, and the expected reward of L if it does not listen to (and therefore does not update its beliefs over) S. This way, a positive value indicates an altruistic speaker (S makes the reward of L larger if L listens to S), a negative value indicates a deceptive speaker (S makes the reward of L smaller than it would be if L did not listen to S), or neutral if they are the same. (*)\n\nThe paper states that different concepts of deceptiveness can be captured by plugging different reward functions in Eq. (1). In particular, deceptiveness can be defined as (i) S producing worse outcomes (reward for deception = reward for the task), (ii) S producing beliefs in L further from the truth (reward for deception = belief is close to reality), or (iii) a combination of both (i) and (ii). \n\nArmed with this definition, the paper presents an experimental study, where they generate scenarios with different pairs of agents, ask human subjects to rate the degree of deceptiveness and compare the results with the degree of deceptiveness given by Eq. (1), as well as the degree of deceptiveness when substituting human subjects by state of the art large language models (LLMs). The paper claims that their results support the hypothesis that their definitions of deceptiveness align with human intuition, especially when the reward combines outcome and beliefs, and that the alignment with human intuition is much better than that given by state of the art LLMs. \n\n-------\n(*) This is inverted to what is stated in the text (lines 198-209). I believe there is either a negative sign missing in Eq. (1) or a mismatch in describing the equation in lines 208-209. In any case, it is at the level of a typo, it does not significantly affect the contribution or quality of the paper.",
        "strengths": "S1. The topic of deception in autonomous systems is relevant and timely. The use of autonomous systems in day-to-day decision-making is increasing (especially with the current development of LLMs), and it is fundamental to have models of different intentional harms to increase trust in these systems by the public and accountability for potential harms caused by them. Deceptive language, especially in interactive systems, is of particular relevance.\n\nS2. The paper is written nicely, with a strong motivation, clear structure and understandable examples to guide the reader. \n\nS3. As an experimental evaluation, the paper includes a study on human subjects and addresses state of the art LLMs. The scenarios presented are a good balance of simple and realistic.\n\nS4. The paper has a fair discussion on the limitations of their approach, mentioning how there is work to be done to deploy the proposed concept in more complex and realistic scenarios.\n\nS5. The paper engages with current literature on different definitions of deception and use of LLMs in relation to deception.",
        "weaknesses": "**MAIN WEAKNESSES**\n\nW1. The formalism has no clear novelty. Defining deceptiveness as the level of regret just passes the ball of defining deceptiveness to the reward function. This is not in itself a bad decision, but it does mean that the interest does not lie so much in the definition as proposed in Eq. (1), but rather lies in the choice of reward function. The choice of reward function feels underexplored to me as part of the experimental report. \n\nW2. I think the experimental evaluation goes in the right direction, and most of the data obtained will be useful, but I find it weak as it is now. I will structure my criticism into points that I believe are misleading and points that I believe are incomplete. I also number them, to facilitate later discussion.\n\n\nW2.1. Misleading.\n\n- W2.1.1. In Table 1, the numbers presented indicate the correlation between human perceived deceptiveness and the values given by regret and by LLMs. In the table it states that these results were statistically significant, with a p-value < 0.001. I do not see in the text what statistical test this p-value refers to, I can only assume by context that the null hypothesis was \"there is no correlation between human ratings and machine ratings\". If this is the case, the result is hardly surprising (although it is useful as a means of a sanity check), and I find it misleading to accompany it to the concrete values given in this table.\n\n- W2.1.2. The nutritionist scenario is a bad choice, and I do not agree with the reason given to its lower correlation in lines 361-363. In the nutritionist scenario, human subjects are presented with facts that are controversial in the current public opinion (whether protein, restriction of carbohydrates or herbal teas boost energy). The human subject is going to come with its own beliefs to the task, and they would certainly influence their perception of deceptiveness. I think the nutritionist example is a bad choice and without having prior information on the beliefs of the human subjects with respect to protein, carbohydrates and teas, no reliable conclusions can be extracted from the data of that experiment.\n\n- W2.1.3. Lines 355-356 state that \"We largely find that a combined regret formulation better captures human intuitive notions of deception across all three scenarios, confirming our hypothesis from Section 2.3 that both belief and task reward contribute to improving the correlation with human judgment\". While it is true that the \"Combined\" column is larger than the \"Belief\" column, it is not by much. It would be helpful to accompany this statement with a statistical test and its corresponding p-value. \n\n\nW2.2. Incomplete.\n\n- W2.2.1. Lines 368-373 include information about multi-step conversations. It would be useful to have a table similar to Table 1 summarizing the information, maybe in an appendix if it does not fit in the main text. Also, a conclusion is given that the correlation between humans and regret is higher for multi-step conversations. It would be helpful to accompany this statement with its corresponding statistical test and p-value. It would also be interesting to know how much (if any) the LLMs improve in multi-step conversation. \n\n- W2.2.2. One of the conclusions derived from the experimental evaluation is that the presented regret-based formalism aligns better with human intuition than the estimation given by LLMs. Again, it would be interesting to know the statistical significance of this statement, but more importantly, it would be interesting to understand why. Given the 1-5 scale, it is possible that the LLM produces a less extreme (but still on the correct side) value than the human (for example, the LLM would choose 2 instead of 1, or 4 instead of 5). This would produce a smaller correlation, while indicating that the LLMs are still aligned with human intuition. Another possibility is that the lower correlation comes from the LLMs contradicting human intuition (i.e. the LLM choosing a value >3, when the human chooses a value <3, and vice versa). Of course, in reality, it is probably a combination of both phenomena. It would be however very informative to include some information about this, maybe as part of a qualitative analysis (currently Sec. F in the appendix).\n\n- W2.2.3. One of the questions in the study is which reward function produces deceptiveness degrees that best align with human intuition, and the winner is the \"combined one\". However, as far as I can tell, it is not stated in the paper what is the weight used in combining these values. As an extra step, it would also be interesting to see how the correlation varies for different weights, and whether an \"optimal\" weight arises from the experiments.\n\n\nI hope the authors do not get discouraged by this review. I really like the study presented in the paper and think it has much value.  With some additional depth and clarity in the experimental evaluation, this paper could be a strong contribution for a top-tier venue like ICLR in the future.\n\n\n**OTHER (MINOR) REMARKS**\n\nThese are smaller remarks, mostly editing issues. I hope the feedback serves to polish the paper.\n\nR1. The bibliography needs to be polished. Here is a list of issues I found. \n\n- R1.1. There are repeated items, for example [He et al. 2018], [Lewis et al. 2017], [Wang et al. 2020].\n- R1.2. There are many items that lack a journal, conference, arxiv id or similar. I know in the era of the internet one can find papers just from the title, but let's keep good practices. For example [Abdulhai et al. 2023], [Amodei et al. 2016], [Bai et al. 2022], [Sung et al. 2023], [Pan et al. 2023], [Park et al. 2023], [Touvron et al. 2023], [Ward et al. 2023], [Wei et al. 2023].\n- R1.3. There are typos: <i>diplomacy</i> in [Bakhtin et al. 2022], missing capitalization in [Greene 2007], [Wang et al. 2021] .\n- R1.4. This may be a quirk of mine, but I find it misleading to cite papers that have been presented at major AI/ML venues giving only their arxiv id, while other papers are cited in the proceedings of some conference or journal. For example [Aakanksha et al. 2022] appeared in JAIR 2024, [He et al. 2018] in EMNLP 20218, [Lin et al. 2021] in ACL 2022, or [Pan et al. 2023] in ICML 2023. I would appreciate at least consistency.\n- R1.5. I don't think [Brown et al. 2020] is an appropriate citation for GPT3.5-Turbo. See the discussion here for example: https://community.openai.com/t/how-to-cite-text-davinci-003-in-academic-paper/369821.\n\n\nR2. Apart from the missing details mentioned in W2, there are some missing definitions. When defining POMDP (line 104), the set of beliefs (B) is mentioned, without stating what it is. I assume that a belief is a probability distribution over states (as usual), but this should be stated for the sake of completeness. On a similar note, it is not clear to me what b_L(s) means in Eq.(3). To my understanding, b_L is a probability distribution of states, and b_L(s) is the probability of state s. If so, I would find it more suitable for the reward function to be a distance between the probability distribution b_L and the probability distribution where \"s\" has probability 1, and the rest of the states have probability zero. This is a small change, but as a reader, I spent some time having to think over the definition because it was missing. This distracting confusion could be easily avoided by providing the complete definitions.\n\nR3. The authors could consider engaging with the recent literature of explainable RL in terms of deception and intentional behaviour, and how this can be used to analyse harmful behaviours (deception being here a harmful behaviour). See for example:\n- Liu, Z. et al. Deceptive Reinforcement Learning for Privacy Preserving Planning. AAMAS 2021.\n- Lewis, A. et al. Deceptive Reinforcement Learning in Model-Free Domains. ICAPS 2023.\n- Cordoba, F.C. et al. Analyzing Intentional Behavior in Autonomous Agents under Uncertainty. IJCAI 2023.\n- Beckers, S. et al. Quantifying Harm. IJCAI 2023.\n\n\nR4. These are just two suggestions: \n- I would make section 2.4 significantly shorter to allow for more details on the experimental evaluation. The definition is easy enough to grasp, it may not need so much redundant explanations.\n- In lines 256-257 I would put a different example.\n\nR5. The paper needs an editorial pass. Here is a list of typos and the like.\n- obtains --> obtained (line 079)\n- Figures 3, 4, and 5 are mentioned in the main text without stating that they are part of the appendix.\n- Reference to Fig. 4 in line 315-316 should be to Fig. 6. \n- I do not understand the point of Fig. 4 at all. Does it add any insight that is not already provided by Fig. 6?\n- left --> right (line 931).\n- The caption of Fig.4 is misleading, and seems to be referring to Fig. 5 instead.\n- In Figure 7, the image representation of practising photography and being part of community events are swapped. If this is not only a typo in the paper, but this is also how the examples were presented to the human subjects and LLMs, this fact should be stated somewhere, or the experiment repeated.\n- Table 2 is not mentioned in the text. \n- The font in Figures 3 and 4 is significantly smaller than the main text. Consider making it larger to improve readability. Especially in the appendix, where the page limit does not apply."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper presents a formalisation of deception in terms of POMDPs (partially observable Markov Decision Processes). Two agents, a speaker and a listener have their own PODMDPs. The listener has a model of the speaker's policies, and the speaker has a probability distribution over this model.\n\nThe speaker communicates information to the listener. The speaker may be honest or deceptive (including deception by omission), and the listener takes actions based on their model of the speaker's honesty or deception.\n\nThen deception is defined in terms of the listener's regret. This regret is measured, in the paper, via two reward functions: the listener's true reward function (which measures how much they have been harmed by the speaker's possible deception) or a reward function based on the accuracy of the listener's beliefs (which measures how much they have been mislead by the speaker's possible deception).\n\nSome human feedback experiments on simulated data show that human judgments of deception are somewhat correlated with these two regret measures, with the correlation being stronger with the accuracy-of-belief based regret.",
        "strengths": "The formalism is fine and intuitively plausible (though somewhat idealised). The experiments are well executed and well presented.",
        "weaknesses": "The formalism is interesting, but is only a mild variation of multi-agent POMDP (and MDP) formalisms from other papers (see, eg CIRL papers such as \"Cooperative Inverse Reinforcement Learning\"; searching for competitive MDP and POMDP papers will give many other examples).\n\nIt is not that the formalism is exactly the same as previous formalisms, but that it is very similar to many of them. Nevertheless, the formalism is fine (as mentioned above), and, if it lead to powerful examples and demonstrations, would be an excellent introduction to a great paper. But without those powerful examples or demonstrations, it is not enough to make the paper worthwhile in itself. The experiments show that there is a certain overlap between human judgements and these measures (especially the second regret measure with accuracy of beliefs as the reward), but all the correlations, bar one, are below 0.5, and \"humans weakly agree with this regret measure in three examples\" is not enough meat on the bones for this paper.\n\nWhat the paper needs is a great use case for this formalism, powerful experiments that show its use. An intuitively plausible definition is not enough."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper introduces a formal definition of deception based on the interaction between a speaker and a listener, both modeled as Partially Observable Markov Decision Processes (POMDPs). In particular, a \"regret\" theory is proposed to quantify deception by measuring the speaker's influence on the listener's beliefs, actions, and utility. Experiments and evaluations were conducted to assess whether the formal \"regret\" theory aligns with human judgments about deception.",
        "strengths": "Originality: \n- The paper introduces a novel approach to quantifying deception within the framework of POMDPs, capturing different forms of deceptive behavior.\n\nQuality:\n- Several well-designed experiments were conducted to assess how well the formal definition aligns with human intuition regarding deception.\n\nSignificance:\n- The work is highly relevant to the fields of AI ethics and safety, providing a foundation for future work.",
        "weaknesses": "- A realistic scenario of deception typically involves multi-step interactions between the speaker and listener. Although the paper states, \"While we consider this single-step formulation for simplicity of exposition, it is straightforward to extend the formalism into a sequential setting,\" it is not clear how the current single-step communication (PO)MDP model could be adapted to capture multi-step interactions. The extension to a sequential setting is not sufficiently demonstrated or explained.\n\n- The paper overlooks the importance of the speaker’s beliefs and intentions. For example, does the speaker believe their statement is false? This omission can lead to misclassifications, such as confusing incompetence with deception, as noted in the paper’s limitations. Furthermore, intent is crucial in legal and ethical definitions of deception, where AI agents may exhibit intentional deception towards users. The paper’s \"regret\" theory is purely consequentialist, focusing solely on the outcome or utility without considering the speaker's intent. [1][2]\n\n- The experiments involving large language models (LLMs) don't  explore the diversity of deceptive capabilities these models might show. For instance, LLMs can engage in strategic deception under specific conditions, such as when pressured. This aspect needs further  evaluation of LLM deception. [3]\n\n1.Francis Rhys Ward, Francesco Belardinelli, Francesca Toni, and Tom Everitt. Honesty is the best\npolicy: Defining and mitigating ai deception, 2023.\n\n2. Jaume Masip, Eugenio Garrido, and Carmen Herrero. Defining deception. Anales de Psicología, 2004.\nISSN 0212-9728. URL https://www.redalyc.org/articulo.oa?id=16720112.\n\n3. Large Language Models can Strategically Deceive their Users when Put Under Pressure\nhttps://openreview.net/forum?id=HduMpot9sJ"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper provides a formal definition of deception in a setting where a Speaker and a Listener interact with each other over multiple rounds. The goal of the proposed definition is to capture the Speaker's degree of deception by measuring to what extent its action(s) affect the Listener's beliefs and individual utility. The framework that is used to formalize the Speaker-Listener interactions is a POMDP variation, termed as Communication POMDP, where the Listener is modeled as part of the environment and the Speaker plays the role of the acting agent. Under this decision making framework, the Speaker's degree of deception is formalized through a function that measures the Listener's regret w.r.t. the accuracy of its beliefs and its personal utility, that results from the Speaker's actions. Finally, the paper includes extensive experimentation over a diverse set of scenarios that aims to showcase how well the proposed approach aligns with human intuition and how it compares to LLM baselines.",
        "strengths": "The paper is well-structured and the work well-motivated. Related work is sufficiently covered and the gap that the proposed definition aims to address is clearly highlighted.\n\nThe problem of detecting or preventing deception in AI systems is a very important one, and has become especially relevant these days with all the recent advancements in the field. The main idea promoted by this paper, i.e., that a formal definition for measuring deception is needed, is indeed crucial for making progress in this problem. To the best of my knowledge, this is the first work that attempts to provide such a definition that does not merely measure the truthfulness of agent's statements but instead looks deeper into the intricacies of the problem.\n\nThe Speaker-Listener setting on which this paper focuses does not of course capture all real-world decision making scenarios where detecting deception would be useful, but I find it general enough. The regret function proposed for measuring deception is simple to understand and quite reasonable, although far from complete as also mentioned in the Limitations section of the paper (see also Weaknesses below).\n\nThe biggest strength of the paper in my opinion is the experimental evaluation of the approach. I find it to be rigorous and very well-thought. Even though the proposed method does no do that well in all metrics, it significantly outperforms the LLM baselines.",
        "weaknesses": "**Presentation:** My main issue with this paper is Section 2.2 and the proposed POMDP framework. First, I found the presentation of this section to be quite poor: (a) Many things are not adequately explained, e.g., it is not explicitly mentioned that the world state does not change over time; (b) There are non-standard parts of this framework that are not revisited later on in the paper. This can hinder the understanding of the reader w.r.t. the role of these notions in the framework and why they are needed, e.g., caligraphic omicron in lines 162-164 -> no matter how many times I read this part I could not understand what is used for or what it means; (c) There are notions that are mentioned first and introduced later, e.g., reward r_L which shows up in line 145 and later in line 204 is properly explained in Section 2.4 for the first time. In general, I believe that this paper suffers from serious notational issues which can bring confusion to the reader, e.g., I understand what is the difference between states s_S and s_L but it was never made clear what plain s (Equations 1, 2, 3) stands for, to my understanding is the same as s_L but I am also not sure.\n\n**Framework complexity:** Regarding the Communication POMDP framework, I find it overly complicated and poorly motivated, why is this the correct framework to use? Given that the main purpose of this paper is to be the starting point for research on formalizing deception, I believe that a more comprehensive framework would be quite more helpful. Furthermore, the regret notion in Section 2.3 seems quite simple, which does not justify why all this complexity in the proposed framework. Even though, I have not worked out the details, it seems that turn-based Dec-POMDPs [1, 2, 3] with two agents, Listener and Speaker, could be a potentially suitable framework for expressing your regret notion.\n\n**Definition:** I appreciate the honesty in the limitations section, and I think that since you present your solution as a starting point this part is fine. I believe however that your definition has one additional important weakness that is not mentioned in the Limitations section. In case, Speaker tries to deceive Listener, but the latter does not trust the former and hence it is not influenced by its actions, your approach would classify Speaker as not deceitful, even though it is.\n\n[1] Sidford, Aaron, et al. \"Solving discounted stochastic two-player games with near-optimal time and sample complexity.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n\n[2] Jia, Zeyu, Lin F. Yang, and Mengdi Wang. \"Feature-based q-learning for two-player stochastic games.\" arXiv preprint arXiv:1906.00423 (2019).\n\n[3] Frans A. Oliehoek and Christopher Amato. 2016. A concise introduction to decentralized POMDPs. Springer"
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "The reviewers of this paper provided detailed and high-quality feedback. The main concerns raised are as follows:\n\n- The formalization is relatively straightforward and lacks significant novelty.\n- The experimental evaluations include several misleading or incomplete aspects (see Reviewer fzYT’s comments).\n- The presentation could be significantly improved and simplified. For instance, while the formulation is based on POMDP, the world state remains static in both the theoretical framework and the experiments. If the state does not transition, why base the formulation on POMDP in the first place?\n\nThe authors didn't respond to any above concerns, so it's a clear reject.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "ONfWFluZBI",
    "title": "Self-supervised contrastive learning performs non-linear system identification",
    "authors": [
      "Rodrigo González Laiz",
      "Tobias Schmidt",
      "Steffen Schneider"
    ],
    "abstract": "Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically.",
    "keywords": [
      "system identification",
      "dynamics learning",
      "identifiability",
      "self-supervised learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=ONfWFluZBI",
    "forum_url": "https://openreview.net/forum?id=ONfWFluZBI",
    "reviews": [
      {
        "rating": "6",
        "confidence": "2",
        "summary": "The main contribution of this paper is to provide identifiability theory for contrastive learning on time-series data with non-linear mixing, in the same spirit as time-contrastive learning (Hyvärinen & Morioka, 2016) for non-linear ICA. However, the authors discard the independence assumptions typically made in non-linear ICA with respect to the latent variables, and instead define a dynamical system as the data generating process. The proof operates under the assumption that the mapping from latent states to observables is injective but not necessarily linear, which is exploited to show that the composition of mixing and de-mixing by the model is an affine transform. As such, the estimated dynamics via contrastive estimation identify the true dynamics up to affine transformation in the latent space. There are experiments that corroborate the validity of this approach.",
        "strengths": "Applying contrastive learning to recover latent dynamics is itself a relatively new approach and the paper is well organised. The proofs use standard jacobian analysis tools and is easy to follow.",
        "weaknesses": "The paper needs refinement, with minor typos and inadequately captioned figures. While studying the identifiability of time-series contrastive learning might be novel, all the technical tools require carefully controlled assumptions and specific behavior of Jacobians under contrastive loss minimization, which typically do not hold in practice. Nonetheless, such assumptions are common in the literature on the identifiability of dynamical systems from observed time series."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper explores the use of self-supervised contrastive learning for dynamic system identification. It connects self-supervised learning (SSL) with identifiable representation learning, showing that SSL can identify system dynamics in latent space. The authors propose a model to uncover linear, switching linear, and non-linear dynamics under a non-linear observation model, providing theoretical guarantees and empirical validation.",
        "strengths": "1. The connection between contrastive learning and dynamic system identification is novel and could lead to simple encoder-only implementations favored in practice.\n2. The synthetic experiments, especially the ablation studies, are extensive and investigate many aspects of the theoretical results.",
        "weaknesses": "1. The contributions (comparison with previous work), theoretical techniques, and novelty are very lightly discussed. I would appreciate a detailed discussion comparing this work's conditions with those in recent literature on temporal causal representation learning (e.g., [1] and its follow-up work). This would aid the contextualization of this work and make the contribution more transparent. \n\n2. This work strikes me as theoretically oriented. There is a lack of discussion on the theoretical conditions, limitations, and implications. Just to name a few: \n\n(a) the theorem requires the length for each time series to be infinite, which appears to be quite a strong assumption and not necessitated in recent work (e.g., [1], admittedly I am not an expert and would appreciate correction if I have misunderstood the statements). Why is this necessary, and would it be possible to show results for finite-length series?\n\n(b) How does the control distribution influence the identification results? what conditions should it meet (does it have to be a time-independent Gaussian distribution)?\n\n(c) Line 175: what is the significance of $A$ -- is it previously defined? Could you comment on lines 175-177?\n\n3. Some minor issues:\n\n(a) Line 145: what is (...)?\n\n(b) Line 165: shouldn't $L^{\\top} L$ be a matrix?\n\n\n[1] Temporally Disentangled Representation Learning. Yao et al. NeurIPS 2022."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "In this paper, the authors propose a system-identification scheme for non-linear observations of non-linear time series data. In particular, they propose a modified contrastive learning set-up that posits linear latent dynamics. Compared to prior works in (time) contrastive learning, this directly enforces a notion of sequential temporal consistency, and seems to provide some benefit in system identification settings. Some supporting theory is provided, demonstrating that if the underlying dynamics are linear and invertible, then the proposed method asymptotically recovers the true dynamics up to affine ambiguity. For general non-linear systems, a (soft) switched-linear system heuristic is proposed, where Jacobian linearizations are applied at user-provided reference points.",
        "strengths": "Automatic identification of latent variables or dynamics is of critical importance in modern machine / reinforcement learning. The method the authors propose follows a line of self-supervised methods in contrastive learning. In comparison to its closest relative in time-contrastive learning (Hyvarinen and Morioka, 2016), the proposed method is seemingly more well-fit for fitting non-linear time-series data by fitting a latent time-series, rather than predicting a categorical label as in the aforementioned paper.\n\nSince a main inductive bias built into the base method is that the latent dynamics are linear, the proposed method of iterative Jacobian linearizations is a sensible adaptation, and seems to benefit performance significantly.\n\nNumerically, the proposed method appears to make contrastive methods more robustly performant.",
        "weaknesses": "In my opinion, the paper leaves quite a few critical questions unanswered, and in general suffers from a lack of polish. In its current state, I cannot recommend the paper for acceptance. The main weaknesses in my eyes are the following:\n\nThe paper claims to perform latent nonlinear system identification. This is a key desideratum in various fields such as reinforcement learning and continuous control, and thus has a rich history and literature. However, the assumptions in this paper--and notably how these inductive biases propagate to the algorithm design--severely restrict the applicability of the method without further evidence. Notably, a design assumption in this paper is that the observer function (i.e.\"mixing function\") is invertible. This is a very strong assumption in the context of *non-linear system identification*, where even the foundational theory of linear system identification does not presume: in the Linear-Quadratic Gaussian (LQG) model, where the underlying state evolves linearly $x_{t+1} = Ax_t + Bu_t + w_t$, and observations are a linear function of state $y_t = Cx_t + v_t$ (ignoring the control input term for simplicity), the classical set-up has $d_y < d_x$, such that the observations are per-timestep a low-dimensional measurement of the underlying state. This immediately rules out the mixing function $g(x) = Cx$ being invertible, and this is precisely the motivation for notions such as observability/detectability. Partial observability presents the key challenge in non-linear sysID or reinforcement learning. In particular, it is well-known in controls and RL that ignoring partial observability and imposing a Markovian model (which this paper does implicitly by enforcing the state estimate as a function solely of the current observation) can lead to very undesirable outcomes. In the contrastive learning literature, partial observability is usually not a central issue, often because it is irrelevant for the motivating application (e.g. in computer vision), but one must address this problem for time-series data. In fact, the cited Time-Contrastive Learning method (Hyvarinen and Tomioka, 2016), despite making the same assumption in theory, actually propose a method that is more amenable to partial observability, since they predict categorical labels to *chunks* of observed data.\n\nRegarding the polish of the paper, there are various typos and lacking definitions that make the paper hard to parse at times. The minor ones that I have caught are listed below. A particularly confusing point is the role of the control input $u_t$. The paper presents the control input as entering the latent dynamics directly. However, it is typically the case that the control input enters the state through a (possibly state-dependent) actuation matrix $\\mathbf B(x_t) u_t$. In any case, how the control input enters the dynamics should be dependent on the parameterization of the dynamics, e.g. the affine ambiguity in $\\mathbf L$ in the paper, which is not reflected in the authors' method as far as I can tell. Furthermore, it is unclear if the control input is available to the learner (which is usually the case in sysID), or if it is playing the role of stochastic noise, which eq (9) seems to suggest compared to eq (1). In either case, what role is the control input playing here: in the authors' set-up, there is no need to learn the actuation matrix, and the experiments involve learning a low-noise, nearly deterministic Lorenz system, which rules out some persistency of excitation effect (Tsiamis and Pappas, 2019).\n\n**Minor comments/typos:**\n\nFigure 1: x -> $x$\n\nPage 3: \"linear identifiability (...)\", missing eqref?\n\nTheorem 1: \"bijective dynamics model $\\mathbf f$\", should probably mathematically define what that means.\n\nTheorem 1: $\\lambda$ is not defined in the main paper, only in the appendix.\n\nCorollary 1: \"$\\hat{\\mathbf f} :=1$\", seems to be bad notation.\n\nBeginning of Sec 4: \"non-lineary\" -> \"non-linearly\"\n\nEquation (7): where is $g_k$ defined? Possible hash collision with mixing function notation.\n\nTable 1: should probably introduce acronym \"LDS\" = Linear Dynamical System somewhere\n\nTable 1: What does LDS$\\downarrow$ mean?\n\nTable 1: What do $\\mathbf L$, $\\mathbf L'$, $(\\checkmark)$, $\\mathbf I$ in the theory column indicate?\n\nBetween (11) and (12): \"tailor\" -> \"Taylor\"\n\nBefore Sec 5: \"matices\" -> \"matrices\"\n\nImplementation paragraph: possibly missing number of A100 cards?\n\nEq (23): where is $p_u$ defined?\n\nEq (25): what does $p_{D}(y)$ denote precisely?\n\nAfter Eq (50): \"which is probably still fine because $\\exp(-\\|\\mathbf L \\cdot \\|^2)$ is a valid kernel function (?)\". This probably needs to be formalized/reworded.\n\n**References:**\n\nAnastasios Tsiamis and George J. Pappas, \"Finite Sample Analysis of Stochastic System Identification\", 2019."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper \"Self-Supervised Contrastive Learning Performs Non-Linear System Identification\" explores contrastive learning (CL) for identifying non-linear temporal representations. It presents proof of the identifiability of latent variables up to a linear transformation, removing the requirement for independent noise. The proposed model, DynCL, is validated using synthetic data to support the theoretical findings. Additionally, the authors introduce a model called delta-SLDS, designed to capture switching between linear and non-linear dynamic systems.",
        "strengths": "- The theorem is both interesting and novel, demonstrating that the learned latent variables can be identifiable up to linear transformations even in the absence of independent noise, provided that some other assumptions are met.\n- This theorem provides valuable insights into the mechanisms underlying contrastive self-supervised learning methods.\n- A lot of ablation studies and visualization experiments are conducted, which makes the paper more convincing.",
        "weaknesses": "- Notations are not clear. For example, in Eq (3), the meanings of $x, x', x''$ should be mentioned in advance.\n- In theorem (1) and its proof, the assumption is not aligned with Equation (1-2), where all noise disappears. Further discussion is required.\n- The difference in theorem part should be compared with previous works on CL more detailed, since it is a work focusing on theorem. Making the difference more clear will make it more readable.\n- In experiment parts\n    - Baselines like TCL should be compared\n    - By identifiability, some metrics like MCC should be compared even though they are not component-wise identifiable.\n- Lots of typos:\n    - line 133 (supp figure), line 145 (...), seems unfinished part\n    - line 250: tailor -> talyor\n    - line 637: Theorem 2 -> Theorem A1\n    - footnote of page 13: broken reference  \n    - line 659, 665: missing reference"
      },
      {
        "rating": "8",
        "confidence": "3",
        "summary": "This paper focuses on contrastive learning (CL) methods for dynamical systems.\nThe authors show that under certain assumptions CL performs system identification and can therefore uncover the latent dynamics of the data.\nThe theoretical findings are applied to switching linear dynamics and non-linear dynamics, and are demonstrated from an empirical point of view using simulated data.",
        "strengths": "1. Overall this is an interesting paper, that gives an insight on why CL techniques are effective for system identification\n2. The introduced CL variant is well presented and theoretically grounded, and could inspire further theoretical research and models\n3. DynCL can effectively identify latent states and system dynamics in the experiments on simulated data\n4. The authors present a good selection of ablation studies to demonstrate the impact of the different modeling/parametric choices",
        "weaknesses": "MORE REALISTIC EXPERIMENTS\n\nAs stated by the authors in the limitations of this work, the focus of this paper is only on simulated data. While I understand their point of view, and I also agree that the theoretical contribution/simulated experiments are also valuable by themselves, I fear that the impact of this work in this current state will be more limited than it could be if there was a better demonstration of real world applicability.\n\nYou could for example try to apply your method to some of the datasets used in \"Discovering State Variables Hidden in Experimental Data\" (https://www.cs.columbia.edu/~bchen/neural-state-variables/).\n\nIf the above is too challenging to achieve, you should at least try to discuss more in detail what each of you theoretical assumptions means in practice, and what you expect to happen if they are not met in real-world experiments. For example, the fact that $p(u_t)$ is a normal distribution seems quite strict in many applications. \n\nBASELINE\n\nThe baseline you use in your experiment seems quite weak, as it does not even use a dynamics model. Have you tried other approaches, for example models doing next-token prediction tasks?\n\n\n\n\nCLARITY\n\nThere are several missing definitions/clarifications in the paper that make it a bit harder to follow:\n1. N in (3) is not defined\n2. \"Supp figure\" in line 133 is unclear\n3. Not sure what \"(…)\" in line 145 means\n4. The name $\\nabla$-SLDS is never formally defined\n5. In table 1 you have a column called \"theory\" with different options. What do these option represent exactly?\n6. The abbreviation \"GT\", which I assume stands for ground truth is used in many places but never defined\n7. What is $\\pi$ in equation (8)?\n8. The vMF abbreviation is never defined\n9. The DynCL results from Table 1 should be discussed more in depth."
      }
    ],
    "rating_avg": 6.4,
    "confidence_avg": 2.8,
    "decision": "Accept (Poster)",
    "meta_review": "The paper studies representation learning for system identification of the time-series data. Namely, under the assumption that the ground-true time-evolution happens in the latent space, the authors examine the question of whether one can identify the system, i.e. learn the latent representation and the dynamics in the latent space from the time-series observations.\n\nBuilding on the ideas of contrastive learning, the authors propose a model and prove a theoretical result for this model that in the limit of infinite time-series, the dynamical system can be identified up to a linear transformation (notably, assuming that the observation is obtained from the latent by an injective map). They apply their theory to identify the switching linear dynamics (linear dynamics in which parameters change in time by switching between a finite number of values). Furthermore, the authors argue that the non-linear dynamics can be approximated within the family of switching linear dynamics by the corresponding linearization of the dynamics around the time-series points in the latent space. Finally, the authors empirically validate the proposed framework for linear, switching linear, and Lorenz dynamical systems generating synthetic data in 3 and 6 dimensions.\n\nDespite the reviewers' unanimous low confidence and the much room for improvement left in the presentation, I'm leaning toward accepting the paper. The main reason for this is that different reviewers found different parts of the work to be appealing. This signals that the paper might be relevant beyond a single community of researchers.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Ql7msQBqoF",
    "title": "MAC-CAFE: Multi-actor, Centralized Critic Architecture for Feedback-driven Editing",
    "authors": [
      "Naman Gupta",
      "Shashank Kirtania",
      "Priyanshu Gupta",
      "Krishna Kariya",
      "Sumit Gulwani",
      "Arun Iyer",
      "Suresh Parthasarathy Iyengar",
      "Arjun Radhakrishna",
      "Sriram K. Rajamani",
      "Gustavo Soares"
    ],
    "abstract": "Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce MAC-CAFE, a novel Multi-actor, Centralized Critic Architecture for Feedback-driven Editing approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. Each document is assigned to an actor, modeled as a ReACT agent, which performs structured edits based on document-specific targeted instructions from a centralized critic. Experimental results show that MAC-CAFE significantly improves KB quality and RAG system performance, enhancing accuracy by up to 8% over baselines.",
    "keywords": [
      "Retrieval-Augmented Generation",
      "Large Language Models",
      "Knowledge Base Editing",
      "Prompt Optimization"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Ql7msQBqoF",
    "forum_url": "https://openreview.net/forum?id=Ql7msQBqoF",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper presents a framework for editing and refining information in a knowledge base used in a RAG setup. The framework, MAC-CAFE, has several steps, including a multi-agent updating process where each agent proposes updates for an individual document and updates are aggregated using a critic model. The method improves performance in situations with incomplete or out of date knowledge.",
        "strengths": "S1. This is an important problem, and the paper examines it across several reasonable domains. The method clearly outperforms the baselines presented. \n\nS2. The idea of using multiple agents to control knowledge updates and refinement is a nice one, and seems well-executed here.\n\nS3. I like the idea of defining multiple metrics for knowledge base edits and evaluating along these axes. I think the three chosen are reasonable choices for this task, although I don't have much familiarity with current metrics (if any) used to evaluate this.",
        "weaknesses": "W1. The centralized feedback analysis should be compared with self-reflection / self-refine methodologies. \n\nW2. Managing each document with an agent seems quite computationally expensive-- can you provide some analysis of cost/benefit?\n\nW3. Not a factor in my score-- the contributions listed in the intro don't tie back clearly to sections in the paper; it would be stronger to identify which section each of these contributions is described within. This particular subsection also feels like it may have been LLM-generated; I'm not concerned about that for the rest of the paper, but that section reads a bit poorly."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper addresses the issue of hallucination in large language models (LLMs) within the Retrieval-Augmented Generation (RAG) framework. It introduces MAC-CAFE, a novel multi-agent reinforcement learning approach that iteratively refines external knowledge using expert feedback. Experimental results demonstrate that MAC-CAFE enhances LLM prediction accuracy by 8% compared to baseline methods.",
        "strengths": "1. The focused issue of LLM hallucination is an important problem.\n\n2. The motivation of the study that emphasizes knowledge editing makes sense.",
        "weaknesses": "My major concern lies in two aspects:\n1. The rationale behind the design of the proposed method is insufficiently detailed. Although the authors thoroughly describe their implementation, particularly in Section 4, the intuition underlying each component of the design is not clearly described.\n\n2. The result analysis section is quite limited. The authors mainly emphasize the effectiveness of the proposed approach but miss a variety of experiments, such as ablation studies or error analysis, to offer a deeper understanding of its characteristics. For example, which components of the approach are more influential than others? Are there any identifiable patterns in the prediction errors?\n\nAdditionally, the paper writing could be improved. For example, I’m a bit confused about the purpose of section 3. If the illustrative example is intended to motivate MAC-CAFE, it might be more effective to condense this description and incorporate it into the Introduction. Doing so would allow for a more detailed and thorough result analysis in the corresponding section."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes MAC-CAFE, a framework to iteratively refine a knowledge based based on expert feedback. They provide experiments on a relatively lesser used coding language, Pony, to simulate a setting where current KBs do not have high amounts of information available. They also provide results on other datasets such as SciPy, Tensorflow, etc.",
        "strengths": "The idea of knowledge-base fixes via edits/additions/deletions is highly relevant today, and MAC-CAFE proposes an interesting and feasible way to approach the same.",
        "weaknesses": "* Section 4.1 that introduces the problem formulation can be written more clearly, especially regarding the following points:\n    * Introduce tau(q_i, K) before equation-1 in page 4, to ensure the equation is understandable - currently tau is first introduced in the next subsection.\n    * Explain the function g - is g supposed to give a higher score when o_i=c_i?\n    * Define o_i=B(qi, tau(qi, K)) [is this the definition?]\n* I'd suggest to add a short note describing the ReACT agent (perhaps near line 348) - even though it has been cited, not everyone might have a working knowledge of it.\n* Line 348, Actors: I assume a real-life knowledge base would have a huge number of documents - how feasible is it to have a distinct actor model for each document?\n* What are the models used for (1) B the LLM, and (2) the actors? I was unable to find it in Section 6. I suggest to add a subsection or paragraph on the same in Section 6. \n* I also suggest the authors to add a paragraph each about the baseline PromptAgent and about Monte Carlo Tree Search for more accessibility of the paper."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "This paper applies ideas from prompt optimization to optimizing KBs for RAG. \nThe domain considered is text-to-code using documentation as context, focusing on lower-resource coding languages.\nThe authors take a multi-agent approach, splitting a KB into multiple documents and proposing edits for each document according to a fixed set of edits, based on execution feedback. \nAfter a global document selection stage, each document is edited by a separate editing agent (implemented via REACT prompting and PromptAgent's Monte-Carlo tree search) that proposes a sequence of edits to the document.\nTheir method, MAC-CAFE, is evaluated on five text-to-code datasets, with two covering cases where the documentation is incomplete and three covering cases where the documentation is incorrect. \nThe method is evaluated by looking at accuracy before and after editing, as well as coherence metrics. \nMAC-CAFE improves accuracy on the test set for all datasets as compared to the base model and a PromptAgent-based baseline. \nMAC-CAFE also results in more coherent documentation according to G-Eval.",
        "strengths": "1. **Motivating problem**: The problem of outdated KBs (especially in text-to-code) is interesting and well-motivated, as information in these domains does change consistently. A solution for automatically keeping documentation up-to-date here would indeed be interesting. \n\n2. **Multiple domains**: The authors evaluate the method across multiple domains and datasets and include two different settings (incomplete vs. incorrect info). Their method shows improvements across all domains. \n\n3. **Clear method figure**: Figure 2 is easy to parse and describes the MAC-CAFE method in a way that is easy to understand.",
        "weaknesses": "1. **Limited methodological contribution**: Besides the splitting of the problem into multiple documents, the paper seems like a fairly direct application of PromptAgent with limited technical novelty besides the application to a new domain. \n\n2. **Assumption of error knowledge**: The method uses LLMs to generate code and then use feedback from generated code to update the docs. In lines 194-199, the authors correctly point out that there could be multiple sources of error, including sources that do not stem from incorrect docs. However, the authors then say that they assume errors result from *only* from incorrect docs. It's not clear at all from the writing how -- or whether -- this is enforced, i.e. how the authors ensure that the errors are in fact from errors in the docs rather than the generator's shortcomings. This is especially troubling given that they evaluate on lower-resource coding languages where the model might be worse at generating even with correct docs. If this assumption is enforced, the authors should explain how. If it's not enforced, it's the authors responsibility to convince readers that their benefits come from the system in fact improving the docs in some interpretable way, as opposed to addressing simpler kinds of errors. \n\n3. **Limited results**: the \"extensive experiments\" mentioned in fact boil down to 2 short paragraphs on the last page of the paper. There are no ablations for the design choices made (e.g. action space, state representation) and no ablations showing the necessity of splitting the task into a multi-actor setup. The authors only evaluate a single LLM. There is no analysis of the resulting KBs after editing. \n\n4. **Unclear baselines and metrics**: It's not clear what the baseline PromptAgent-E is/how it is implemented, why it was chosen, and why it is a fair/relevant baseline. This kind of detail should be brought up in the main paper (which the paper fails to do) and can then be elaborated in supplementary material (which the paper lacks). The authors also do not convincingly argue for the completeness metric measures completeness. The coherence metric is not clearly explained. The metric section is split across 5 and 6.3 in a way that is very hard to follow \n\n5. **Unclear writing and organization**: In addition to the clarity issues above, the rest of the paper also omits a large number of details. To give a few examples: \n- The method hinges almost entirely on prompting but the authors do not provide their prompts. \n- In Table 3, it's not clear what any of the numbers refer to. They aren't percentages, but they also don't add up to the number of documents. \n- Much of the writing could be compressed. There are several sections that conceptually should be subsections/compressed together. At the same time, other parts are overcompressed, e.g. the results section, where Table 3 is unreadably small. \n\n6. **Method described as using gradients**: The method (which is in fact gradient-free) is described in terms of gradients. My feeling is that the authors should make clear that the \"gradient\" part here is purely a helpful metaphor, as the method does not actually compute any gradients at all. However, from the writing, this point is not brought through clearly."
      }
    ],
    "rating_avg": 3.25,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "The paper introduces MAC-CAFE, a novel Multi-actor, Centralized Critic Architecture for Feedback-driven Editing to improve knowledge bases (KBs) used in Retrieval-Augmented Generation (RAG) systems. By employing a multi-agent framework with structured edits guided by a centralized critic, MAC-CAFE refines KBs iteratively using expert feedback.\n\nStrength:\nAll reviewers agree the problem studied in this paper is quite interesting and has important practical value.\n\nWeakness:\nThe paper writing needs improvement, such as more clarification about the motivation and rationales behind the designs of the proposed method. And also there is insufficient analysis / ablation experiments to justify the effectiveness of the approach. Also, it is also pointed out by Reviewer eoD8 that this paper has some issues in its error knowledge. \n\nOverall, this paper still needs significant work in its writing and experiments in order to make it publishable.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "NHe6guO3l6",
    "title": "Achieving Exact Federated Unlearning with Improved Post-Unlearning Performance",
    "authors": [
      "Ze Yu Zhang",
      "Bui Thi Cam Nhung",
      "Arun Verma",
      "Bolin Ding",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "Federated learning is a machine learning paradigm that allows multiple clients to train aggregated model via sharing model updates to a central server without sharing their data. Even though the data is not shared, it can indirectly influence the aggregated model via the shared model updates. In many real-life scenarios, we need to completely remove a client's influence (unlearning) from the aggregated model, such as competitive clients who want to remove their influence from the aggregated model after leaving the coalition to ensure other clients do not benefit from their contributions. The influence removal is also needed when the adversarial client negatively affects the aggregated model. Though the aggregated model can be retrained from scratch to ensure exact unlearning (completely removing the client's influence from the aggregated model), it performs poorly just after the unlearning, which is undesirable during deployment. To overcome this challenge, this paper proposes federated unlearning algorithms that ensure exact unlearning while achieving better performance post-unlearning. Our experimental results on different real datasets validate the performance of the proposed algorithms.",
    "keywords": [
      "Exact Federated Unlearning",
      "Improved Post-Unlearning Performance",
      "Multi-Models Training"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=NHe6guO3l6",
    "forum_url": "https://openreview.net/forum?id=NHe6guO3l6",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper tackles the challenge of achieving exact federated unlearning while maintaining good post-unlearning performance. Existing methods, such as retraining the federated model from scratch, fail to provide satisfactory initial performance after unlearning. Therefore, the authors propose  Bi-Models Training (BMT) and Multi-Models Training (MMT) to address this issue. BMT preserves isolated copies of local models and reuses clients' existing knowledge during unlearning. MMT trains multiple sub-federated learning models on disjoint subsets of clients and aggregates the best sub-models upon unlearning. Both methods ensure exact federated unlearning while achieving improved performance compared to retraining from scratch.",
        "strengths": "**S1**. The topic of how to simultaneously preserve the model performance while exactly removing one client's influence is an important problem in practice.\n\n**S2**. Both theoretical and empirical results were provided to verify the effectiveness of the proposed method.",
        "weaknesses": "**W1**. The technical contributions of the proposed methods are limited. For BMT, it only re-initializes the global model with local models that are only trained once by remaining clients, which is only equal to saving one round's communication cost compared to the random initialization case. For MMT, it maintains multiple series of model training (e.g., global model training, sub-FL training, and local training) to increase the training process's robustness to clients' exclusion. The local computation and communication cost can be larger than restart training the model from the randomly initialized one.\n\n**W2**. Some claims are confusing. For example, the reasons for the new model's low accuracy need further clarification in line 161 \"the new model may have very low accuracy compared to the aggregated model before unlearning due to restarting the FL process with random initialization \", since restarting the FL process with remaining clients should not cause performance drop given unlimited the communication and computation resources. \n\n**W3**. The definition of *double influence* and its impact are also unclear. Please clarify this term in the response."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "In this paper, the authors propose an exact unlearning method for federated model training. The main idea behind this paper is to better initialize the global model and re-train the global model on the remaining clients in a federated way. The authors further propose two strategies for the global model initialization in federated unlearning, including using the first-round local models of remaining clients and using the first-round local models and the corresponding sub-FL models of the remaining clients. Experiments on several datasets show the proposed method can outperform a naive baseline.",
        "strengths": "1. The federated unlearning studied in this paper is an important research problem.\n2. This paper is well-written and easy to follow.",
        "weaknesses": "1. The academic findings found in this paper are not novel. \nThe main academic findings brought by this paper are that using the proposed initialization strategies (using some locally trained models) rather than random initialization can improve the efficiency of federated exact unlearning. However, it is straightforward that using some locally trained models can speed up the convergence of the federated exact unlearning.\n\n2. The proposed initialization algorithm, i.e., MMT, is not well-motivated. \n(1) In lines 241-253, the authors claim that we should capture the joint influence of multiple clients for better global model initialization. However, the authors do not give any clear definition of the client's joint influence. To the best of my knowledge, \"client joint influence\" is not common knowledge in the area of federated learning. \n(2) In lines 263-269, the authors claim that \"If the number of models to aggregate is less, it implies that the initialization of the global model contains the most joint influence of clients\". It is also difficult to understand this statement. If we only use a small part of local models for global model initialization, why can we better capture the client joint influence?\n\n3. The MMT method is inefficient in both computation and storage.\nIn the proposed MMT method, the authors introduce a lot of sub-models to memorize the historical training state. However, this will introduce many extra computation costs. For example, if there exists $n = 2^m$ clients, and we maintain an influence tree with $h$ levels (including the top level and the bottom level), the computation costs of the FL systems will increase $h$ times, which may be impractical in real-world applications.\n\n4. The experiments in this paper can be improved.\n(1) Only small-scale datasets are used for experiments, i.e., MNIST, FMNIST, and CIFAR. The authors should compare different methods on larger datasets to demonstrate the superiority of the proposed method.\n(2) Only small-scale models, i.e., two-layer MLPs and two-layer CNNs, are used for experiments. Can the proposed method be applied to large-scale models, especially for the LLMs?\n(3) The authors only compare a trivial baseline method, i.e. training from scratch. However, there are many SOTA federated unlearning methods, the authors should compare them in this paper."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes an exact federated unlearning method which completely removes the influence of a particular client on the aggregated model. The traditional method for exact unlearning involves retraining the model from scratch. This is effective in removing influence, but also leads to degrading performance after unlearning. To address this issue, the authors propose to maintain a local model that is trained fully on local datasets. Instead of randomly reinitializing the model after unlearning, they use isolated local models for initialization. This method ensures exact unlearning while improving the post-unlearning performance.",
        "strengths": "1. The proposed algorithm effectively achieves exact unlearning by retraining from isolated local models instead of random reinitializations.\n2. The method maintains model performance post-unlearning, making it highly practical for deployment in real-world federated systems.\n3. The method is validated across multiple datasets, demonstrating its effectiveness in achieving better post-unlearning accuracy.",
        "weaknesses": "1. The core idea of this paper stems from using local models instead of random initialization. This trick is quite common in regular federated learning.\n2. Definition 1 and theorem 1 seem redundant. Utilizing a disjoint binary tree structure to eliminate double influence is intuitive enough that it does not require additional justification or explanation.\n3. The paper assumes that each local client has the same amount of data. However, varying data quantities can lead to significant differences in the performance of local models, which may impact post-unlearning aggregation. The authors should investigate this realistic scenario.\n4. In figure 4(d), there seems to be a performance decline in BMT and MMT while the curve for retraining is still rising. Can the authors let the curve to converge and explain why this happens?"
      }
    ],
    "rating_avg": 3.6666666666666665,
    "confidence_avg": 4.333333333333333,
    "decision": "Reject",
    "meta_review": "The paper studies unlearning from an FL standpoint. The authors propose two new methods for federated unlearning, that require training multiple models per subsets of clients, which is made efficient though the construction of a hierarchy of influence across client subsets, identifying how to group them together. This is shown to be performant w.r.t. to unlearning but also w.r.t. post unlearning predictions via experiments.\n\nSome reviewers recognized strengths in the importance of the problem and in the experiments. Including larger datasets both for vision and text during the rebuttal stage improved the paper, and the diversity of datasets was considered a strength.\n\nIssues were raised regarding the technical novelty of the paper: techniques were considered straightforward heuristics. Both the motivation of the proposed method and its overall efficiency were brought into question.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "LJWPYzjDz4",
    "title": "Extending Flexibility of Image Coding Enhancement Framework for IoTs",
    "authors": [
      "Yu Mao",
      "Jingzong LI",
      "Jun Wang",
      "Hong Xu",
      "Tei-Wei Kuo",
      "Nan Guan",
      "Chun Jason Xue"
    ],
    "abstract": "Neural image compression, necessary in various edge-device scenarios, suffers from its heavy encode-decode structures and inflexible compression level switch. The primary issue is that the computational and storage capabilities of edge devices are weaker than those of servers, preventing them from handling the same amount of computation and storage. One solution is to downsample images and reconstruct them on the receiver side; however, current methods uniformly downsample the image and limit flexibility in compression levels. We take a step to break up this paradigm by proposing a conditional uniform-based sampler that allows for flexible image size reduction and reconstruction. Building on this, we introduce a lightweight transformer-based reconstruction structure to further reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of our system over existing compression techniques, especially in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.",
    "keywords": [
      "Data Compression",
      "IoT infrastructure",
      "Edge Computing",
      "Scalable Design"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=LJWPYzjDz4",
    "forum_url": "https://openreview.net/forum?id=LJWPYzjDz4",
    "reviews": [
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes a new downsampling strategy for image compression on edge devices. The method achieves non-uniform sampling by erasing and squeezing the image blocks. In addition, the down-sampled image can be recovered using a transformer. Experiments show that the proposed method achieves superior rate-distortion performance (distortion measured by Brisque, Pi and Tres) with reduced computational complexity.",
        "strengths": "1. The paper is well written and easy to follow.\n2. The idea of erasing and squeezing is interesting.",
        "weaknesses": "1. The proposed method is not suitable for image compression. The erasing process removes the image blocks randomly, which can lead to irretrievable losses of objects. An importance-based or smoothness-based method may be a better choice.\n2. There is no comparison with uniform sampling methods, which is necessary to show the advantages and disadvantages of the proposed sampling method.\n3. The non-reference distortion metrics used in this paper are not appropriate. As far as I know, the non-reference metrics can only measure whether the image is real, which has nothing to do with fidelity. So we can generate a completely different image at the decoder size with an image generation model without any bitstreams to get a high rate-distortion performance, which may be meaningless in applications."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The paper proposes Easz, a novel image compression framework designed for edge devices, addressing the limitations of existing NN-based compression methods. By introducing an erase-and-squeeze technique, Easz allows for flexible compression levels and efficient image reconstruction, making it suitable for various IoT applications.",
        "strengths": "1. The erase-and-squeeze method offers a flexible alternative to traditional uniform downsampling techniques, enhancing compression adaptability.\n2. The framework is evaluated on a real-world testbed, providing credible evidence of its performance compared to existing compression methods.\n3. Easz is compatible with existing compression algorithms, increasing its utility in practical applications.",
        "weaknesses": "1. The review of related work lacks depth, particularly regarding recent advancements in image compression techniques applicable to edge devices.\n2. The reliance on GPU capabilities for reconstruction may limit the framework's applicability in low-power or resource-constrained environments."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper propose a paradigm to offer compression flexibility and efficiency improvement for edge-device scenarios. It propose a conditional uniform-based sampler for flexible image size reduction and reconstruction, as well as a lightweight transformer-based structure to redeuce reconstruction load on the receiver side.",
        "strengths": "- To avoid issues stems from adjacent sampled areas, this paper add constraints for row-based randoming sampling.\n\n- This paper introduces an erase-and-squeeze method to enhance flexibility and efficiency.",
        "weaknesses": "- The proposed uniform-based sampler gives better results by adding constraints. However, it is still a rule-based sampling method that is unaware of the content. Why didn't you consider a learn-based sampler solution to improve the performance?\n\n- The input of the reconstruction. Is the masking map also needs to be coded and transmitted? Is it also an input of the reconstruction network? I think these details should also be described in Figure 2.\n\n- Settings about Table 1. Easz is a lossy compression method but other super-resolution methods such as SwinIR are free from bit cost constraints. Why does the proposed method Easz show such a performance improvement (larger than 3dB)? What are the testing settings(such as the input size) for the proposed method and the other methods? \n\n- Some citations are neglected, e.g., the citation for the methods in Table 1 and Table 2. Please check about these problems."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes an image compression enhancement framework for edge devices called Easz, which uses the ‘erase-and-squeeze’ technique to improve compression flexibility and efficiency.",
        "strengths": "It seems reasonable to downsample the original image to reduce the coding complexity at the transmitter side.",
        "weaknesses": "1. Under the experimental conditions where GT images are available, relying on no-reference metrics to assess the reconstruction quality will result in an incomplete assessment and make it difficult to clearly demonstrate the advantages of the method in preserving image details. Reference metrics can provide more direct quantitative comparisons in the presence of GT, so the inclusion of metrics such as PSNR and SSIM in the paper will make the experimental results more convincing and comparable.\n\n2. The NN-based image compression models compared in this article all use autoregressive context modules, thus resulting in coding and decoding times that can be very long, and the authors need to compare them with parallel context modules (e.g. checkerboard [1]).. \n\n3. Higher complexity occurs on the receiver side using transformer reconstruction, especially for HD images. The complexity calculation in Supplementary Material Section B seems to be wrong. If the patch size is $N \\\\times N$, then the dim should be $d \\\\times N \\\\times N$, so the complexity of the model is not reduced.\n\n\n\n[1] He, Dailan, et al. \"Checkerboard context model for efficient learned image compression.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 5.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "llW4qRsF0o",
    "title": "Physics-Transfer Learning: A Framework to Address the Accuracy-Performance Dilemma in Modeling Complexity Problems in Engineering Sciences",
    "authors": [
      "Yingjie Zhao",
      "Zhiping Xu"
    ],
    "abstract": "The development of theoretical sciences traditionally adheres to an observation-assumption-model paradigm, which is effective in simple systems but challenged by the `curse of complexity’ in modern engineering sciences. Advancements in artificial intelligence (AI) and machine learning (ML) offer a data-driven alternative, capable of interpolating and extrapolating scientific inference where direct solutions are intractable. Moreover, feature engineering in ML resembles dimensional analysis in classical physics, suggesting that data-driven ML methods could potentially extract new physics behind complex data. Here we propose a physics-transfer (PT) learning framework to learn physics across digital models of varying fidelities and complexities, which addresses the accuracy-performance dilemma in understanding representative multiscale problems. The capability of our approach is showcased through screening metallic alloys by their strengths and predicting the morphological development of brains. The physics of crystal plasticity is learned from low-fidelity molecular dynamics simulation and the model is then fed by material parameters from high-fidelity, electronic structures level, density functional theory calculations, offering chemically accurate strength predictions with several orders lower computational costs. The physics of bifurcation in the evolution of brain morphologies is learned from simple sphere and ellipsoid models and then applied to predict the morphological development of human brains, showing excellent agreement with longitudinal magnetic resonance imaging (MRI) data. The learned latent variables are shown to be highly relevant to uncovered physical descriptors, explaining the effectiveness of the PT framework, which holds great potential in closing the gaps in understanding complexity problems in engineering sciences.",
    "keywords": [
      "Physics-Transfer Learning; Accuracy-Performance Dilemma; Engineering Sciences; Complexity; Materials Strength; Brain Development"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=llW4qRsF0o",
    "forum_url": "https://openreview.net/forum?id=llW4qRsF0o",
    "reviews": [
      {
        "rating": "5",
        "confidence": "1",
        "summary": "This paper introduces a physics-transfer (PT) learning framework designed to bridge digital models of varying fidelities and complexities, effectively addressing the accuracy-performance trade-off in multiscale problem analysis. By leveraging PT learning, the model achieves reduced computational costs compared to traditional machine learning models, making it a more efficient solution for understanding complex, representative physical phenomena.",
        "strengths": "This model achieves lower computational costs than traditional ML models, with a method that is easy to understand.",
        "weaknesses": "1. No new models have been proposed.\n2. Why was CNN chosen over other state-of-the-art models?\n3. The comparison methods are insufficient.\n4. Appropriate statistical analysis is required."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "In this manuscript, the authors present a physics-transfer (PT) learning framework to merge physics-based modeling and machine learning techniques in complex engineering problems. The framework aims to resolve the accuracy-performance dilemma by learning physics across digital models of varying fidelities. The paper demonstrates the framework's capabilities through two case studies: predicting the strength of metallic alloys and modeling the morphological development of human brains. The authors claim that their approach not only enhances predictive accuracy but also provides new insights into the underlying physics of these systems.",
        "strengths": "The PT framework represents a novel integration of physics and machine learning, introducing an approach to the challenges posed by multiscale problems in engineering sciences. By considering both low-fidelity and high-fidelity models, the authors creatively combine existing ideas to form a new methodology. The motivation for the PT framework is articulated clearly, establishing a strong rationale for the research. The paper successfully outlines the accuracy-performance dilemma, making it accessible to readers familiar with the challenges in engineering modeling. Furthermore, the potential applications in materials science and neuroscience could lead to substantial advancements in these fields.",
        "weaknesses": "The manuscript lacks a comprehensive comparison with existing methods such as $\\Delta$-learning and transfer learning. A more in-depth analysis highlighting the advantages and limitations of the PT framework relative to these approaches may enhance the credibility of the manuscript. Specific metrics and results demonstrating improved performance would provide stronger evidence of the framework's contributions.\nThe manuscript contains sections that are overly technical, which may limit its accessibility to a broader audience. E.g., the explanation of the PT framework's mechanics could benefit from simpler language and additional visual aids to enhance understanding.\nThe manuscript does not adequately address the scalability of the PT framework, particularly regarding its application to larger datasets or more intricate models. A discussion about the challenges and potential solutions for scaling the approach would be valuable."
      },
      {
        "rating": "1",
        "confidence": "3",
        "summary": "The authors purport to develop a novel framework for modeling physical systems using deep learning architectures in this work. The authors claim that models that first learn the \"physics\" of a given simple system will generalize better to more complex system variants. The authors utilize metallic alloy strength and brain morphological development as two systems of study, providing simple examples of how learning lower-fidelity models can aid in modeling more complex systems.",
        "strengths": "Transfer learning from low to high fidelity systems is an interesting sub-domain within transfer learning at large. I thank the authors for highlighting this problem in their work, even if it is a domain which has already been significantly explored elsewhere.\n\nI do think the core intuition at work in this paper is interesting - the use of lower-fidelity models for developing physics is quite common in the engineering sciences and the sciences at large; however, I think this core insight is marred by a lack of detail or acknowledgement of other related works which approach these problems in similar ways.",
        "weaknesses": "I am chiefly concerned with the significance of this contribution to the literature on transfer learning and the deep learning community at large. Many approaches to transfer learning using lower-fidelity systems or simulations exist already in the literature, and it is well-understood that such approaches can provide benefits over training directly on the more complex system. It is not clear to me how the approach in this work differs from these methodologies significantly other than in applications.  See [1], [2], [3], [4] for some examples which utilize a very similar underlying approach to that in this work. I would like to challenge the authors to differentiate their work more from this existing literature and consider a resubmission. If anything, the authors should consider these other works as baseline approaches for the sake of comparison. \n\nAdditionally, this paper suffers from a lack of detail regarding the proposed framework. The most obvious omission is any rigorous definition of what the \"learning the physics\" means within this work. In previous literature, \"learning the physics\" more frequently means learning a set of differential equations which describe the system, rather than learning a black-box model with parameters which can predict system changes, but which doesn't provide any direct physical interpretation. Based on the discussion in section 2, the authors seem to be utilizing the latter kind of approach. It is not at all clear to me how learning a simple convolutional neural network provides any manner of learning of the physics of a system. The authors either need to clarify this connection, or clarify that they are doing something other than learning the physics of the system. I am willing to reconsider this point given a very compelling argument from the authors; however, I think even if an argument is provided here, a more significant revision would be needed to further clarify the details omitted in this work.\n\n[1] De, Subhayan, et al. \"On transfer learning of neural networks using bi-fidelity data for uncertainty propagation.\" International Journal for Uncertainty Quantification 10.6 (2020).\n\n[2] Chakraborty, Souvik. \"Transfer learning based multi-fidelity physics informed deep neural network.\" Journal of Computational Physics 426 (2021): 109942.\n\n[3] Liu, Zeyu, Meng Jiang, and Tengfei Luo. \"Leveraging low-fidelity data to improve machine learning of sparse high-fidelity thermal conductivity data via transfer learning.\" Materials Today Physics 28 (2022): 100868.\n\n[4] Song, Dong H., and Daniel M. Tartakovsky. \"Transfer learning on multifidelity data.\" Journal of Machine Learning for Modeling and Computing 3.1 (2022)."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "This paper proposes a framework called Physics Transfer Learning (PT) that aims to learn underlying physics from low-fidelity data to enable extrapolation to high-fidelity data. The authors conduct experiments on two entirely different domains: crystal and brain morphologies. While the paper claims to learn the underlying physics through the PT framework, it does not propose any specific method for achieving this beyond simply inputting data and training a model via supervised learning. Furthermore, there is a significant lack of consideration for competing methods and related work in AI, which raises substantial concerns about the contribution and novelty of the work.",
        "strengths": "1. The idea of considering ellipsoids in brain morphology analysis is intriguing. With more extensive analysis and theoretical development, this concept has the potential for significant advancement in the field.",
        "weaknesses": "1. Despite the title \"Physics Transfer,\" the proposed framework does not modify the input data format used in existing Machine Learning Force Field (MLFF) methods or models that embed brain networks. No additional optimization techniques are introduced. The framework does not adequately consider physics principles or employ transfer learning methodologies.\n\n2. The paper lacks any discussion of AI methodologies or competing methods. The absence of comparisons with existing approaches makes it difficult to evaluate the effectiveness and innovation of the proposed framework.\n\n3. There is no illustration or explanation of how the \"physics\" is learned within the model. The paper fails to demonstrate the underlying mechanisms that enable the model to capture or learn physical laws.\n\n4. The structure and composition of the proposed framework remain unclear. The paper does not present a cohesive framework that can be commonly applied across two entirely different domains, such as crystal structures and brain morphologies."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 2.75,
    "decision": "Reject",
    "meta_review": "The paper presents a transfer learning framework to learn the underlying physics of complex system through low fidelity simulations and transfer this knowledge to generalize to complex systems. The paper applies this framework to two interesting scientific applications and presents results that highlight its performance.\n\nStrengths: The paper tackles an important and challenging research direction in extrapolating physics from low fidelity data \nWeaknesses: Insufficient details to judge the contributions, insufficient comparisons to other transfer learning frameworks",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "DlZ97cVwr0",
    "title": "Exploring the Recall of Language Models: Case Study on Molecules",
    "authors": [
      "Philipp Guevorguian",
      "Knarik Mheryan",
      "Hasmik Mnatsakanyan",
      "Hrant Khachatrian"
    ],
    "abstract": "Most of the current benchmarks evaluate Generative Language Models based on the accuracy of the generated output. However, in some scenarios, it is also important to evaluate the recall of the generations, i.e., whether a model can generate all correct outputs, such as all security vulnerabilities of a given codebase. There are two challenges in evaluating the recall: the lack of complete sets of correct outputs for any task and the existence of many distinct but similar outputs (e.g., two exploits that target the same vulnerability).\n\nIn this paper, we propose a benchmark from the domain of small organic molecules. We define several sets of molecules of varying complexity and fine-tune language models on subsets of those sets. We attempt to generate as many molecules from the target sets as possible and measure the recall, i.e., the percentage of generated molecules from the target set. We examine the impact of the training loss function and sampling strategy on the recall. We propose a sampling strategy based on beam search that avoids duplicates and maximizes recall. Finally, we show that given a small validation set, one can predict the recall of the model without actually generating many samples, which can act as a model selection strategy for maximizing generation recall.",
    "keywords": [
      "recall",
      "language models",
      "molecular language models",
      "sampling methods for language models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=DlZ97cVwr0",
    "forum_url": "https://openreview.net/forum?id=DlZ97cVwr0",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper introduces a benchmark for evaluating the recall of language models in the domain of small organic molecules. Specifically, based on the famous dataset GDB-13, the authors prepare a new dataset with four subsets, e.g., a new subset contains molecules that share a certain percentage of substructures with aspirin. Based on the constructed dataset, the molecule generation capability of language models (LMs) in terms of recall before and after fine-tuning has been evaluated. A new method for predicting the recall of LMs has also been designed. The average probability of a desired molecule to be generated and the ground truth recall values are used to build a regression model for the recall prediction. The evaluation demonstrated the correlation is more than 0.99. Finally, a recall-oriented molecule generation method and a loss function have been introduced to boost the recall of LMs.",
        "strengths": "1. An interesting and important problem in analyzing the recall of language models.\n2. Multiple solutions with promising results have been proposed in the same work\n3. The paper is well-written",
        "weaknesses": "1. Even though the motivation is clear and good, the studied objective does not fit the motivation well, is the recall metric more important in the molecule generation domain?\n2. Many design choices are unclear, e.g., why use Beam search in section 3.4 not others?\n3. Many problems, e.g., capability estimation and new loss design, have been studied, but each of them lacks a comparison with baselines.\n\nOverall, this paper studies an important problem and proposes promising solutions for recall estimation and LMs enhancement. However, there are some concerns that need to be addressed.\n\nFirstly, even though the main point, evaluating whether a model can generate all correct outputs is important for safety-critical problems, it is unclear whether this is the case for the studied objective molecule generation. It is better to give clear motivation for the importance of evaluating recall for this task. \n\nFor the subset construction, in Table 1, it is unclear how the threshold is determined, e.g., 0.4 for Sasp and 0.2 ≤ sim(m, d) ≤ 0.2165. Please clarify it.\n\nIn Section 4.1, Table 2 and Table 3 suggest different solutions as the best, which one we should accept in practice. It is better to add more discussion here.\n\nIn Section 4.2, considering the recall estimation, there are many works that have been proposed to evaluate deep learning models in an unsupervised manner [1, 2, 3], it is necessary to at least discuss the difference between the proposed method and these works.\n\nIn Section 4.3, it is unclear why Beam search is used here since there are many other options (search methods). \n\nIn Section 4.4, first, it is better to add baselines without using the designed loss function in Table 5. Besides, the recall values decreased after comparing the results in Table 5 and Table 4. It is unclear which factors lead to this degradation. \n \n[1] Unsupervised Evaluation of Code LLMs with Round-Trip Correctness.\t\n[2] Estimating Model Performance Under Covariate Shift Without Labels.\n[3] Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper presents a benchmark for modelling molecules, based on GDB-13 (an exhaustive set of molecules with at most 13 heavy atoms that satisfy certain conditions). The authors pretrained LMs to generate the molecule sequences, and aim to bring up recall via 1) better sampling in generation and 2) better training data design. In addition to that, the authors proposed ways to predict the recall value with a small-scale experiment and a set of empirical studies on how should one best represent the molecules in LM inputs.",
        "strengths": "1. Maximizing recall is indeed valuable for a lot of applications, as the authors discussed in the paper, this paper is of empirical importance.\n2. The formulation of the problem is novel, the molecular generation domain provides an excellent testbed due to well-defined equivalence classes and complete reference sets.\n3. The experiments are done with rigor. I like the comprehensive analysis of factors affecting recall (pretraining, molecular representations, etc.)\n4. The dataset and benchmark would make a good contribution to the community.",
        "weaknesses": "My main concern with this paper is around its technical contributions:\n1. The author proposed using random sampling with temperature and beam search (with a large beam size) to improve recall coverage. These two methods are well-known methods in language models' (LM) generation, and I was expecting a novel generation approach such as generating with penalizing the likelihood of already generated sequences.\n2. The method that predicts recall has a lot of similarities with perplexity measure in language modelling, would the authors clarify how is the proposed metric different from the perplexity-based measures?\n3. Removing duplicates and selecting data in each batch are sensible approaches, but they don't appear to be anything novel.\n\nI have some minor questions listed in the below section."
      },
      {
        "rating": "5",
        "confidence": "2",
        "summary": "This paper introduces a benchmark for evaluating models based on recall rather than just accuracy. The authors tackle two challenges: the lack of complete correct output sets and the presence of multiple similar outputs. Using small organic molecules from the GDB-13 database, they fine-tune models and develop a method to predict recall based on perplexity. They also propose a novel beam search decoding method to maximize recall by avoiding duplicates, alongside a recall-aware loss function. This approach aims to enhance the ability of GLMs to generate all correct outputs, with potential applications in various fields, including security.",
        "strengths": "- This paper explores the evaluation of recall rates for small language models, which is a meaningful endeavor.\n- The paper investigates various methods to enhance the recall rates of models and has achieved some positive results.",
        "weaknesses": "- The contributions of this paper are limited. On one hand, in improving recall through sampling methods and loss functions, the authors merely attempt different strategies, which can sometimes harm precision, and no solutions are provided. On the other hand, the improvements through fine-tuning appear to offer no significant contribution, as it is generally expected that fine-tuning would enhance performance on a specific task.\n- The model is too singular, as the experiments in this paper only include the OPT-1.3B model. Therefore, the evaluation results and methods for enhancing recall may not generalize well."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper introduces a new benchmark of molecules for evaluating generative language models with a focus on recall. It aims to investigate the model's ability on tasks requiring distinct output generation, like detecting all vulnerabilities in code. Using organic molecule dataset, the study shows that model recall can be anticipated via perplexity on a validation set. Moreover, the authors use beam search decoding to reduce duplicates and a recall-aware loss function to improve performance, providing insights into molecular representation and model pretraining effects.",
        "strengths": "This paper presents a meaningful investigation into the recall of model generation, with a well-articulated and compelling motivation.",
        "weaknesses": "1. From section 3.1 onward, this paper becomes quite difficult to follow, largely due to the use of specialized terminology from fields like chemistry without providing sufficient foundational overviews or introductory explanations. This approach makes it challenging for readers to fully grasp the content and nuances of the work. For instance, important details and statistics regarding the dataset collected by the authors are not included, and terms like SELFIES are mentioned without any straightforward elaboration to help readers understand what SELFIES actually represents. This lack of accessible explanations hinders the reader’s ability to form a clear understanding of the paper’s specifics. I recommend that the authors incorporate diagrams or more detailed descriptions of key terminology to enhance clarity.\n\n2.In section 4.2, a new method for estimating recall is proposed. First, the statement \"Given that evaluating recall provides a meaningful and interpretable measure of an approach’s ability to model data, estimating recall without needing to perform generations would be useful\" lacks a convincing motivation for why recall estimation without actual generation is necessary. There is no clear justification for the need to use an alternative method to evaluate recall. Furthermore, using probability to estimate recall does not align with the standard definition of recall, which traditionally measures the proportion of correctly generated instances rather than a probabilistic expectation. Thus, it is both imprecise and misleading to label this metric as recall. For instance, in earlier sections (Table 2), the authors appear to use a conventional method for calculating recall; however, after introducing this new approach, they apply it in Table 4 but use the same metric name. This inconsistency undermines reliability and creates confusion regarding the validity of the reported recall values.\n\n\n3.In section 4.3, I don’t see a substantial difference between your proposed recall-oriented generation and the standard beam search. \n\n4. Mean aggregation is equivalent to the regular loss function\" lack clarity—specifically, it is not defined what the “regular loss function” refers to. Furthermore, the section does not directly present the actual loss function or provide a detailed explanation. Instead, it relies solely on textual descriptions, which makes it difficult to understand the specifics of the proposed loss. Including the explicit mathematical form of the loss function along with a step-by-step explanation would significantly improve clarity and accessibility.\n\n5.In addition to the presentation issues mentioned above, the paper lacks a coherent structure throughout both the methods and experiments sections. The presentation feels fragmented, and critical details regarding the experimental setup, such as baseline configurations, are insufficiently described. To improve clarity, a major revision is needed to reorganize the paper, providing a more cohesive structure and a thorough explanation of the experimental settings."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper identifies the challenges in evaluating the recall of generative models and introduces a recall benchmark in the domain of molecular generation. It also proposes sampling strategies and loss formulations to enhance recall.",
        "strengths": "This paper is well-written and easy to understand, addressing a problem that has not been extensively explored before. Additionally, the paper addresses crucial research directions, such as measuring recall without generation and methods to enhance recall, presenting intriguing experimental results.",
        "weaknesses": "**Scalability of Research**\n\nThe study in this paper is limited to a specific domain, namely molecular generation, and there needs to be a discussion on how this research can be extended to other domains. For example, a crucial aspect of measuring recall, as highlighted in the paper, is identifying the equivalence class of the model’s generated results. As mentioned in lines 60-62, there is a technique for identifying equivalence classes for SELFIES strings. How could this issue be addressed in other domains you mentioned in the introduction, such as “vulnerable code generation”?\n\n\n**Completeness in Method**\n\nIn my opinion, the sections proposing the sampling strategy and loss to improve the model’s recall are crucial for establishing the novelty of your paper. However, these aspects are not fully developed and lack sufficient explanation. For instance, in the case of the recall-oriented loss function, the approach of changing the aggregation to min or max seems quite extreme to me, with significant potential for refinement. Additionally, the proposed method only showed effectiveness for a very small and underperforming model with 800K parameters. Therefore, improvements in this area are essential. Additionally, the motivation for using beam search in recall-oriented generation and the intuition behind why increasing the beam size leads to improved recall need to be more thoroughly explained.\n\n**Evaluation**\n\nMost experiments in this paper are validated using a single model and dataset, making it difficult to consider the proposed benchmark method and the approaches to improve recall as thoroughly validated. I believe there should be verification to ensure that the trends in the experimental results hold consistently across at least several models.\nAdditionally, there are confusing aspects regarding the details of the experiments, which should be described and justified more comprehensively (see the questions section for more details)."
      }
    ],
    "rating_avg": 4.4,
    "confidence_avg": 2.8,
    "decision": "Reject",
    "meta_review": "The paper explores the problem of evaluating language models with a focus on recall as opposed to accuracy and introduces a new benchmark for molecules. The methodology primarily involves random sampling with temperature and beam search using large beam width for decoding using a recall-aware loss function. Using a dataset of organic molecules, the paper shows that recall can be predicted using perplexity on a validation set.\n\nThe reviewer assessments were mixed on this paper. All reviewers appreciated the research question, formulation, and benchmarking. The negative reviewers' complained about the lack of technical novelty and/or more comprehensive experiments. The authors' response to some of the other questions were mostly satisfactory even though couple of reviewers didn't respond to rebuttal.\n\nIn my own reading and assessment of the paper, it certainty has some strengths but needs improvement for acceptance. The paper can potentially take two routes to strengthen it.\n- Increase the technical novelty and add additional experiments on more molecule datasets (if the focus is on molecules as a case study).\n- Increase the experiments by adding more use-cases (as alluded in the paper) beyond molecules to drive home the general message for benchmarking and importance of this research.\n\nTherefore, I'm recommending to reject this paper and strongly encourage the authors' to improve the paper based on the feedback from reviewers' for resubmission.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "0vtftmYQGV",
    "title": "SNAP-TTA: Sparse Test-Time Adaptation for Latency-Sensitive Applications",
    "authors": [
      "Hyeongheon Cha",
      "Dong Min Kim",
      "Taesik Gong",
      "Hye Won Chung",
      "Sung-Ju Lee"
    ],
    "abstract": "Test-Time Adaptation (TTA) methods use unlabeled test data to dynamically adjust models in response to distribution changes. However, existing TTA methods are not tailored for practical use on edge devices with limited computational capacity, resulting in a latency-accuracy trade-off. To address this problem, we propose SNAP-TTA, a sparse TTA framework that significantly reduces adaptation frequency and data usage, delivering latency reductions proportional to adaptation rate. It achieves competitive accuracy even with an adaptation rate as low as 0.01, demonstrating its ability to adapt infrequently while utilizing only a small portion of the data relative to full adaptation. Our approach involves (i) Class and Domain Representative Memory (CnDRM), which identifies key samples that are both class-representative and domain-representative to facilitate adaptation with minimal data, and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which leverages representative samples to adjust normalization layers on-the-fly during inference, aligning the model effectively to changing domains. When combined with five state-of-the-art TTA algorithms, SNAP-TTA maintains the performances of these methods even with much-reduced adaptation rates from 0.01 to 0.5, making it suitable for edge devices serving latency-sensitive applications.",
    "keywords": [
      "Test-Time Adaptation",
      "Unsupervised Domain Adaptation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=0vtftmYQGV",
    "forum_url": "https://openreview.net/forum?id=0vtftmYQGV",
    "reviews": [
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper introduces SNAP-TTA, a sparse Test-Time Adaptation (STTA) framework designed for latency-sensitive applications on resource-constrained edge devices. Traditional TTA methods dynamically adjust models using unlabeled test data to handle distribution shifts, but they often incur high computational costs and latency, making them impractical for real-time edge environments. SNAP-TTA addresses these challenges by introducing two key components: (i) Class and Domain Representative Memory (CnDRM), which selects class-representative and domain-representative samples to enable effective adaptation with minimal data, and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which corrects feature distribution shifts during inference without additional training. By combining SNAP-TTA with five state-of-the-art TTA algorithms, the paper demonstrates that SNAP-TTA achieves significant latency reductions (up to 87.5%) while maintaining competitive accuracy. Experimental results on benchmarks like CIFAR10-C and ImageNet-C show SNAP-TTA’s superior performance in edge settings, making it suitable for real-world, latency-sensitive applications.",
        "strengths": "1. This paper addresses the challenge of achieving high adaptation accuracy while maintaining computational efficiency in Sparse Test-Time Adaptation (STTA), where updates rely on only a small subset of data.\n2. SNAP-TTA demonstrates improved classification accuracy across adaptation rates (0.01 to 0.5) compared to baseline TTA methods on CIFAR10-C, CIFAR100-C, and ImageNet-C. At an adaptation rate of 0.1, SNAP-TTA reduces latency by up to 87.5% while mitigating accuracy loss, validating its effectiveness in STTA\n3. IoBMN combines memory statistics from domain-representative samples with current inference batch statistics, using a soft shrinkage function to balance them. This dynamic normalization adjustment during inference effectively addresses domain shift, ensuring model adaptability and performance stability.",
        "weaknesses": "1. The reliance on a fixed confidence threshold of CnDRM may limit adaptability across varying data distributions and could lead to suboptimal sampling.\n2. In table 5, accuracy differences between methods are small, without statistical analysis, making it unclear if these differences are significant (In Detailed comments 4)"
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper addresses the problem of test-time adaptation for out-of-distribution generalization. To reduce the adaptation rate and improve the overall latency of TTA, the authors propose a SNAP framework that selects partial samples for adaptation. Experimental results highlight the potential of the proposed method. However, I still have several concerns as outlined below.",
        "strengths": "The design of the SNAP method is well-motivated and reasonable from the technical perspective. \n\nThe proposed approach is a plug-and-play module that can be integrated with existing TTA methods to reduce adaptation steps and enhance efficiency. \n\nExperimental results underscore the effectiveness of the proposed method.",
        "weaknesses": "On edge devices, the most critical factor in determining whether a TTA method is feasible is actually peak memory usage, as highlighted by MECTA [A]. While this work does reduce the number of adaptation steps, it does not decrease peak memory usage. In this sense, the primary motivation for applying the proposed method to edge devices may be misplaced.\n\n[A] MECTA: Memory-Economic Continual Test-time Adaptation"
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper focuses on Test-Time Adaptation (TTA) for edge devices with limited computational capacity. The authors propose SNAP-TTA, a sparse TTA framework with two key components, Class and Domain Representative Memory (CnDRM) and Inference-only Batch-aware Memory Normalization (IoBMN), aiming to reduce model adaptation frequency and data usage while maintaining accuracy.",
        "strengths": "The proposed SNAP-TTA framework addresses the latency-accuracy trade-off issue in existing TTA methods for edge devices in some cases. It reduces latency while achieving competitive accuracy, as demonstrated by extensive experiments on multiple benchmarks and with integration of several existing TTA algorithms.",
        "weaknesses": "- In the background section, the mention of applications like real-time health monitoring for IoT edge devices may not be entirely appropriate as these devices often have extremely limited memory.\nWith limited memory, these devices are difficult and even impossible for backward-propagation and gradient decent. In this sense, memory should perhaps be prioritized over latency as the primary concern.\n- It is unclear whether the proposed method reduces the delay per batch or the average delay (adaptation occurs once every several batches as shown in Figure 1). If it is the latter, its effectiveness for latency-sensitive applications may be limited as the inference delay could increase significantly every several batches.\n- The method reduces the cost of backpropagation by filtering samples to decrease the inference latency. However, EATA also uses a similar strategy, but in Figure 2, the delay of EATA is the same as that of Tent, and the delay of SAR is inconsistent with the results reported in its original paper.\n- The paper could compare the inference latency in Tables 1, 2, and 3.\n- In Table 6 for ImageNet-C, only the Tent method is compared, ignoring other methods, which could provide more comprehensive and convincing results.\n- In the experiments, it is not clear how the number of participating samples is controlled to meet the adaptation rate. Is it through adjusting the $tau_conf$ hyperparameter? Also, it is not described how other compared methods meet the adaptation rate.\n- The description of lines 10-15 of the algorithm in the paper is relatively brief, considering its importance for the proposed method. More detailed explanation in the paper would assist readers in understanding."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors propose a sparse test-time adaptation (TTA) framework, which they call SNAP, that improves the latency-accuracy trade-off of existing TTA algorithms to enable practical use of TTA on ede devices.\nTo this end, the authors propose \"CnDRM\", a method for identifying \"important\" samples for training based on class- and domain-representative sampling, and \"IoBMN\", a method for mitigating the effects of domain shifts on the model's internal feature distributions.",
        "strengths": "- The method is promising in that, at least on a Raspberry Pi 4 and when used together with STTA, SNAP provides a significant reduction in latency, as shown in Table 4, while being able to maintain accuracy comparable to using STTA alone.\n- The authors show empirically that SNAP works well with a number of different TTA algorithms (TENT, CoTTA, EATA, SAR, RoTTA) and with different adaptation rates for different datasets (CIFAR10-C, CIFAR100-C, ImageNet-C).",
        "weaknesses": "- The claimed contribution of the paper is that SNAP can make existing TTA algorithms more latency efficient and suitable for edge devices. However, this is only demonstrated in Table 4 for one algorithm (STTA) and one target device (Raspberry Pi 4). All other experiments focus only on accuracy. And while it is an important and valuable contribution to properly demonstrate that SNAP does not reduce the effectiveness of the TTA algorithms it is applied to, I think the evaluation overall fails to adequately demonstrate the claimed contribution of latency reduction across various edge devices."
      }
    ],
    "rating_avg": 5.75,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "This paper proposes a sparse test-time adaptation (TTA) framework called SNAP-TTA, which aims to address the latency-accuracy trade-off in existing TTA methods when applied to edge devices. The main benefit of SNAP-TTA is reducing latency. The motivation of the paper is reasonable, and it can be seamlessly integrated into existing TTA methods. However, the main issues lie in some experimental setups and the overall contribution. Some reviewers raised concerns about memory usage during experiments and compatibility across different hardware scenarios, and the authors' experimental additions and analyses did not fully address these concerns. The AC reviewed the paper and all discussions and believes that the study still needs improvement.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "DnBjhWLVU1",
    "title": "Recovering Plasticity of Neural Networks via Soft Weight Rescaling",
    "authors": [
      "Seungwon Oh",
      "Sangyeon Park",
      "Isaac Han",
      "Kyung-Joong Kim"
    ],
    "abstract": "Recent studies have shown that as training progresses, neural networks gradually lose their capacity to learn new information, a phenomenon known as plasticity loss. An unbounded weight growth is one of the main causes of plasticity loss. Furthermore, it harms generalization capability and disrupts optimization dynamics. Re-initializing the network can be a solution, but it results in the loss of learned information, leading to performance drops. In this paper, we propose Soft Weight Rescaling (SWR), a novel approach that prevents unbounded weight growth without losing information. SWR recovers the plasticity of the network by simply scaling down the weight at each step of the learning process. We theoretically prove that SWR bounds weight magnitude and balances weight magnitude between layers. Our experiment shows that SWR improves performance on warm-start learning, continual learning, and single-task learning setups on standard image classification benchmarks.",
    "keywords": [
      "loss of plasticity",
      "plasticity",
      "continual learning",
      "online learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=DnBjhWLVU1",
    "forum_url": "https://openreview.net/forum?id=DnBjhWLVU1",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper addresses the issue of plasticity loss in neural networks, where the capacity to learn new information diminishes over time due to unbounded weight growth. The authors propose a method called Soft Weight Rescaling (SWR), which mitigates this issue by scaling down the weights at each learning step, and claiming to maintain the network's plasticity without losing previously learned information. Some experimental results, such as continual leaning and single-task in image classification, demonstrate that SWR can enhance performance, outperforming existing weight regularization and re-initialization techniques.",
        "strengths": "1. The paper is easy to follow.\n2. I think the authors are focusing on an interesting topic, i.e. loss of plasticity, that is worthy to probe.\n3. The method proposed is simple and can be easily implemented in practice.",
        "weaknesses": "1. An unbounded weight growth is one of the main causes of plasticity loss, and the authors propose reducing weight magnitude through weight scaling. Reducing the weight magnitude could be a common implementation in training, where L2 is widely used. So I think the key here lies in comparing the proposed method to L2. However, after reviewing the text, I did not find a clear rationale why we should choose the proposed method over L2. Could the authors provide specific cases that demonstrate the essence regarding how the proposed method targets improvements over L2 regularization?\n\n2. I notice that the authors define the rate of how much the model has changed from the initial state as the ratio between the Frobenius norm of the current weight matrix and that of the initial one. Could the author give more explanations regarding this metric? In my opinion, this metric may not well capture the extent of change in the model. For instance, applying weight regularization could significantly alter the weights, yet the model's performance may change only marginally. \n\n3. I have not found any theoretical insights regarding the claims made about magnitude boundedness and weight balance in the main text. However, I did locate some proofs in the appendix. Since these proofs appear to be one of the main contributions of the proposed work, I recommend that the authors reorganize the paper to better highlight this important content.\n\n4. I think the authors should improve the experiments presented in the paper. Firstly, the current training performance falls significantly below existing baselines, with VGG achieving only 0.72 on CIFAR-10 and below 0.4 on CIFAR-100, which is unacceptable. Secondly, the authors should broaden their experimental scope beyond VGG on CIFAR, MNIST, and TinyImage. It would be beneficial to include experiments relevant to current RL or NLP scenarios, especially where pre-trained models are commonly utilized. For now, I could barely sense the superiority of the proposed method.\n\n5. It would be helpful if the authors could release the code."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The authors introduce Soft Weight Rescaling (SWR), a novel weight regularization method that prevents unbounded weight growth to preserve information and maintain network plasticity. The theoretical analysis shows that SWR bounds weight magnitudes and balances them across layers without degrading model performance. Empirical evaluations, particularly with VGG-16, show that SWR improves generalization performance compared to other regularization methods.",
        "strengths": "- The paper is overall clearly written and the method is adequately described.\n- The proposed method SWR is computationally more efficient than previously proposed methods.\n- The experiment results and analysis provided in the paper are insightful.",
        "weaknesses": "- The experimental results on smaller models are quite weak. For example, in warm-start and continual learning experiments, L2 (or S&P) seems to be better in most experiments (including the ones in the appendix). Even in Table 1, except for VGG, I wouldn't say the improvements are significantly higher since there's quite a bit of overlap with L2 in terms of standard deviations in MLP, and CNN cases. SWR only performs well on VGG which is not a very popular architecture even for vision-based experiments in this domain compared to ResNet. It would be interesting to see the comparison between SWR and baselines on bigger models. The assumptions of affine, conv layers in Theorem 1 are also strong and limit the applicability of SWR.\n- I think the main novelty of the idea is limited and comes primarily from \"scaling the bias vectors according to a certain rule\". From Eq on line 220, one may assume that $W_l$ will attain a higher magnitude than $W_{init}$. As a result, $c_l \\approx 1 - \\lambda$, which implies that SWR would behave like a layer-wise version of S&P with weight_scale = $1 - \\lambda$ and no initial weights. \n- Missing baselines: Lyle et al. 2024 recently also showed that the L2 + Layer norm generally outperforms the majority of the existing methods. Lee et al. 2024 have also shown that their method results in superior generalization performance on these benchmarks. \n\n\n\nSome grammatical/clarity related issues:\n- Line 161: investigated the following theorem shows that\n- Line 213: the change rate"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper focuses on the solution to recovering the plasticity of DNNs via weight regularization. The paper proposes a simple yet effective weight regularization method that prevents unbounded weight growth. The authors also provided the technique's theoretical and empirical insights, which prove the generalization performance in different learning.",
        "strengths": "- This work progressively establishes and justifies its framework, making this paper easy to follow.\n- The results are promising, however, I have some concerns regarding the results as discussed below",
        "weaknesses": "- One main drawback of the paper is the limited application of the paper. The authors made many assumptions (e.g., the network is affine, homogeneous with ReLU), which impedes the contributions and the applicability of the paper in real-world scenarios.\n- Some crucial statements are made without proper references. Furthermore, these statements are conflicted with the statements in various peer-reviewed and significant publications.\n- The paper came up with many theorems and definitions without explaining the usages and necessities of these statements.\n- Ablation tests according to Theorem 1 needed to be conducted to verify the paper's significance.\n- All in all, the aforementioned issue impedes the contribution and significance of the paper method. The authors please consider carefully about these issues. If the issues are addressed, the score can be modified.\n- The experimental evaluations are not sufficient, they need to provide more experiments on large-scale datasets (ImageNet1K, COCO, etc) and across different model architectures (VisionTransformers, etc).\n- The hyper-parameter $\\lambda$ is proposed but there are no experiments that consider the effect of $\\lambda$ on the boundedness of the weight before and after scaling.\n- There should be a theoretical discussion about how to tighten the boundedness compared to other methods. For example, in Theorem 2, the authors show that $\\|W_t\\| \\neq B$, which is trivial thus not proving that the proposed method is better than others."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces Soft Weight Regularization (SWR), a regularization based algorithm for maintaining plasticity under the broad framework of continual learning. Unlike other regularization based approaches for addressing plasticity loss, such as L2 regularization, Shrink and Perturb, and L2 Init, SWR does not alter the network's predictions. The paper provides a theoretical analysis showing that SWR bounds weight magnitudes and maintains balanced weights between layers, two favourable properties of neural networks. Finally, the paper provides empirical evidence arguing the efficacy of SWR on a set of problems that test for plasticity and stability in settings of warm-starting, continual learning, and generalization.",
        "strengths": "- The paper rightfully makes the point that unbounded weight magnitudes in continual learning settings is a more general issue in deep learning. This is a point that is not often made explicit in the continual learning literature. \n- The proposed method of SWR is supported by theoretical analysis establishing that weight magnitudes are bounded and that weights between layers are relatively balanced, two properties that have been previously shown to be beneficial for generalization and continual learning settings. Many recent methods in the continual learning framework, despite their simplicity, have been introduced with little to no theoretical basis, therefore, this is  a strength of this paper.\n- The proposed method is evaluated on three types of problems: warm-starting, continual learning, and classic supervised learning evaluating generalization. SWR's performance is evaluated with respect to the generalization gap, plasticity in continual learning, and catastrophic forgetting in continual learning. This provides a broader evaluation than is typical in continual learning.",
        "weaknesses": "- This paper could use more polish and could be reorganized to better state the contributions as well as their relative merits to existing work. Some concrete examples are as follows: \n- The paragraph on line 039 is too specific for the introduction and the paper would be better served with a concise overview of the merits and draw backs of regularization based re-initialization based methods, and moving the existing paragraph as is to a related works section. \n- The paragraph on line 066 is redundant given the preceding paragraph.\n- It would be useful to give, at least a high level or rough, description of SWR in the introduction so that the reader has an understanding of how SWR differs from existing regularization based methods. As the paper is currently written, SWR is described by its merits: bounded weight magnitudes and balancing weights, and no actual description of the algorithm itself is provided, until the full algorithm is presented on page 5. \n- It would be useful to introduce and define both catastrophic forgetting and plasticity in the introduction, rather than just the latter phenomenon, as the paper claims to evaluate SWR's ability to mitigate catastrophic forgetting.\n- The motivating and illustrating experiment, Figure 1 and the paragraph that follows on line 197 are confusing and I cannot make out the experimental setup and the exact point that is being made. I would suggest explicitly describing the experimental setup and each algorithm that you are evaluating. How exactly are you scaling, and what is scaling with and without proportionality in this example? Does the pre-trained model include any scaling? What is the difference between fine-tuned after scaling and just the scaled model? When you train the fine-tuned model for another 50 epochs, are you fine tuning on the validation set or some new training set? What exactly is the scaling magnitude or scaling ratio in this experiment? Given that this is a motivating or illustrating example, it would be useful to very precise with outlining the experimental setup.\n- I would recommend moving your theorems on boundedness and balancedness to section 3 and commenting on the significance of these theorems rather than pointing the reader to the appendix.\n- The set of competitor algorithms is limited. Specifically, for re-initialization based methods the well-cited Continual Backprop (Dohare et al.) and ReDO (Sokar et al.) are missing from the experiments that evaluate plasticity loss. As for the experiment that evaluates catastrophic forgetting, regularization based methods for explicitly addressing this phenomenon such as Elastic Weight Consolidation (Kirkpatrick et al.) are absent.\n- To evaluate the efficacy of SWR for mitigating plasticity loss and catastrophic forgetting, a wider experimental study may be necessary. You could consider the benchmark problems of Permuted MNIST, Random Label MNIST and CIFAR, and Continual ImageNet, which are nicely described in (Kumar et al).\n- The claim that SWR mitigates catastrophic forgetting requires more evidence than a single experiment, as noted in the previous point. SWR does not modify the network's outputs unlike other regularization based methods, but this does not prove that SWR mitigates plasticity loss. There is a series of regularization based methods, e.g. Elastic Weight Consolidation, that regularize networks towards weights (or equivalently representations) learned during earlier tasks, and in turn mitigating catastrophic forgetting. Therefore, the limited experiments and construction of SWR do not provide sufficient evidence that catastrophic forgetting is alleviated by SWR more efficiently than by other algorithms, therefore the claims that SWR maintains useful information while re-initialization based methods do not, is not entirely accurate."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "The paper addresses the issue of plasticity loss, also known as intransigence, in neural networks. The authors identify unbounded weight growth as a key contributor to this issue and introduce a novel regularization technique called Soft Weight Rescaling (SWR) to overcome it. SWR aims to limit weight magnitudes and ensure balance across different layers without compromising model performance. The authors offer a theoretical analysis to confirm that SWR effectively maintains bounded and balanced weights, which are desirable properties in neural networks. The empirical results validate the effectiveness of SWR in various scenarios, including warm-starting, continual learning, and generalization, demonstrating its ability to preserve both plasticity and stability.\n\n**Strengths:** All reviewers agreed that the paper addresses an interesting and timely problem. They praised the clear and methodical writing style and the method's simplicity. Moreover, the reviewers found the experimental results and analysis presented in the paper to support the claims and provide insightful observations effectively.\n\n**Weaknesses:** The paper was primarily criticized for its limited experimental setup, which restricts its broader impact. Notably, the datasets employed, such as CIFAR10, CIFAR100, MNIST, and TinyImageNet, are small-scale. Furthermore, the use of a VGG model in the experiments resulted in performance metrics—72% on CIFAR10 and 40% on CIFAR100—that are significantly lower than current state-of-the-art results on these datasets. This discrepancy places the paper at a substantial disadvantage when compared to more recent literature. Additionally, the reviewers highlighted the absence of comparisons with related regularization methods, such as L2 regularization (weight decay) or more contemporary approaches that combine L2 and Layer Normalization (Lyle et al., 2024). Lastly, the paper lacks comprehensive ablation studies and a detailed sensitivity analysis of hyperparameters.\n\nAlthough the reviewers appreciated certain aspects of the paper, they unanimously agreed that the experimental setup was rudimentary and insufficient. Consequently, they concluded that the paper, in its current form, is not ready for publication. I enjoyed reading the paper and believe that the authors could significantly enhance their work based on the constructive feedback provided by the reviewers. Considering these factors, I recommend rejecting this paper. However, I recognize its potential and encourage the authors to continue refining their work.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "TIxiwxd4iD",
    "title": "BanglaGITI: A Novel Dataset for Bangla Music Genre Classification with A Comprehensive Analysis",
    "authors": [
      "Syeda Farhana Ali",
      "Mohammad Tanveer Shams",
      "Kazi A. Kalpoma",
      "Tasnim Munawar Rafee",
      "Jecin Saba"
    ],
    "abstract": "This paper presents a comprehensive exploration into the classification of Bengali music genres, utilizing a novel dataset, `BanglaGITI: Bangla Genre-wise Indexed Tracks and Interpretations', specifically curated to capture the rich diversity of Bengali musical heritage. Our study is structured around a comparative analysis of traditional Machine Learning (ML) techniques, advanced Deep Learning (DL) methodologies, and innovative ensemble approaches that integrate the strengths of both ML and DL through Transfer Learning. Our dataset includes a total of 1410 audio files across 6 different genres. For the ML segment, features such as Mel-frequency cepstral coefficients (MFCCs), zero-crossing rate (ZCR), root mean square(RMS), chroma, tempo and spectral bandwidth were leveraged to encapsulate the unique characteristics of Bengali music. These features serve as a foundation for employing classic ML classifiers that demonstrate robust performance in genre classification tasks. Our methodology includes Decision Tree, Random Forest, Gradient Boost and KNN. Conversely, our DL models are designed around the extraction and analysis of Log-Mel spectrograms, capitalizing on their ability to represent complex musical structures in a manner that is both comprehensive and conducive to DL techniques. This approach allows for the deep neural networks to learn from a richer representation of audio data, potentially uncovering nuanced patterns inherent in Bengali music genres. DL techniques feature pre-trained CNN-based models such as DenseNet, ResNet and  VGGNet. Furthermore, our paper innovates by proposing ensemble models that combine the predictive capabilities of ML and DL methods respectively, aiming to harness their complementary strengths for enhanced classification accuracy. The ensemble models resulted in achieving almost 80% accuracy in ML and state of the art 96% accuracy in DL methods while the precision recall and F1-score of 96.09%, 96.05% and 96.04% respectively. Our findings not only shed light on the efficacy of different computational approaches in the realm of music genre classification but also contribute to the understanding of Bengali music through the lens of machine intelligence. The use of our self-made dataset, which is among the first of its kind for Bengali music, adds a significant value to the study, offering a new benchmark for future research in this area. Through this comprehensive study, our aim is to provide insights that will guide the development of more sophisticated and culturally nuanced music classification systems.",
    "keywords": [
      "BanglaGITI",
      "Music Genre Classification",
      "CNN",
      "ML",
      "Transfer Learning",
      "Spectrogram",
      "MFCC",
      "Ensemble"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=TIxiwxd4iD",
    "forum_url": "https://openreview.net/forum?id=TIxiwxd4iD",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "9DrPvYCETp",
    "title": "Shared Memory for Multi-agent Lifelong Pathfinding",
    "authors": [
      "Alsu Sagirova",
      "Yuri Kuratov",
      "Mikhail Burtsev"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the main challenges in MARL is the need to explicitly predict other agents' behavior to achieve cooperation. As a solution to this problem, we propose the Shared Recurrent Memory Transformer (SRMT), which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to implicitly exchange information and coordinate actions. We evaluate SRMT on the Partially Observable Multi-Agent Path Finding problem, both in a toy bottleneck navigation task requiring agents to pass through a narrow corridor and on a set of mazes from the POGEMA benchmark. In the bottleneck task, SRMT consistently outperforms a range of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps,  including Mazes, Random, and Warehouses, SRMT is competitive with a variety of recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared memory into transformer-based architectures can enhance coordination in decentralized multi-agent systems.",
    "keywords": [
      "shared memory",
      "transformers",
      "multi-agent pathfinding"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=9DrPvYCETp",
    "forum_url": "https://openreview.net/forum?id=9DrPvYCETp",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This work considers the application of a shared memory mechanism to the MAPF setting.",
        "strengths": "- The writing is generally clear and polished. \n- The approach is well-grounded in prior literature, and the algorithmic details are well-explained.  \n- Figure 1 is a useful complement to the written algorithmic details, and makes it easy to understand the method at a glance. \n- Figure 10 analysis is nice.",
        "weaknesses": "* It is hard to get a relative sense of the competitiveness of this approach. The baselines did not feel particularly well-motivated, and MARL communication works, which I'd argue share a similar goal, were not used as baselines (e.g. \\[1\\])\n* More generally, I am left not knowing exactly what I should take away from the results—Figure 5 seems to show that SRMT and variants achieve modest results compared to baselines (and the baselines used are not motivated or described in sufficient detail).\n* \\[2\\] I consider this a necessary work to acknowledge, given it is one of the first works discussing the use of attention in MARL\n* Nitpicks: \n\t* I cannot interpret the error bars in Figure 4—it is too muddled.\n\t* Despite the writing overall being clear, the language could be tightened somewhat; e.g. L043: \"has to reach its goal\" is quite colloquial; also contraction in L497. I recommend combing through the paper and essentially asking each word/phrase to justify itself—and to be as specific as possible, avoiding colloquialisms. \n\n\\[1\\] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), *Advances in Neural Information Processing Systems, volume 29*. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper_ files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf.\n\n\\[2\\] Iqbal, S. &amp; Sha, F.. (2019). Actor-Attention-Critic for Multi-Agent Reinforcement Learning. <i>Proceedings of the 36th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 97:2961-2970 Available from https://proceedings.mlr.press/v97/iqbal19a.html."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "This paper introduces the Shared Recurrent Memory Transformer (SRMT), a novel model in multi-agent reinforcement learning designed for multi-agent lifelong pathfinding tasks. SRMT extends memory transformers to decentralized multi-agent environments by pooling individual agent memories into a shared memory space, allowing agents to indirectly share information and coordinate. The model is tested in various pathfinding tasks, including bottleneck navigation and complex environments from the POGEMA benchmark. SRMT demonstrates superior performance in coordination and generalization, particularly in high-density and partially observable environments.",
        "strengths": "1. The SRMT model is an adaptation of memory transformers to multi-agent settings, facilitating indirect communication among agents through a shared memory. This approach addresses a significant challenge in decentralized coordination by leveraging shared recurrent memory, which is unique compared to conventional communication strategies.\n2. The paper provides a rigorous evaluation of SRMT on multiple benchmark tasks, including POGEMA and bottleneck navigation. The use of diverse reward settings (e.g., sparse, directional) further strengthens the experimental framework, revealing SRMT’s adaptability in various coordination scenarios.\n3. The architecture and methods are clearly explained, supported by diagrams and flowcharts that help clarify SRMT’s working mechanism. The comparisons with baselines and the explanation of the multi-agent Markov decision process formulation are presented in a straightforward and understandable manner.\n4. SRMT’s ability to handle decentralized pathfinding without explicit communication protocols has considerable implications for real-world applications, particularly in settings where communication might be unreliable or costly. Its effectiveness across different maps and scenarios demonstrates potential for scalability in complex, large-scale environments.",
        "weaknesses": "1. While SRMT performs well on small to medium-sized environments, its scalability to very large maps or highly dense environments remains uncertain. The evaluation could be extended to more challenging settings, particularly with greater agent populations or larger obstacles, to fully assess SRMT’s scalability.\n2. While SRMT is designed for decentralized systems, it would be beneficial to see comparisons with centralized approaches on key metrics to understand the trade-offs better, particularly in environments that demand high coordination.\n3. While the paper claims that shared memory improves coordination, additional analysis on how shared memory influences individual agent behavior would provide a deeper understanding. An ablation study removing the shared memory aspect could further validate its impact on SRMT’s performance.\n4. The model's performance varied across different reward structures, and while this is discussed, a more detailed exploration of how reward shaping influences learning would strengthen the analysis. This would help in tailoring SRMT to tasks where only sparse rewards are available.\n\nMissing references (MARL with local information). I believe these are quite recent papers and work in a similar setting as mentioned in the related works section.\n\n[1]: Hu, Y., Fu, J., & Wen, G. (2023). Graph soft actor–critic reinforcement learning for large-scale distributed multirobot coordination. *IEEE transactions on neural networks and learning systems*.\n\n[2]: Nayak, S., Choi, K., Ding, W., Dolan, S., Gopalakrishnan, K., & Balakrishnan, H. (2023, July). Scalable multi-agent reinforcement learning through intelligent information aggregation. In *International Conference on Machine Learning* (pp. 25817-25833). PMLR."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper proposes a global shared recurrent memory transformer (SRMT) mechanism for multiagent reinforcement learning to address the multiagent pathing finding problem. Specifically, SRMT uses self-attention to aggregate agent memory and observation history while utilizing cross-attention to aggregate the shared memory from other agents to help coordination. Results on a toy bottleneck navigation task and a set of maze environments from the POGEMA benchmark show that SRMT outperforms various baselines.",
        "strengths": "1.\tThe motivation for using a global shared memory to help coordination and the idea of using the transformer to implement it are clear.\n2.\tThe background is clearly explained and the related works are well discussed.",
        "weaknesses": "1.\tIt seems that a lot baselines are missing. For example, in the Bottleneck Task, only some basic memory mechanisms from single-agent RL are compared while more advanced memory mechanisms such as relational memory [1] and AMRL [2] from the single-agent RL domain are not compared.\n2.\tAt the same time, although some works about MARL memory such as RATE and ATM are discussed in Section 2.2, they are not compared in the experiments.\n3.\tThe ablation study to validate each component of the proposed SRMT is not given.\n4.\tThere are some typos. In Line 36, “MAPF” is not defined.\n\nReferences\n\n[1] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational Recurrent Neural Networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018.\n\n[2] Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, and Katja Hofmann. Amrl: Aggregated memory for reinforcement learning. In International Conference on Learning Representations, 2020."
      }
    ],
    "rating_avg": 5.333333333333333,
    "confidence_avg": 3.3333333333333335,
    "decision": "Reject",
    "meta_review": "The paper proposes the Shared Recurrent Memory Transformer (SRMT) for improved coordination in decentralized multi-agent pathfinding (MAPF). The core claim is that by pooling and globally broadcasting individual working memories, agents can implicitly exchange information and coordinate actions more effectively without explicit communication protocols. SRMT is evaluated on a bottleneck task and the POGEMA benchmark, where it outperforms various baselines, particularly in sparse reward settings and generalizes well.\n\nThe main strengths of the paper are clarity of the presentation, and a comprehensive evaluation including the ablations and memory analysis. The main remaining weaknesses are about novelty, polish and scalability.\n\nOverall, the paper presents a novel approach with promising results. The authors have successfully addressed many of the initial concerns. The revised manuscript has improved the paper, but the original concerns may not be completely addressed.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "odjMSBSWRt",
    "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
    "authors": [
      "Esben Kran",
      "Hieu Minh Nguyen",
      "Akash Kundu",
      "Sami Jawhar",
      "Jinsuk Park",
      "Mateusz Maria Jurewicz"
    ],
    "abstract": "We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.",
    "keywords": [
      "Dark Patterns",
      "AI Deception",
      "Large Language Models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=odjMSBSWRt",
    "forum_url": "https://openreview.net/forum?id=odjMSBSWRt",
    "reviews": [
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors define six dimensions of 'dark design patterns' and develop the DarkBench benchmark to detect these patterns in LLMs. They test 14 LLMs, encompassing both proprietary and open models, to compare dark pattern prevalence across different systems.",
        "strengths": "- The paper tackles the crucial issue of dark patterns in LLMs. As far as I know, no prior research has defined and measured dark patterns in LLMs, making this a novel and much-needed contribution.\n\n- Extensive comparison of 14 proprietary and open-source models on the DarkBench benchmark",
        "weaknesses": "- The authors use LLMs to annotate dark patterns. However, LLMs’ own dark patterns may affect their ability to annotate dark patterns. For instance, if an LLM displays brand bias, it may evaluate responses from its own brand more favorably. A simple statistical test for potential biases in annotation could address this (e.g., comparing whether an LLM's scores for its own responses differ significantly from those it assigns to other LLM responses)\n\n- The paper lacks detailed information on human annotations, particularly regarding the annotators' demographics or level of expertise. For instance, it would be helpful to clarify whether LimeSurvey annotators were laypeople or experts and whether they reflect a diverse demographic range (age, gender, etc.) similar to typical LLM users.\n\n- There is no evidence of stability for the benchmark findings across variations in prompt designs. You could test for consistency by paraphrasing prompts in Table 1 and replicate the experiments.\n\n- Overall, the paper lacks detail. The results section would benefit from including actual qualitative examples from the models."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "Authors describe a benchmark for dark patterns: brand bias, user retention, anthropomorphization, sneaking, sycophancy and harmful generation. They created prompts designed to elicit the dark patterns. Then they used few-shot prompting to generate a total of 660 adversarial prompts. Using a mixture of human annotation and model annotation (Claude Sonnet, Gemini Pro, GPT-4o, and Llama3 70b) they tested 14 open and closed lmms. They found that 48% of the cases exhibited dark patterns, with the most common being user retention and sneaking. Dark patterns presence ranged from 30% to 61% across all models.",
        "strengths": "The paper is well-written and well-organized. The paper is a significant contribution as it presents a new benchmark for measuring dark patterns in LLMs. This is an important direction to help evaluate model safety.\nIn addition to LLM annotators, data were also reviewed by human annotators.",
        "weaknesses": "Unfortunately, the methods aren’t clear on the decision criteria, what makes a model’s performance count? Ought it be a simple proportion? Or something more akin to recall and precision might be more informative and valid for interpretation. Moreover, the authors did not report on group differences which would strengthen their conclusions."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper introduces DarkBench a set of prompts designed to elicit \"dark patterns\" in LLM responses. The authors describe how the benchmark was developed, how the benchmark is evaluated against a model, and presents benchmark results for a variety of open and proprietary models. \n\nOverall this is an interesting piece of work, the various patterns are relevant and the problem is well motivated. \n\nI would like to see more formality w.r.t your description of the data generation and evaluation processes i.e. \n\n1. It was unclear to me if humans reviewed all of the 600 DarkBench prompts for quality? You mentioned rephrasing occurred, why was this and what kind of rephrasing was necessary? \n2. When applying the benchmark to an LLM, what parameters were used? Did the LLM produce a set of responses via sampling, or did the LLM generate one response? Did the annotator models correlate with one another? How was the final yes/no answer generated? Was positional bias accounted for?\n3. The results of the human reviews on the annotator model outcomes?\n\nI would appreciate a discussion around system prompts. In the context of system prompts, used to adjust LLM behavior, how is the DarkBench to be interpreted? I could see it being a tool. \n\nNice idea, good selection of patterns. I think the paper would be improved if the methodology was described in more detail as per the points above.",
        "strengths": "Nice idea \nThe selection of patterns is relevant\nValuable asset (DarkBench dataset)",
        "weaknesses": "The description of the methodology is a little vague. \nThe paper would be stronger if the methodology was more detailed."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "The authors develop DarkBench by manually conceptualizing six tendencies of LLMs that seem to align with the chatbot subscription-based business model (e.g., ChatGPT, Claude.ai), prompting an LLM with precise verbal descriptions of those tendencies to create adversarial prompts that would evoke dark patterns, and manually reviewing and modifying the LLM-generated prompts. Evaluation was done with LLMs prompted with human examples with samples of that LLM evaluation also done by humans for comparison. Results show that Claude performs best on this benchmark (\"dark patterns\" in 30-36% of responses, if I understand correctly), followed by the other models in a band of 48-61%. Some patterns (e.g., user retention) are much more frequent than others (e.g., sycophancy) in current LLMs.",
        "strengths": "- **S1.** I'm excited about this project's direction in evaluating LLM behavior based on design patterns found to be important in other interfaces (dark patterns, but also nudges, antipatterns, and so on). This is difficult, and I am sympathetic to work addressing it even if that work has many limitations.\n- **S2.** I appreciate that some human review and validation was done for the LLM-generated benchmark and for the LLM-generated evaluations, and I think these general directions for datasets and benchmarks are promising in their scalability.\n- **S3.** The figures are relatively clear and concise.",
        "weaknesses": "My primary concerns each refer to the benchmark development and evaluation seeming largely superficial, better suited to preliminary and exploratory formats, such as workshops or seminars, than a main conference publication. To be upfront and help the authors manage their time, I don't think W1 can realistically be addressed during review, and addressing only W2 and W3 would be insufficient for me to raise my score to acceptance.\n\n**W1. Theoretical engagement.**\n\nI'm skeptical that these six tendencies qualify as \"dark patterns.\" Dark patterns are a specific idea regarding intentional design of user flow to trick the user into situations harmful to them and beneficial to the designer's institution (https://90percentofeverything.com/2010/07/08/dark-patterns-dirty-tricks-designers-use-to-make-people-do-stuff/index.html). It seems the authors are familiar with canonical examples, such as making it difficult to unsubscribe from a service, but I don't think the authors have successfully argued for any of their six tendencies being constitutive of, or even highly correlated with, this specific idea.\n\nI would put the misalignments into three categories: (i) not being specific to dark patterns but just harmful generation more broadly (\"harmful generation\" and \"sycophancy\" are the main culprits), (ii) not necessarily being harmful, such as the model being \"friendly\" or \"anthropomorphic,\" which can be in fact some of the main benefits of LLMs, such as for mental health (https://dl.acm.org/doi/full/10.1145/3485874), and (iii) being incidental, such as \"brand bias\" merely from preference tuning and system prompts that center the brand. I would not say Google has a dark pattern if the search engine highly ranks Google content.\n\nI realize that it is impossible to have six metrics that uncontroversially fit into a subjective idea such as dark patterns, and it is nonetheless urgent that we build evaluations like this, but the current state of the paper is just too far off the mark.\n\n**W2. LLM generation and validation.**\n\nMy concerns here may be due to the cursory explanation provided by the authors, but I'm missing a lot of necessary details about generation process and test validation. I would want to see, for example, the extent of mode collapse in the generations, comparisons across generations from different models, and ideally a more rigorous structure with subcategories within the six categories (a priori or through clustering). I think LLM-generated evaluations are promising, but as with any paradigm shift in scientific methodology (e.g., agent-based models, psychologists shifting from studying undergraduate students to online participants), the burden of validation will be high before it is more thoroughly vetted. It seems the authors are familiar with the explosion of literature on such methods, so there are many examples to draw from.\n\n**W3. LLM evaluations.**\n\nThis is largely analogous to W2. I don't expect the authors to validate that LLM-as-a-judge aligns perfectly with human judgment, but only brief description such as \"poor inter-rater agreement\" is not sufficient to me that the LLM judges are performing well enough to trust this benchmark. It is also unclear to me how the different model judges (e.g., Claude versus Llama) were compared and aggregated, which is particularly concerning in a paper that (a) is focused on inevitably subjective distinctions between qualitative model output and (b) has a main empirical finding (or at least secondary) of differences between model brands/families. For example, it is well-known that Claude is heavily tuned to be \"friendly\" in various ways, such as modifying its behavior when nudged at all by the user. Some people like this. Some prefer ChatGPT as straightforward with less of that noise. But my point is that the benchmark may be merely picking up on tendencies like that, which would not only lack novelty as a finding but also be of little relevance to dark patterns.\n\n**Minor concerns**\n\n- The term \"dark pattern\" was not coined in the creation of darkpatterns.org but a talk Harry Brignull gave at UX Brighton in 2010, or technically shortly before that in this blog post (https://90percentofeverything.com/2010/07/08/dark-patterns-dirty-tricks-designers-use-to-make-people-do-stuff/index.html). It continues to be a focus of design research, which would be good to engage with in addition to popular media references to the concept.\n- The presentation of the paper seems rushed, including numerous typos and some structural choices that may need correction or at least clarification (e.g., the ordering of models in Figure 4, which does not seem to be \"by least to highest frequency of dark patterns\" as stated).\n- Where is the stated \"Appendix 5\"? Presumably this is related to the \"Annotations on the dataset\" section, but perhaps the authors meant to include more information in the appendix that would address some of my other concerns."
      }
    ],
    "rating_avg": 7.0,
    "confidence_avg": 4.0,
    "decision": "Accept (Oral)",
    "meta_review": "This paper presents a new benchmark— DarkBench — for evaluating dark patterns in LLMs. The authors develop the benchmark using few-shot prompting, resulting in a total of 660 adversarial prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. Using a mixture of human annotation and model annotations, the authors evaluate 14 open-sourced and preparatory models on the DarkBench and find prevalence of dark patterns across all models. \n\nThis is a timely and novel contribution towards tackling dark patterns in LLMs and to the evaluation of model safety in general. Given how arduous and complex building such a benchmark is, this is a thoughtful and significant contribution. The writing is concise and the figures are clear.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Of5F2GdGLA",
    "title": "VeSX: A Framework Featured by Verification, Self-Correction and In-context Learning for Web Automation Tasks",
    "authors": [
      "Lin Li",
      "Zhuyu Yao",
      "Xinyi Yang",
      "Boxun Li",
      "Qingmin Liao",
      "Yu Wang"
    ],
    "abstract": "While large language models have achieved remarkable success in tasks such as reasoning and question answering, applying LLMs to interactive tasks like web automation remains challenging. In web automation, existing planning-execution workflow often faces limitations due to the infeasible subtasks. We propose VeSX, a framework designed to enhance subtask feasibility through verification, self-correction, and in-context learning. VeSX introduces three key improvements: (1) subgoal-guided verification, which verifies the execution results of subtasks based on the preset subgoals; (2) hierarchical self-correction, which combines reflection and replanning, targeting to self-correct mistakes in both planning and execution phases; (3) exemplar bank, which improves in-context learning by partitioning execution trajectories and heuristically generating metadata for exemplars. We evaluate VeSX on WebArena benchmark and achieve the state-of-the-art average success rate of 0.34, which significantly outperforms existing methods without human guidance on all five scenarios.",
    "keywords": [
      "LLM agent",
      "web automation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Of5F2GdGLA",
    "forum_url": "https://openreview.net/forum?id=Of5F2GdGLA",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "VeSX is a framework for interactive web automation tasks using LLMs that focuses on improving sub-task feasibility, a common issue for planning based methods that initially break down tasks into multiple steps before execution. To improve sub-goal feasibility, VeSX introduces three components: sub-goal guided verification, which verifies either with the model itself or external methods if the sub-task is feasible. The second is a hierarchical self-correction method that takes place when verification fails during planning as well as during execution. Hierarchical self-correction uses reflection to correct verification errors, and replans if necessary. Lastly, VeSX uses an exemplar bank for in-context learning for both planning and execution. Unlike previous uses of in-context learning, the VeSX exemplar bank does not use full trajectories, instead sampling from existing trajectories to build the examples. For evaluation, VeSX uses 5 scenarios from the WebArena benchmark.",
        "strengths": "- Identifies key weaknesses in current methods for web automation\n- Method tries to account for different types of failures through the dual verification system and self-correction\n- Notable observations as part of method:\n    - A) It is easier to verify then come up verification for different goals \n    - B) Having the LLM output expected results as part of reflection \n- Exemplar bank: I think this is one of the strongest contributions since it is very different than existing work in particular using parts of trajectories instead of full trajectories.",
        "weaknesses": "- Presentation:\n    - I am a bit confused about the overall workflow. It would be helpful to have it written in an algorithm. \n    - It would also be helpful to see more examples \n- Extra Time and Cost:\n    - How much extra time and tokens does it take for this method compared to others (if available for other methods)? If these other methods also had access to more compute, they might also have higher performance. \n- Original of exemplars: Are the exemplars produced from questions in the benchmark? Are those questions included in the final results? This could also lead to an unfair comparison. \n- One stated advantage of the approach is that human guidance is not needed. Is any human guidance used to design the prompts for the different steps? Is the exemplar bank used as in-context examples for all of the different steps?"
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper presents VeSX, a framework for enhancing large language models (LLMs) in web automation tasks by introducing verification, self-correction, and in-context learning. VeSX aims to tackle the common challenges in web automation workflows, such as subtask infeasibility and data scarcity, by implementing three key components: subgoal-guided verification, which checks the accuracy of each subtask; hierarchical self-correction, allowing the model to reflect and replan when errors occur; and an exemplar bank for in-context learning, storing structured examples that improve decision-making. Evaluated on the WebArena benchmark, VeSX achieved a state-of-the-art success rate of 34% across multiple scenarios without human guidance, demonstrating its potential to improve accuracy and reliability in complex, multi-step web interactions.",
        "strengths": "- The web automation task is interesting and worth exploring.\n- The proposed self-reflection approach seems to have great improvement in performance, highlighting its potential to enhance task success and reliability in complex, interactive environments.",
        "weaknesses": "- The novelty is limited. Compared to previous work on web automation, the paper integrates self-reflection and retrieval-augmentation components, both of which have been widely explored. The paper also lacks discussions on relevant works on reflection and retrieval augmentation. \n- The writing needs to be improved, especially in explaining the main components and their novelty. \n    * Section 2.1 Overview is empty\n    * Clearly indicate success rates as percentages by adding the percentage sign (e.g., 34% instead of just 34)\n    * It will be better to put short descriptions in the captions for terms in the table (‘Shop’, ‘CMS’, ‘Red’, ‘Git’, ‘Map’).\n    * Adding example prompts would provide readers with a practical understanding of the pipeline.\n- Figures need to be significantly improved:\n    * ‘orders’ rather than ‘oreders’ in the teaser figure.\n    * Miss left bracket for ‘click sorted by]’. Is ‘click [sorted by]’ and ‘click [sortby]’ the same operation?\n    * the texts frequently touch or cross the boundaries of the icons. \n    * Some figures are blurry and difficult to interpret. (e.g. In Figure 2, it is not clear what the four boxes below the environment represent.)\n    * The figure captions should be refined to clearly describe each major component."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors present a solution to automating web tasks such as checking on shopping orders. The solution leverages LLMs that break down the task into subtasks, executes those subtasks in the browser, verifies the subgoals are accomplished, can self corrects and replan if necessary, and leverages in context examples retrieved from an exemplar bank created by the authors. Experimental results show the authors' superior approach compared to the literature on WebArena, a popular benchmark in the literature.",
        "strengths": "Summary: \n- Solves a relevant problem\n- Adopts a solution that is based on the latest technology\n- Beats the state of the art with their experimental results on a well-known benchmark from the literature\n\nDetails: \nThe problem of automating web tasks is difficult and very relevant in this age of enterprise productivity. Many tasks are quite repetitive and could benefit from automation but the diversity of browsers and apps and tasks makes it challenging for automated systems. \n\nLLMs have proven beneficial and the paper not only leverages them but also tests GPT-4o which is one of the newest and less costly models compared to others from the literature. \n\nThe proposed framework introduces three key components to the LLM pipeline: 1) sub-goal verification, 2) self-correction and 3) exemplar bank. Each of these components are not particularly original but combining them into a single framework and applying this framework to the web task automation leads to state of the art of results.",
        "weaknesses": "Summary:\n- Limited experimental results and analysis including missing computational cost analysis, error analysis especially when linked to the various contributed components in their framework\n- Typing and grammar mistakes\n\nDetails:\nThe experimental results show that the proposed approach (including individual components) do improve the state of the art on the web arena benchmark. The authors compare to other approaches from the literature and do an ablation study on the components they proposed. However, the experimental analysis is still missing some key results that could help the community understand and evaluate this approach better. Notable, the authors perform multiple LLM calls during their pipeline. Quantifying the computational cost (whether with number of calls per input or some other metrics) would help evaluate the approach and compare to other in the literature. Furthermore, the authors do not analyze what errors benefited more from what components in their pipelines. What types of errors needed replanning, which were addressed with reflection only, why did some of the verifications fail, etc. Finally, the authors perform an end to end evaluation but do not evaluate each component individually on intrinsic metrics; e.g., how often was the reflection component able to correct an error that is within its scope, etc."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents VeSX, a framework for web automation that integrates verification, self-correction, and in-context learning mechanisms.",
        "strengths": "1. The exemplar bank's approach of breaking down trajectories into smaller, reusable components is innovative and practically valuable for reducing context length while maintaining effectiveness.\n2. The ablation studies are comprehensive and help validate the contribution of each component.\n3. The design of local reflection and global reflection are interesting.",
        "weaknesses": "1. The literature review on LLM-based agents appears incomplete, missing several relevant recent works\n2. About \"subgoal-based verification,\" process supervision is a well-studied research direction[1]. This paper's key difference lies in the hierarchical verification mechanism. However， to prove the effectiveness of hierarchical verification，more comparison experiments and discussions should be made。\n3. Although the authors don't use ground-truth labels, their exemplar construction process still utilizes tasks from the target domain. While this doesn't constitute supervision in the traditional sense, it does provide the model with domain-specific information that zero-shot baselines may not have access to, potentially creating an unfair comparison if the baselines are purely zero-shot.\n\nReferences:\n\n[1] Lightman H, Kosaraju V, Burda Y, et al. Let's verify step by step[J]. arXiv preprint arXiv:2305.20050, 2023."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper presents VeSX, a framework designed to enhance web automation tasks by improving the subtask feasibility of Large Language Models (LLMs). It addresses the challenges of error-prone workflows by introducing three key components: (1) Subgoal-Guided Verification, which ensures that subtasks are completed correctly by generating subgoals during the planning phase and verifying the execution results against those subgoals, (2) Hierarchical Self-Correction, which adds layers of error correction during both the planning and execution phases. If mistakes occur, the model first reflects on its actions, and if needed, replans the task, (3) Exemplar Bank for In-Context Learning, which uses stored examples of previous tasks to help the model learn from experience and improve performance on future tasks.",
        "strengths": "Originality: The paper presents VeSX, a framework that introduces a combination of verification, self-correction, and in-context learning for web automation tasks. The approach is notable for its hierarchical self-correction mechanism, which allows the model to reflect on errors and replan, addressing potential common challenges.\n\nQuality: The idea proposed in this paper is straightforward and clear. The overall structure is clear, despite some minor confusion. \n\nClarity: Key concepts such as subgoal-guided verification and hierarchical self-correction are explained straightforwardly, and the diagrams effectively support the explanations.\n\nSignificance: VeSX addresses a common issue in web automation—handling subtask failures and error correction. Its ability to autonomously verify and correct errors while using in-context learning is a useful enhancement.",
        "weaknesses": "While the paper proposes an interesting framework for web automation, the technical contribution feels somewhat limited. The system is more focused on practical application rather than introducing a novel method or algorithm. Additionally, there is no follow-up evaluation of the entire system under real-world conditions. It would be beneficial to see both quantitative and qualitative analyses of VeSX in real-world usage scenarios to better understand its performance in practical settings. E.g., a statistical evaluation or user study focusing on whether this system truly works for real-world tasks would significantly strengthen the paper. A field study or feedback from real users would also provide practical insights into how the system performs in dynamic, unstructured environments.\n\nThe concept of \"self-correction\" is promising, but the evaluation of this feature is not comprehensive enough. Although the paper includes an ablation study, a more detailed analysis of the self-correction mechanism is necessary to demonstrate its effectiveness. For example, breaking down how self-correction functions in different failure cases or assessing the time and resource costs associated with error correction would provide deeper insights into the feature’s utility. \n\nThe paper does not address what happens if the system or specific components fail. While self-correction is included, there is no discussion of how the system handles scenarios where self-correction or verification mechanisms fail. For real-world applications, understanding the system’s resilience and fallback options is crucial. Including an analysis of fail-safe protocols would enhance the system’s reliability and robustness."
      }
    ],
    "rating_avg": 4.6,
    "confidence_avg": 3.8,
    "decision": "Reject",
    "meta_review": "This paper proposes a agent-based framework for web automation tasks (VeSX), with 3 main new modules: subgoal-guided verification, hierarchical self-correction, and exemplar bank. The goal is to ensure that subtasks are are executable, and be able to correct both subtasks and the plan. Instead of storing entire trajectories as previous work, the exemplar bank in VeSX stores single-action exemplars, as well as planning exemplars. Evaluation on WebArena shows that VeSX (which does not require human feedback) performs comparably to a baseline method that uses human feedback. While reviewers appreciated the introduction of different modules in VeSX and the importance of the application, concerns were raised on potential issues in the execution of the exemplar bank (the usage of the benchmark examples to construct this), computational cost of the framework, and additional analysis (e.g. what happens when modules fail?). There was additional discussion between reviewers during the discussion phase on these concerns. The AC agrees with the concern that there are a lot of design choices in the examples used to form the exemplar bank (while test labels were not used, it is difficult to evaluate how these choices impacted performance). Additionally, the ablation study is only on one of the 5 scenarios, and the effect on the entire dataset is unknown. Additional examples on comparing the use of exemplar bank with in-context learning in VeSX would be helpful, or what if the exemplar bank is formed with 1 scenario held-out? The framing of the paper need to be more clear if all test tasks need to be known ahead of time to form the exemplar bank, vs. being used with in-context learning. The proposed method is interesting, but additional experiments are required to fully understand the contribution of each component. With completed experiments, I think this could be a good submission in the future.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Y0kmI2zqqi",
    "title": "Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time Series Classification",
    "authors": [
      "Xavier Mootoo",
      "Alan Arnoldo Diaz Montiel",
      "Milad Lankarany",
      "Hina Tabassum"
    ],
    "abstract": "ile the majority of time series classification research has focused on modeling fixed-length sequences, variable-length time series classification (VTSC) remains critical in healthcare, where sequence length may vary among patients and events. To address this challenge, we propose $\\textbf{S}$tochastic $\\textbf{S}$parse $\\textbf{S}$ampling (SSS), a novel VTSC framework developed for medical time series. SSS manages variable-length sequences by sparsely sampling fixed windows to compute local predictions, which are then aggregated and calibrated to form a global prediction. We apply SSS to the task of seizure onset zone (SOZ) localization, a critical VTSC problem requiring identification of seizure-inducing brain regions from variable-length electrophysiological time series. We evaluate our method on the Epilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial electroencephalography (iEEG) recordings obtained from four independent medical centers. SSS demonstrates superior performance compared to state-of-the-art (SOTA) baselines across most medical centers, and superior performance on all out-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally provides post-hoc insights into local signal characteristics related to the SOZ, by visualizing temporally averaged local predictions throughout the signal.",
    "keywords": [
      "Time Series",
      "Healthcare",
      "Medicine",
      "Epilepsy",
      "Neuroscience"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Y0kmI2zqqi",
    "forum_url": "https://openreview.net/forum?id=Y0kmI2zqqi",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This article proposes a meta-heuristic to improve time series classification algorithms by classifying random windows and aggregating scores. The algorithm is tested on a real-world task where it achieves promising performance.",
        "strengths": "- The article is well-written and easy to follow.\n- The methodology is sound and has the potential to provide an interpretable time series classification strategy that can be combined with virtually any time series classification algorithm.",
        "weaknesses": "- The proposed approach is simple, which is perfectly fine, but lacks a more thorough analysis, at least empirical. For a reader with a use case in mind, it is difficult to assess if this method is appropriate.\n- Another drawback is that this approach has only been tested on one data set. To assess its generalization capability, the authors could test their method on additional publicly available time series datasets.\n- Section 4.5 on qualitative interpretation needs to (re)written to be more convincing as it lacks much information or comments. Interpretability is one of the contributions listed in the introduction."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper describes using aggregation of time-series classification model predictions across windows during training and inference a way to go beyond fixed-context length window processing and infinite context models recurrent neural networks.  The aggregation method explored here is simple averaging. An additional calibration step is used after the model is trained. \n\nThe method is applied to different EEG channels in order to learn to classify a channel as being in the seizure originating zone or not. Cross-subject and cross institution results show very promising performance compared to fixed-context length approach and infinite context models.",
        "strengths": "Very clear presentation and well-fit for this type of time series classification problem. The calibration step after pooling during training is a thoughtful addition.",
        "weaknesses": "Main concern is the single domain/task used to test the method. While the single domain is very interesting, there is something different in the fact that the seizure periods are themselves randomly occurring throughout the time series. In other tasks, long-term dynamics of the time series may require extracting patterns through time rather than this which is more akin to multiple instance learning where the search is for any evidence of positive class. \n\nThe lack of other tasks weakens the generality of the method, but I don't have the perfect case of variable length it is hard to say where there would"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces Stochastic Sparse Sampling (SSS), a new framework for classifying variable-length medical time series. SSS employs fixed windows sparsely to make local predictions, which are then combined to form a global prediction.",
        "strengths": "1) Seizure onset zone (SOZ) detection is a novel direction that may be important for clinical intracranial EEG (iEEG) research.\n2) The method is described in detail, ensuring clarity and reproducibility. \n3) The experiments use a large-scale public iEEG dataset for evaluation, showing the robustness and effectiveness across diverse and heterogeneous data sources.",
        "weaknesses": "1) This paper uses a single-channel approach for SOZ detection, justifying it by citing the challenges posed by varying numbers of iEEG channels across iEEG recordings. However, this reason does not demonstrate that single-channel analysis is more effective. In clinical practice, SOZ and early propagation zones involve multiple channels with timing differences in abnormal discharges [1]. The authors do not provide sufficient theoretical or experimental support to explain why a single-channel approach would effectively capture these critical distinctions.\n2) This paper defines SOZ detection as a variable-length time series classification (VTSC) task. Still, the authors do not clarify the specific benefits of VTSC over general anomaly detection for this application. Their justification, that “effective treatment requires analysis of variable-length signals,” lacks concrete references or explanations relevant to variable-length signals, weakening the rationale for using VTSC in this context.\n3) The proposed method is quite simple. How does it differ from other downsampling or sparse sampling approaches?  Model output still provides a global classification result, which seems no different from a standard classification model. The paper should compare its approach with more methods that share a similar motivation and more details of related works to clarify its position within the research field. \n4) The authors state in the abstract that their method outperforms \"state-of-the-art (SOTA) baselines across most medical centers.\" However, none of these baselines were designed for the SOZ detection task or medical time series data.Instead, all baseline models were built for general time series or other sequential data forecasting tasks, not even for time series classification. The authors should have used baselines developed for iEEG or EEG signal analysis, such as methods in references [2-4], or, at the very least, models designed for time series classification or anomaly detection [4,5]. \n\n\n[1] Li et al., Neural fragility as an EEG marker of the seizure onset zone, Nature Neuroscience, 2021.\n\n[2] Tang et al., Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis, ICLR, 2022.\n\n[3] Luo et al., Exploring Adaptive Graph Topologies and Temporal Graph Networks for EEG-Based Depression Detection, IEEE Transactions on Neural System and Rehabilitation Engineering, 2023.\n\n[4] Rikuto et al., SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning, ICDM, 2024.\n\n[5] tang et al., Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification, ICLR, 2022.\n\n[6] Lu et al., Out-of-Distribution Representation Learning for Time Series Classification, ICLR, 2023."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper introduces a multi-scale learning approach for medical time series classification. The proposed method comprises multiple independent models, each with a distinct patch length, allowing it to capture information across various temporal scales. The patching method follows the PatchTST framework, which employs single-channel patching. To reduce computational costs, the authors implement stochastic sparse sampling, randomly selecting models during training. The final representation is an aggregation of outputs from all models, combining multi-scale information. The model is evaluated on intracranial EEG (iEEG) data for seizure onset zone classification, using a dataset collected from four independent medical centers.",
        "strengths": "The use of sparse sampling for computational savings in multi-scale learning is an interesting idea. Additionally, the out-of-distribution classification on unseen subjects from different medical centers demonstrates strong potential for generalizability in real-world applications.",
        "weaknesses": "The motivation to save computational resources is well-intentioned, though I am concerned about its practicality in actual training. For a given set of window sizes with corresponding independent models, even if only subsets of window sizes are selected during training, the space complexity of the models remains unchanged. This approach primarily improves training speed without reducing memory requirements. Additionally, using an independent model for each patch length may not be optimal for memory efficiency. A shared backbone across different patch lengths could be a more effective choice for memory savings. Overall, the method resembles an enhanced version of MTST [1], employing a random subset of models with varying patch lengths during training.\n\nMoreover, while the paper’s title refers to medical time series, only a single seizure dataset is used for evaluation. Expanding the evaluation to include additional datasets would strengthen the claim of generalizability. The ablation study could also benefit from a deeper investigation into multi-scale learning with various patch lengths. For instance, exploring which combinations of patch lengths yield the best performance would be informative. Additionally, the impact of stochastic sparse sampling should be assessed in detail. For a given list of patch lengths, how do memory usage and running time between training with and without stochastic sparse sampling? Lastly, a recent work, Medformer[2], should be compared in baseline methods, as it is also designed for medical time series classification using multi-scale patching. A discussion on the differences between this method and Medformer would also be valuable for highlighting the unique aspects of the proposed approach.\n\n\n[1] Multi-resolution Time-Series Transformer for Long-term Forecasting\n[2] Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification"
      }
    ],
    "rating_avg": 4.75,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "qK6U4Ahfms",
    "title": "OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents",
    "authors": [
      "Yuwei Yan",
      "Qingbin Zeng",
      "Zhiheng Zheng",
      "Jingzhe Yuan",
      "Jun Zhang",
      "Jie Feng",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, scaling LLM agents to large city simulations presents significant challenges. Existing models are limited by the computational and communication costs of LLMs, compounded by the dynamic nature of urban environments that require continual updates to agent behavior. To address these limitations, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a ``group-and-distill'' prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70\\% reduction in LLM requests, and a 50\\% reduction in token usage. These improvements enable the simulation of 10,000 agents’ daily activities in 1 hour on commodity hardware. Additionally, OpenCity establishes a benchmark for LLM agents, comparing simulated mobility behaviors, origin-destination flows, and segregation indices against real-world data. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.",
    "keywords": [
      "LLM Agent",
      "Large Language Model",
      "Urban Study"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=qK6U4Ahfms",
    "forum_url": "https://openreview.net/forum?id=qK6U4Ahfms",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper describes an approach\twhere LLM agents are used to simulate individual behaviour in large (city-scale) simulations of people.\tThe proposed platform uses LLM agents that can adapt their behaviour depending on context and memory. This is different to the classic agent based approach for this type of simulation where behaviours are static over time. \nThe development of the platform is one of the main contributions of the work, and the development of a user-friendly web interface is another contribution highlighted by the authors. From a machine learning perspective, the proposed “group-and-distill” approach to reduce LLM usage is the main contribution of the work, essentially a clustering approach before prompting the LLM for each cluster (as opposed to prompting an LLM for each individual).",
        "strengths": "The use of a LLM for the purpose of larger scale population modelling appears to be novel, and the suggested group-and-distill approach enables this idea, with relatively low hardware resources. \nOverall, considerable effort appears to have gone into development of the system. The system could be an interesting resource for research in complex systems.",
        "weaknesses": "The paper has quite a broad focus, like an overall project report. For a venue like iclr, it would have been better to focus on the specific contributions in machine learning, and to provide more technical details rather than an overall description of architecture and usability aspects as the main contributions. In its current form, ICLR does not appear to be the right venue for the work as it is presented.\n\nThe work lacks depths in aspects that I would see essential for any ML paper: for example the group-and-distill concept is introduced, but the paper is very sparse in detail of the specific algorithms. Similarly it would have been interesting to see what are the initial prompts and the optimised prompts, in contrast. \nAny details comparing to the original approach without group-and-distill / ablation would have been an improvement too. \n\nMoreover it didn’t become clear to me what LLM has been used or how was it trained, and how do LLM outputs influence agents’ behaviours.\n\nFinally, the paper mentioned at the beginning the explainability of ABM as an advantage over black box neural network approaches. with the lack of detail on how the actions are influenced by the LLM or how the LLM are trained or fine tuned, the proposed model has the same disadvantage as any other neural network model.  \n\nMinor presentation issues:\n\n\"Agent-based models (ABMs) were first introduced to urban studies in the seminal work of Thomas Schelling about 50 years ago Schelling (2006)\"\n- if the work referenced was from approx 50 years ago, Schelling 2006 is the wrong reference. I believe the correct year would be 1978.\n- there are two\tkinds of citations, narrative (like the one in the sentence), and parenthetical (Schelling, 2006). It doesn't make sense to use\tnarrative style\twhen it doesn't fit into the sentence structure. In LaTeX with natbib, this is the difference between \\citet and \\citep. \nThe referencing is an issue throughout the paper."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. Nevertheless, the extreme high computational cost of LLMs presents significant challenges for scaling up the simulations of LLM agents. With this motivation, this paper introduces OpenCity, a scalable simulation platform designed to efficiently simulate urban activities using a large number of LLM agents. The platform incorporates innovative techniques, including LLM request scheduler and a group-and-distill prompt optimization strategy, to reduce the computational overhead of simulating LLM agents significantly. OpenCity achieves a 600-fold speedup and reduces both LLM requests and token consumption. Extensive experiments on six global cities verify the platform's scalability and its capability to replicate real-world urban dynamics.",
        "strengths": "Strengths\n1.\tThis paper introduces a scalable platform for urban simulations using LLM agents, which addresses a growing need for realistic human behavior modeling in urban environments.\n2.\tThis paper shows a high quality of presentation. The paper is technically sound and the research question is clear. The optimizations, particularly the LLM request scheduler and prompt optimization strategies, demonstrate clear performance benefits. The experimental results showing a 600x speedup and significant resource savings are compelling.\n3.\tThe paper is generally clear and well-structured. It provides a clear problem statement, introduces the proposed framework, and highlights key findings.\n4.\tThe contribution of the paper is relevant for LLM agent. The results of this paper is interesting and significant in LLM agent simulation. The proposed OpenCity framework is relevant for urban planning and policy-making. The development of a web portal that allows researchers to configure and visualize simulations without requiring programming skills is a valuable addition, making the platform accessible to a broader audience.",
        "weaknesses": "1.\tThe introduction part fails to convey to the reviewers what is the motivation and novelty in this paper. In fact, the authors should add more previous work on LLM agents based simulation platform. The problem this paper addresses and the reason why this paper uses system-level LLM request scheduler and prompt-level “group-and-distill” strategy to solve the problem of scalability should be further explained. Besides, the contribution the authors listed in the introduction section is inaccurate，the authors should focus on the system-level LLM request scheduler and prompt-level “group-and-distill” strategy. Thus, I would recommend a revision for the introduction section in this paper.\n2.\tThis paper utilizes Group-and-Distill Meta-Prompt Optimizer to classify similar agents to reduce computational complexity, which indeed improve efficiency. However, this may overlook differences between individuals, so the reason why this method can preserve the distinctive characteristics of the agents, as show in the experimental part, should be further explained in the method section。\n3.\tFigure 2 illustrates the principle of Group-and-Distill Meta-Prompt Optimizer. However, it seems difficult to follow. It is more intuitive to add an example to explain how IPL works. \n4.\tThere lacks explanation for the reason why the proposed method IPL is superior to conventional prototype learning. Moreover, the principles for setting the value of M and T in IPL should be further illustrated.\n5.\tExperimental part: the authors should add an explanation of the indicators including JSD, T1 and bold the important data in Table 2 . Similarly, Table 3 also requires revision. The metrics of RMSE of New York和San Franciscoin are not displayed in Table 3, which seems a little bit confusing, the authors need to provide explanations. \n6.\tThe authors should pay attention to the standardization of citations throughout the paper, especially in introduction and related works section. For example, “conventional prototype learning methods...”(line305), “the baseline represents the simulation time without optimization” (line 389), “we analyze the performance of the Generative Agent and EPR Agent ”(line 450).\n7.\tThe authors should carefully proofread the manuscript for typos and formatting issues. There exists some typos: in the abstract: “we deisgn a “group-anddistill” prompt optimization”, “where τqα is is the proportion”(line 687) , etc."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper is considering the problem of agent-based modeling of environments such as cities. Such models had been previously used with the agents performing relatively simple behaviors. While LLMs open new opportunities for controlling the behaviors of the individual agents, their computational cost presents a significant scaling problem. \n\nThe paper describes an architecture that enables the parallelization of the agents, to allow the modeling the daily activities of a city with 10,000 agents. The architecture appears to be based on an efficient polling model of the LLM, as well as the development of a prompting model called \"group-and-distill\". The application of these models show a more than 600-fold increase.",
        "strengths": "* The overall goals of the paper, of capitalizing on the abilities of LLMs to achieve a better ABM model of cities, as well as addressing the scaling problems, are laudable.",
        "weaknesses": "* Achieving a more than 600 fold speed increase in terms of an improved process scheduler and I/O scheduler can be probably seen as \"debugging\", rather than research result, and very likely has nothing to do with the LLM. \n* It seems that the very considerable computational effort of an LLM can only achieve an approximate parity with the much cheaper rule based efforts. This is understandable, as description of the behavior of the agents described in the paper follows the same position based rules that the ABM models historically use. As there is no consideration of language or other type of reasoning, the paper does not make it clear what type of benefits one would expect from LLMs.\n* The only part of the paper that has a connection to the topic of this conference is the way in which the \"group-and-distill\" model is proposed to achieve the simulation of multiple agents with one prompt. However, there is very little about this technique in the paper proper, so it is difficult to form a judgement."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "ABM and LLM is leveraged to develop one platform for open city modeling and planning. It is a nice simulation platform and the paper provides application scenarios. Concretely, \n\n1. This paper combines agent-based models with large language models to develop the OpenCity platform for simulating urban activities. It reduces simulation computational costs through IO multiplexing and the \"group-and-distill\" prompt optimization strategy.\n2. Through experiments conducted in six major cities worldwide, OpenCity demonstrates a 635 plus increase in average simulation speed per agent, along with a 70% decrease in LLM requests and a 50% reduction in token usage. The time savings are mainly concentrated in the LLM response wait time and the CPU multiplexing process.\n3. The OpenCity platform proposed in this paper achieves the first benchmark testing for LLM agent-based urban activity simulation research.",
        "strengths": "The paper give a detailed introduction of the novel methods and the real outcome.\n\n1. Originality: The paper presents a novel approach by integrating Large Language Models with agent-based modeling to simulate urban activities. There has been limited research on combining LLMs with agent-based models, and even less so in the context of large-scale urban activity simulations. By using IO multiplexing and the \"group-and-distill\" prompt optimization strategy to reduce the computational cost of simulations, this paper has made the application of LLMs in large-scale urban activity simulations possible.\n2. Quality: The research is well organized with a clear methodology and experiments conducted in real cities data. The results show notable improvements in both simulation efficiency and accuracy, confirming the effectiveness of the proposed platform.\n3. Clarity: The paper is written in a clear and concise manner; it is easy to understand through the explanation of figures\n4. Significance: This paper establishes a benchmark for LLM agent-based urban activity simulation research. This paper also provides a scalable framework for simulating urban dynamics.",
        "weaknesses": "There is a lack of theoretical contribution, overall, rather it is an application tool development with leveraging well established tools. It may not fit ICLR the best though not out of scope at all. Further,\n\n1. Some parts of the main body text are not rigorous enough. For example, Equation 1 is missing a parenthesis, and the IPL method is mistakenly labeled as the LPL method in Figure 2.\n2. This research has high requirements for data quality. Additionally, despite significantly improving computational efficiency and reducing costs, the platform may still require substantial computational resources.\n3. When simulating cities in different countries, the dynamic properties to be considered should not be entirely the same, and some of the assumed static properties may also change during the simulation process."
      }
    ],
    "rating_avg": 4.25,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "The paper proposes OpenCity, a platform combining agent-based modeling (ABM) with large language models (LLMs) to simulate urban dynamics at scale. By leveraging techniques like the “group-and-distill” prompt optimization, the platform achieves significant computational efficiencies, including a 600-fold speedup and reduced resource consumption in large-scale simulations. The paper claims to establish a new benchmark for LLM-powered urban simulation and provides a user-friendly web interface for broader accessibility.\n\nWhile the proposed OpenCity model shows promising results, the reviewers have identified several weaknesses that need to be addressed:\n1. The paper’s broad focus, which includes platform architecture and usability aspects, lacks sufficient technical depth on the machine learning contributions, especially regarding the “group-and-distill” approach.\n2. The absence of detailed algorithmic descriptions and comparisons, such as ablation studies or specifics on optimized prompts, weakens the paper’s technical rigor.\n3. There is a lack of clarity on which LLM model is used, how it is trained, and how its outputs influence agent behavior, leaving the model’s inner workings inadequately explained.\n\nBased on these weaknesses, we recommend rejecting this paper. We hope this feedback helps the authors improve their paper.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "drrXhD2r8V",
    "title": "Structure-Aware Parameter-Efficient Machine Unlearning on Transformer Models",
    "authors": [
      "Wenjie Bao",
      "Jian Lou",
      "Yuke Hu",
      "Xiaochen Li",
      "Zhihao Liu",
      "Jiaqi Liu",
      "Zhan Qin",
      "Kui Ren"
    ],
    "abstract": "Transformer has become fundamental to a vast series of pretrained large models that have achieved remarkable success across diverse applications. Machine unlearning is an emerging field focused on efficiently removing the influence of specific data from trained models, to comply with privacy regulations enforcing the right to be forgotten. The sheer size of Transformer-based models poses a significant challenge to unlearning efficiency. Existing methods find it promising to restrict unlearning updates to a small portion of influence-critical parameters. However, their parameter-efficient unlearning methods are largely devised in a structure-oblivious manner, which tends to inaccurately identify these parameters and leads to inferior unlearning performance for Transformers. In this paper, we propose {\\tt SPE-Unlearn}, a structure-aware parameter-efficient machine unlearning approach tailored for the Transformer architecture. {\\tt SPE-Unlearn} introduces a learnable pair of masks to respectively pinpoint influence-critical parameters in the heads and filters of Transformers. The learning objective of these masks is derived by jointly considering both desiderata of unlearning, i.e., sufficiency in influence removal and efficiency, and optimized through an efficient algorithm featured by a greedy search with a warm start. Equipped with the identified key parameters, {\\tt SPE-Unlearn} facilitates second-order unlearning, memory-free unlearning, and memory-aided unlearning scenarios. Extensive experiments on various transformer models and datasets demonstrate the effectiveness and efficiency of {\\tt SPE-Unlearn}~for Transformer unlearning.",
    "keywords": [
      "Machine Unlearning",
      "Parameter-Efficient",
      "Transformer"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=drrXhD2r8V",
    "forum_url": "https://openreview.net/forum?id=drrXhD2r8V",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper introduces a sparsity-aware machine unlearning method. First, it proposes to adaptively identify influence-critical parameters using a derived score function. Then, it applies existing machine unlearning methods (e.g., second-order unlearning) exclusively to optimize the identified parameters. The process of parameter identification and optimization is performed iteratively.",
        "strengths": "1. The approach to identifying influence-critical parameters based on the retain set (and the forget set) appears original. The derivation is clear and reasonable. (However, the necessity needs clarification, as mentioned in the weaknesses below.)\n2. The paper is well-organized and easy to understand.\n3. It is meaningful to explore the applications of the proposed method for both memory-free and memory-aided successive machine unlearning scenarios.",
        "weaknesses": "1. In line 273, it appears that the differentiation is performed on $\\theta$ rather than $m$.\n2. Doubts regarding the necessity of the proposed parameter identification approach. The authors derive a score function to identify informative parameters, involving calculating the derivative with respect to $m$ over $D_f$ and $D_r$. Why not directly optimize the mask parameters base on Eq.(4), which seems computationally cheaper and simpler with only once gradient calculation? Or why we need to transform Eq.(4) to Eq.(5)?\n3. Doubts about the claimed efficiency benefits. The paper claims improved efficiency for machine unlearning, but this may not be the case:\n   - Computation: Compared to vanilla second-order unlearning, the proposed method requires additional calculations involving twice the differentiation to identify informative parameters and still requires computing gradients for all parameters during the unlearning phase.\n   - Memory: Although masking hides certain parameters, storing the mask and associated gradients requires extra memory.\n4. Insufficient complexity analysis. A comprehensive analysis is needed, detailing the complexity of both the parameter identification and unlearning phases from perspectives like memory and computation. \n5. What are the specific advantages of the proposed method for the successive machine unlearning? The descriptions (e.g., in Line 362) is  too broad and hard to understand. Detailed, clear and reasonable analysis is needed.\n6. It is recommended to use the standard notation format as outlined by ICLR guidelines instead of using a uniform font throughout all equations. The correct formatting guidelines can be found on the official website."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper proposes an efficient unlearning method with multiple benefits, such as reduced time and memory costs. The proposed method is a pruning-based unlearning approach that filters out sensitive weights to forget specific data. By converting to a differential formulation for the masking variable, the authors approximate unlearned weights through a combination of masking and the Fisher information matrix. Using a second-order Taylor expansion and the convergence assumption from LeCun et al., the method is simplified to this combination. In experiments, the authors compare their approach to prior methods and report promising performance.",
        "strengths": "I believe the strongest benefit of this work is its time and memory efficiency. Across all tables, the authors report significantly reduced costs for unlearning, which highlights a promising advancement in the field of machine unlearning.",
        "weaknesses": "However, I have several concerns:\n\n1. It is unclear why the method is considered structure-aware. While mask variables are applied to each head in the multi-head attention block, this approach is not unique to Transformers. The authors mention as \"widely-adopted unlearning methods in Transformers, e.g., fine-tuning (Golatkar et al. (2020)) and gradient difference (Liu et al. (2022); Jia et al. (2024)),\" but they are not specified to transformer architecture. As I understand from the manuscript, the authors suggest that their method’s efficiency makes it well-suited to large-scale Transformer models, but large scale is not a Transformer-specific attribute.\n\n\n2.  Although the method is not specialized for Transformer architectures, much of the paper focuses on Transformer-specific content. Reducing this content could make the paper more concise and focused.\n\n\n3. In line 190, the authors assume that \" L is differentiable with respect to m\", to develop Equations (5-9). However, as stated in line 160, $m$ is a binary variable. This raises concerns about whether the differentiability assumption is valid.\n\n\n4. In line 181, the authors state, \"we formulate the unlearning objective (1) with a learnable pair of masks for the heads and filters as a constrained optimization problem.\" However, objective (1) aims to retain a model with the same architecture trained only on D_r. The proposed method, which involves pruning, alters the architecture, making it misaligned with this objective.\n\n\n5. The experimental section lacks details on the partitioning of D_r and D_f. This split is an essential part of the experimental setup and should be clearly specified.\n\n6. Evaluation Concerns in Table 2 and Appendix:\n\n6-1) The unlearning performance should be evaluated by comparing it to the performance of a retrained model, as specified by Eq (1). In specific, the MIA metric should follow this protocol, with retrained models serving as the gold standard. However, the authors set the lower values of efficacy and higher values of fidelity are better without any explanation.\n\n\n6-2) In Table 1, the proposed method unlearns to a greater extent (achieving 85.94% as the lowest Unlearning Accuracy) than other methods, resulting in lower Remaining Accuracy. To ensure a fair comparison, it would be better to report Remaining Accuracy and Testing Accuracy at the point where each method reaches a common Unlearning Accuracy threshold.\n\n\n6-3) All methods (FT, GD, SA) have hyperparameters that control unlearning speed, such as learning rate, but there is no discussion of these parameters in the manuscript. Without this information, it’s unclear whether the evaluations are fair.\n\n\n6-4)  The authors used only D_r for Fine-Tuning (FT) and Sparsity-Aware Unlearning (SA) but used both D_r and D_f for their method. This discrepancy in dataset usage should be clarified, and comparisons with more unlearning methods that use both D_r and D_f are recommended.\n\n\n6-5) Many of the unlearning methods used for comparison are outdated. It would strengthen the evaluation to include more recent works, such as:\n\n[1] Fan, C., Liu, J., Zhang, Y., Wong, E., Wei, D., & Liu, S. (2023). Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508.\n\n[2] Chen, M., Gao, W., Liu, G., Peng, K., & Wang, C. (2023). Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7766-7775).\n\n\n7. (Minor) In the second sentence of the Related Work section, \"Jang et al.\" is cited twice in a single sentence."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper proposes a structure-aware parameter efficient machine unlearning approach for transformer models.",
        "strengths": "The research problem is interesting and the approach is novel.",
        "weaknesses": "Some experiments are insufficient, and the experimental setup should be more comprehensive.\n\nFor example, in page 7 the author notes that if the number of unlearning requests exceeds a certain threshold, the model must be retrained from scratch to regain its performance. What exactly is this threshold?\n\nAdditionally, in Table 3, is there a specific reason for selecting 90% sparsity instead of alternatives like 85% or 95%? Based on Figures 2 and 3, is this value specific to the model?"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The authors tackle the challenge of efficient machine unlearning in large Transformer models, essential for meeting privacy regulations. Existing methods, often structure-agnostic, struggle to accurately target influence-critical parameters in Transformers. To address this, the authors introduce SPE-Unlearn, a structure-aware approach that uses learnable masks to identify key parameters within Transformer heads and filters. Optimized via a greedy search, SPE-Unlearn enhances unlearning by balancing efficiency and effectiveness. Extensive experiments show that SPE-Unlearn significantly improves unlearning performance across various Transformer models and datasets.",
        "strengths": "1. The paper is clearly written, well-formatted, and well-organized. \n\n2. The mathematical derivations are rigorous.",
        "weaknesses": "1. The paper mentions that SPE-Unlearn can enhance the effectiveness of various methods; however, the authors only integrate SPE-Unlearn with SO and do not test it with other methods. In Table 2, SPE-SO does not show a significant improvement over SO, while requiring more time.\n\n2. Common LLM unlearning tasks, such as TOFU, MUSE, and WMDP, are missing.\n\n3. Several standard baselines, like NPO and \"I don't know,\" are not included, which weakens the argument.\n\n4. For robustness, the authors employ memory-free and memory-aided unlearning but do not explore other approaches, such as jailbreak prompts."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This paper proposes a structure-aware parameter-efficient machine unlearning approach (SPE-Unlearn) for Transformer architectures. After reviewing the paper and author-reviewer discussions, I find the current version has unresolved concerns and is not ready for acceptance.\n\n### 1. Complexity Analysis (Reviewer ZE8b)\nThe authors failed to address Reviewer ZE8b’s request for a quantitative complexity analysis of memory and computational efficiency. Their response lacked concrete evidence.\n\n### 2. Implementation Details (Reviewer Lm6c)\nKey implementation details remain unclear:  for example, threshold setting, sparsity setting, and data volume influence.\n\n### 3. Generalizability and Robustness (Reviewer fgpp)\nConcerns about generalizability and robustness remain unresolved:  \n- Benchmarks: Common benchmarks like MUSE and WMDP are not included.  \n- Robustness: No exploration of robustness against attacks (e.g., jailbreak prompts).  \n- Time Cost: SPE-SO’s higher time cost (Table 2) compared to SO is unexplained.\n\n### Conclusion\nWhile the approach is interesting, these unresolved issues in complexity, implementation details, and robustness make it unsuitable for acceptance without significant revision.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "hWF0HH8Rr9",
    "title": "Large-Scale Multi-Agent Reinforcement Learning for Traffic Signal Optimization",
    "authors": [
      "Magnus Müller",
      "Alexander Prochnow",
      "Jonas Otten",
      "Lionel Peer"
    ],
    "abstract": "We present a novel approach to Traffic Signal Control (TSC) in a multi-agent environment by modeling communication among agents as a sequence problem, enabling intersections within road networks to communicate with one another. Taking inspiration from point cloud processing and graph neural networks, we make our architecture capable of handling variable road network topologies, including differing numbers of intersections and intersection types, and demonstrate this by successfully training on real & randomly generated road networks and traffic demands. Furthermore, we demonstrate that even utilizing minimal state information can achieve competitive performance.",
    "keywords": [
      "Reinforcement Learning",
      "Traffic Signal Control",
      "Multi-Agent",
      "Transformer"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=hWF0HH8Rr9",
    "forum_url": "https://openreview.net/forum?id=hWF0HH8Rr9",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper presents a Novel Traffic Sign Control approach in a multi-agent setting by modelling communication as a sequence problem and allowing road networks to communicate. The model uses Graph Neural Networks to handle road network topologies and demonstrate that their approach can provide competitive performance despite minimal state information.\n\nThe paper presets:\n- An automated pipeline for dataset generation\n\n- Utilize a Transformer to model inter-agent dependencies by encoding state history.\n\n- Capability to model non-uniform input size based on number of intersections and intersection sizes.\n\n- A Model that can provide competitive performance to baselines without using expensive sensors.",
        "strengths": "- Experiments are done in both simple and complex network designs and show that ththe results are comparable to models that contain more information.\n \n- Lane encoding is clearly presented in Figure 1.\n\n- Utilizing existing Traffic Simulation models to verify the capabilities.",
        "weaknesses": "- It is unclear how the Graph Neural Network is used in this paper.\n\n- Length of the Sequences for the provided results is not mentioned. It is unclear how the transformer based model would perform for longer sequences.\n\n- Page limit is exceeded as per Lines 423 to 427."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a Traffic Signal Control framework modeled as a multi-agent environment. The proposed framework consists of a Transformer architecture for feature extraction and an MLP for computing the action. The trained policy can be implemented for networks of different size.",
        "strengths": "The paper is well structured and was easy to follow. The problem considered in this manuscript is a very important problem and has been motivated very well in the introduction. The problem description and formulation is adequately. The use of transformer architecture is also well justified.",
        "weaknesses": "1) Clarity of writing and presentation: There are multiple instances where the paper lacks clarity in terms of writing and the meaning can only be understood with someone who are in this domain. \n\n2) Novelty: The paper does not propose any innovative approach. The use of Transformer or Transformer type architectures with RL and MARL is a well-known approach. This paper showcases the implementation of the architecture for the TSC problem.\n\n3) Baseline comparison: There should be a comparison to traditional TSC algorithms such as SCOOT. Without a comparison to traditional baseline methods, it is not possible to infer the effectiveness of the method.\n\n4) The abstract can be more meaningful. The abstract should cover the outline of the paper. But that’s not the case here.\n\n5) What is the 'static' baseline comparison? Please have a formal definition for it.\n\n6) Figure 7b and 7c: it would be nice if a different color palette and markers were used to represent these plots. In its present for it is very difficult to interpret.\n\n7)Font size for all the plots can be increased"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This work proposes a novel RL framework for large scale traffic signal control, independent of the road network topology, based on the transformer architecture, together with a pipeline for generating diverse environments for this setting.",
        "strengths": "This work addresses an important and challenging setting, traffic signal control. It demonstrates the approach on large-scale settings, with up to 73 agents.",
        "weaknesses": "The work has merits and interesting contributions, but I argue it also has clarity issues and requires additional work on the empirical validation and presentation. I detail below some key issues and point to a few questions that can hopefully guide the future development of this contribution. \n\nThe first remark concerns the problem formulation. The multi-agent setting is defined using the MDP framework (i.e., single-agent setting), but using a joint action space. Should we understand that the MARL setting is approached using a centralised learning paradigm? Also the motivation for learning enriched states using information exchange signals a partially observable setting. I advise to reconsider the mathematical framework, given all these elements. Perhaps a Dec-POMDP [1] is more appropriate? See Q1. \n\n\nFurther clarity issues regards the two stage state encoding:\n- The idea of using PointNet to generate an encoding independent of the road network size is interesting. But one can also argue that lane level information is detailed information, that is not always available. See Q3. \n- It is not clear how exactly the communication is defined as a sequence modeling problem. As far as I understand the transformer further encodes into the state the topology information. See Q4. \n\nFinally, the evaluation was only performed against MAPPO and it is unclear what the 300 mentioned experiments were, since the presented results do not seem to be averaged over multiple runs. There are numerous potential baselines that could strengthen the results: [2, 3, 4]. While the related work was great within the application domain, there is a lot left to explore on the algorithmic side. \n\n[1] Oliehoek, F. A., & Amato, C. (2016). A concise introduction to decentralized POMDPs (Vol. 1). Cham, Switzerland: Springer International Publishing.\n\n[2] Wen, M., Kuba, J., Lin, R., Zhang, W., Wen, Y., Wang, J., & Yang, Y. (2022). Multi-agent reinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35, 16509-16521.\n\n[3] Jiang, J., Dun, C., Huang, T., & Lu, Z. Graph Convolutional Reinforcement Learning. In International Conference on Learning Representations 2020.\n\n[4] Sheng, J., Wang, X., Jin, B., Yan, J., Li, W., Chang, T. H., ... & Zha, H. (2022). Learning structured communication for multi-agent reinforcement learning. Autonomous Agents and Multi-Agent Systems, 36(2), 50."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a multi-agent RL method to enable communication between multiple intersections using sequence modeling. It also introduces a pipeline for dataset and road network generation. It further tries to handle issues with minimal state information.",
        "strengths": "The originality is good. It aims to solve some real-world problems in current traffic signal control research, such as varying network and intersection structures, multi-intersection coordination, and the lack of expensive sensor-captured data.",
        "weaknesses": "* The abstract doesn’t provide enough information about the problem, method, and contributions.\n* Figures 5 and 6 both contain irrelevant parts in the screenshots. Moreover, the authors should use high-definition figures instead of screenshots.\n* It’s not clear what the “difference in vehicle waiting time” means in the reward setting in Line 159.\n* This paper states that one major contribution is the dataset, traffic flow and road network generation. However, how you generate them is not clearly explained.\n* There are not any latest baseline methods for comparison.\n* No uncertainty evaluation for any experimental results. Multiple runs are necessary for model evaluation.\n* The experiments lack comprehensiveness, and the analysis does not provide sufficiently convincing insights.\n* No code or implementation details are provided."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "The paper introduces Traffic Signal Control (TSC) in a multi-agent communication problem. They demonstrate the approach by training on real and randomly generated road networks with varying traffic demands, showing that competitive performance can be achieved even with minimal state information. \n\nReviewers acknowledge the importance and challenge of the traffic signal control setting, noting that the approach is tested on large-scale settings with many agents. However, several reviewers raise concerns about the clarity of presentation, and the novelty of the proposed method. Reviewers also question the formulation of the problem as a Markov Decision Process (MDP), suggesting that a Dec-POMDP might be more appropriate. Additionally, reviewers request a comparison to traditional TSC algorithms and other MARL methods, and request multiple runs. Overall, the reviewers generally agree that there are some interesting ideas, but that there are significant issues with clarity, novelty, and experimental validation that need to be addressed.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "owR9ofvkFQ",
    "title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data",
    "authors": [
      "Meng Fang",
      "Xiangpeng Wan",
      "Fei Lu",
      "Fei Xing",
      "Kai Zou"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced natural language understanding and demonstrated strong problem-solving abilities. Despite these successes, most LLMs still struggle with solving mathematical problems due to the intricate reasoning required. This paper investigates the mathematical problem-solving capabilities of LLMs using the newly developed ``MathOdyssey'' dataset. The dataset includes diverse mathematical problems at high school and university levels, created by experts from notable institutions to rigorously test LLMs in advanced problem-solving scenarios and cover a wider range of subject areas. By providing the MathOdyssey dataset as a resource to the AI community, we aim to contribute to the understanding and improvement of AI capabilities in complex mathematical problem-solving. We conduct benchmarking on open-source models, such as Llama-3, and closed-source models from the GPT series and Gemini models. Our results indicate that while LLMs perform well on routine and moderately difficult tasks, they face significant challenges with Olympiad-level problems and complex university-level questions. Our analysis shows a narrowing performance gap between open-source and closed-source models, yet substantial challenges remain, particularly with the most demanding problems. This study highlights the ongoing need for research to enhance the mathematical reasoning of LLMs. \nThe dataset, results, and evaluation code are publicly available.",
    "keywords": [
      "Math",
      "LLMs"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=owR9ofvkFQ",
    "forum_url": "https://openreview.net/forum?id=owR9ofvkFQ",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The manuscript presents an original and challenging dataset for mathematical problem-solving, encompassing various subjects and difficulty levels. Then the paper conducts comprehensive examinations on both open-source and closed-source LLMs. The findings reveal that while closed-source models currently lead, open-source models are rapidly catching up, highlighting the competitive landscape of LLM capabilities in mathematical problem-solving.",
        "strengths": "* The paper is well-motivated, as GPT-4o poses a significant challenge to current mathematical benchmarks. The introduction and open-sourcing of high-quality, difficult mathematical problems is a meaningful contribution to the field.\n\n* The dataset features distinct levels of difficulty and sub-domain classifications, which enhance its uniqueness.",
        "weaknesses": "1. The manuscript lacks coverage of important related work and further clarification on the difference and improvements compared to them.\n    * OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems\n    * Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models\n    * OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI\n    * Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models\n\n2. There are missing baseline comparisons that are crucial for evaluating the open-source models on the proposed dataset, like Qwen2.5-MATH, DeepSeek-Coder, and so on.\n\n3. It is unclear how the authors ensure that the data has not been previously encountered. If the problems are original, details regarding the creation principles and methodologies should be included in the paper. Additionally, how is the correctness of answers verified? Have the authors conducted cross-validation or sampling tests to ensure reliability? What is the accuracy rate?\n4. The conclusions drawn seem predictable and do not provide substantial insights. Are there fine-grained analyses and interesting findings?"
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors identify a need for a mathematics benchmark that spans a wider breadth of topics and difficulties. They propose MathOdyssey, a benchmark that includes hand-written and curated high-school, university, and Olympic-level problems. Each problem has a unique expected answer and detailed reasoning to aid LLM assessment. They demonstrate that the benchmark is not saturated since GPT-4 o1-preview archives ~65% overall. Further, their wide coverage of topics enables the identification of problem topics for LLMs enabling researchers to focus on those areas.",
        "strengths": "+ The proposed benchmark should have a unique answer enabling easier verification of the correct answer.\n+ The problems are crafted specifically for the benchmark avoiding their presence in the pre-training data for LLMs.\n+ The problem space covered in terms of topics and difficulty is wide, allowing the identification of problem areas for further research as well as \"solved\" areas if a topic saturates.",
        "weaknesses": "- Some of the paper language is hyperbolic, for example, S3.1, \"Design Principle.\" Paragraph, L178: \"representing the pinnacle of human intellectual achievement\" is very strong language and am uncertain the authors could substantiate such a claim unless it is an opinion. Another example is S3.1 L234: \"This rigorous process\": The curation process, while I can trust was done rigorously, is not presented in sufficient detail for me to make that assessment, and it would be better to instead tone it down to \"This process facilitates the quality and dependability...\". The paper in general would benefit from a pass that tones down the hyperbolic language to instead focus on the proposed advancements in an objective tone.\n- L256-264 could be replaced with \"Fig. 1 shows the detailed information\". and L266-268 repeat the same information that is already in the figure and could be pointed to.\n- The benchmark claims easy verifiability by code but uses GPT-4 as an evaluator (with the associated errors this induces even if the prompt enables the use of tools).\n- The ease of having a unique answer is counter-balanced by the false positives induced by correct-answer-with-wrong-reasoning."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "The paper aims to investigate and understand LLMs’ strong problem-solving abilities. The paper introduces a novel dataset “MathOdyssey”. The dataset includes a diverse set of mathematical programs at three levels: High school, university and olympiad level. Each category has a wide range of different problem areas such as Algebra, Number Theory, calculus etc. The dataset contains a total of 387 data points and has novel problems created by mathematics professionals, including high school educators, university professors and researchers. Each problem is accompanied by its final answer and its reasoning chain. They also did a comprehensive evaluation of the dataset and tested it on both closed and open models. They used GPT-4 to assist in evaluating the model accuracy, as the dataset contained a wide range of answer types (open answer, MCQ, and true-false). The evaluation shows that the closed source model particularly GPT-4, o1 and GPT-4 Turbo shows strong performance in high school and university-level math. For open-source models such as Llama-3, the results show that the selected open-source models only surpass the performance of GPT3.5 but are also approaching the capabilities of the earlier version of GPT-4.",
        "strengths": "1. Release of a novel dataset that will help the community as this dataset has not been used in the training of existing models.\n2. Comprehensive benching of different models, highlighting their efficiency in solving different categories/areas of problems.\n3. Effective use of GPT-4 for evaluating the accuracy of models. The author employs a prompt-based method and provides scores of various categories.",
        "weaknesses": "1. Even though the dataset provides various categories of questions in different areas, the count of individual categories is very small. For example Number Theory – Olympiad-level accounts for only 4 problems, Differential Equations – University-level for 14 problems etc. So do the authors have any plan to extend the count of problems in these areas?\n2. Even though the dataset does not use any existing problems, a sanity check for data contamination should be done. Experiments from the paper [1] should be added to ensure no data contamination.\n3. Evaluating the dataset using models fine-tuned specifically for solving math problems, such as MathCoder [2] helps show how models trained specifically to solve math problems perform on MathOdyssey.\n\nReference:\n\n[1] : Golchin, Shahriar, and Mihai Surdeanu. \"Time travel in llms: Tracing data contamination in large language models.\" arXiv preprint arXiv:2308.08493 (2023).\n\n[2] : Wang, Ke, et al. \"Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning.\" arXiv preprint arXiv:2310.03731 (2023)."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents a new dataset called MathOdyssey, which aims to evaluate the reasoning abilities of large language models (LLMs). The dataset consists of 387 problems, including 148 at the Olympic level, 101 at the university level, and 138 at the high school level. The problems cover several subjects with three answer formats: true/false, multiple choice, and open answers.\n\nThe authors evaluate LLMs' math reasoning performance on MathOdyssey using GPT-4 as the answer judger in a zero-shot manner, providing it with specific instructions. They conduct their experiments with seven closed-source LLMs and one open-source model, Llama-3-70B. Their findings reveal that the Llama-3-70B model still falls short when tackling more complex problems.",
        "strengths": "The proposed MathOdyssey dataset is novel and may be somewhat useful to certain researchers. While it introduces a variety of challenging problems, the experimental results provide a rough indication of the performance of closed-source LLMs. Overall, the dataset and findings suggest potential (but limited) usefulness to offer insights into LLM capabilities in mathematical reasoning, albeit with room for more comprehensive analysis.",
        "weaknesses": "MathOdyssey **offers no clear advantages over existing benchmarks**, which may limit the usefulness and contribution of this paper.\n\n- Compared to existing datasets, MathOdyssey is **limited in size**, containing only 387 problems, whereas datasets like GSM8K and MATH include 1,319 and 5,000 problems, respectively. This limitation might impact the reliability of accuracy in ranking the mathematical reasoning abilities of different LLMs.\n\n- **The \"difficulty levels\" within MathOdyssey are not well-defined**. Although it claims to cover comprehensive levels of math problems, it includes only three educational stages. In contrast, the MathBench [1] dataset offers a wide range of problems, spanning from primary school to university level. By the way, some datasets define the difficulty level as a rating (e.g. an integer number)\n\n- While the authors claim to have diversified answer types, MathOdyssey **only encompasses three distinct answer types**. OlympiadBench [2], however, incorporates a more fine-grained variety of answer types.\n\n- Although MathOdyssey includes several subjects, **the number of testing examples within each subject is relatively small, with many subjects containing fewer than 10 examples**. This limitation may lead to inaccurate analyses across different subjects.\n\n**The experiments are not comprehensive and compelling**:\n\n- The evaluation process is flawed because the authors use GPT-4 as the judge for answers in a zero-shot manner. However, **it is unclear how often this judgment aligns with human evaluators**. An analysis of judgment errors is necessary, and I recommend considering rule-based matching.\n\n- They **include only one open-source LLM, Llama-3-70B** in experiments, which is not comprehensive. The authors should include more open-source LLMs, including both general-purpose chat models and math-specialized LLMs.\n\n- The results and analysis are not compelling, as the reliability of GPT-4's judgment is uncertain. Additionally, **conducting an error analysis could provide valuable insights into how LLMs get wrong in solving math problems**.\n\n\nThis paper **is poorly written** and either **lacks important details or makes inaccurate claims or claims without proper citations** (see Questions).\n\n\n[1] Liu, H., Zheng, Z., Qiao, Y., Duan, H., Fei, Z., Zhou, F., ... & Chen, K. (2024). MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark. arXiv preprint arXiv:2405.12209.\n\n[2] He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., ... & Sun, M. (2024). Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008."
      }
    ],
    "rating_avg": 4.5,
    "confidence_avg": 3.75,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "vVlNBaiLdN",
    "title": "ESMGain: Effective and Efficient Prediction of Mutation’s functional Effect via ESM2 Transfer Learning and robust Benchmarks",
    "authors": [
      "Moritz Glaser"
    ],
    "abstract": "Functional effect prediction of mutations, especially for properties like catalytic activity, holds greater significance for clinicians and protein engineers than traditional pathogenicity predictions. Recent approaches leveraging static ESM1 embeddings or multimodal features (e.g. embeddings, structures, and evolutionary data) either (1) fall short in accuracy or (2) involve complex preprocessing pipelines. Moreover, functional effect prediction suffers from (3) a lack of standardized datasets and metrics for robust benchmarking. We address these challenges by systematically optimizing ESM2-based functional effect prediction: Through extensive ablation studies, we demonstrate that fine-tuning significantly outperforms static embeddings, scaling laws for model size are non-transferable and LoRA matches full fine-tuning performance, deviating from trends observed in natural language processing. Our framework, ESM-Effect, fine-tunes 35M ESM2 layers with an inductive bias regression head achieving state-of-the-art performance. It slightly surpasses multimodal competitor PreMode indicating redundancy in structural and evolutionary features. We further propose a benchmarking framework featuring robst test datasets and strategies, and the relative Bin-Mean Error (rBME), as a metric designed to emphasize prediction accuracy in challenging, non-clustered, and rare gain-of-function regions. rBME better reflects model performance compared to commonly used Spearman’s rho, as evidenced by improved plot-based analyses. As ESM-Effect exhibits mixed transferability to different unseen mutational regions, we identify multiple areas for improvement such as finer-grained pretraining strategies.",
    "keywords": [
      "protein",
      "language model",
      "deep learning",
      "biology",
      "gain of function",
      "enzyme"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=vVlNBaiLdN",
    "forum_url": "https://openreview.net/forum?id=vVlNBaiLdN",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a method for fine-tuning protein language models, specifically ESM2, using deep mutational scanning (DMS) data. The fine-tuning process involves generating both local and global representations of the reference and mutant protein sequences by utilizing separate, mostly frozen ESM models for the two sequences. These representations are combined and passed through a two-layer linear neural network to predict quantitative measurements from a DMS assay.\n\nFurther, the authors propose two modifications to the evaluation of fine-tuned models. First, they recommend fine-tuning models on one protein and testing them on a different protein within the same family, rather than using held-out positions from the original protein. Second, they suggest calculating correlation metrics separately for LoF, neutral, and GoF mutations. These separate correlation scores are then combined using a harmonic mean to produce a single protein-level metric.",
        "strengths": "1. The authors introduce important ideas for better evaluating fine-tuned models: (a) evaluating models on completely held out proteins and (b) developing a metric that prioritizes performance on LoF and GoF variants over neutral variants.\n2. Their fine-tuning approach demonstrates superior performance compared to existing methods, such as PreMode and augmented versions of unsupervised models.\n3. Through ablation studies, the authors establish that using larger versions of ESM2 does not significantly improve performance and that employing separate models for reference and mutant sequences provides some benefits.",
        "weaknesses": "1. Limited dataset evaluation: The authors do not evaluate their method on the large compendium of DMS datasets that are available in ProteinGym (217 datasets covering 2.5 million mutations), instead focusing on only 5 datasets (Figure 2). To convincingly prove that their fine-tuning approach outperforms existing methods, they should expand their analysis to more datasets.\n\n2. Insufficient comparison to existing fine-tuning approaches: PreMode and augmented unsupervised models are not the only approaches that have been proposed to fine-tune protein language models on DMS datasets. See https://www.nature.com/articles/s41467-024-51844-2 and https://arxiv.org/pdf/2405.06729. These papers explore strategies such as parameter-efficient fine-tuning and fine-tuning jointly on multiple DMS assays that this paper does not consider. In particular, the approach proposed in the second paper listed above shows improved performance on entirely held out proteins, which is in stark contrast to the poor generalization to new proteins exhibited by ESMGain in Fig. 4. \n\n3. While the idea to compute separate correlation metrics for LoF, neutral, and GoF variants is clever, the method of dividing variants into these categories by splitting the ground-truth scores into thirds is arbitrary. A more robust method, such as a Gaussian mixture model with three components, could provide a more principled assignment of variants to these classes."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This work proposed a method called ESMGain to use fine-tuning ESM2 with a custom regression head incorporating inductive biases and enable the application of learned protein semantics to functional effect prediction. This method outperforms state-of-the-art competitor PreMode on deep mutational scans from three different enzymes.",
        "strengths": "1. The proposed method performs the best for functional effect prediction in the dataset.\n2. The methodology of ESMGain can predict functional effects without the limitation of feature redundancy and task specificity.",
        "weaknesses": "1. The organization needs improvement. Some terms like \"PTEN\" didn't have full names. The size of font in those figures is too small to read and is not consistent. The section 7 should be in the section of the experiential setup.\n2. In Fig4, \"LoF, Neutral and GoF\" in captions should be the same as the text in x axis of figure. How about the performance in all other baselines like competitor PreMode in Fig4?\n3. Have you conducted multiple train-test split seeds in ablation study of ESMGain? Why the result of the original ESMGain in the ablation study is different from the one in Fig2? Do they use different datasets or strategies to train and test?"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper proposes a novel model method  ESMGain for predicting the functional impact of protein mutations, expanding addressing limitations in existing binary pathogenicity predictors. By fine-tuning ESM2 embeddings with a custom regression head, ESMGain aims to accurately classify mutations as loss-of-function, neutral, or gain-of-function. Through evaluations  in catalytic activity prediction tasks, ESMGain outperforms the state-of-the-art baselines by leveraging only ESM2 embeddings. Besides, the authors propose a new benchmarking framework for functional effect prediction, emphasizing cross-protein generalization tests within the same protein family. A Harmonic Spearman metric is also introduced to balance performance evaluation across mutation effect categories.",
        "strengths": "1) The paper is well-structured and clearly written.\n\n2) The proposed method achieves state-of-the-art performance on selected datasets in functional effect prediction.\n\n3) By employing two independent ESM2 models to embed wildtype and mutant sequences separately, the paper addresses potential information loss in mutation representation, enhancing the model’s ability to capture subtle differences. Ablation studies demonstrate that only using ESM2 embeddings effectively captures most of the relevant information on DMS datasets,  effectively reducing the reliance on additional data modalities.\n\n4) The paper proposes a novel benchmarking framework for functional effect prediction incorporates a cross-protein generalization test within the same protein family.",
        "weaknesses": "1) The novelty of this paper is limited. The use of dual ESM2 embeddings to separately represent wildtype and mutant sequences, along with the introduction of the Harmonic Spearman metric to address label imbalance, appears more incremental than groundbreaking.\n\n2) It seems that the motivation of the proposed benchmarking framework is underdeveloped. While focusing on cross-protein generalization within the same family is technically interesting, it lacks a clear connection to real-world situations where this type of evaluation would be essential.\n\n3) While ESMGain performs well on the tested DMS data, its generalization to other samples within the same protein family is weak (cross-family tests). The model may be overfitting in the specific training proteins."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces a method for mutation effect prediction. The method relies on two ESM2 heads for generating protein sequence embeddings, one used with wildtype sequences and the other one for the mutated sequences. On top of the embeddings a custom regression head is trained. The technical novelty of the method is their design of the regression head and the fact that the two ESM2 models have different weights one fine-tuned for wildtype sequences and the other for the mutated sequences. Other contributions claimed by the paper are towards better bencharking (i) testing generalization of the models fine-tuned on one protein by testing them on a different protein from the same family and (ii) introduction of “Harmonic Spearman” as a new metric.",
        "strengths": "1. The generalization test is of interest. In Figure 3, authors show the different distribution of labels for two different proteins from the same family and convincingly show why generalization between proteins (even in the same family) is not easy.",
        "weaknesses": "1. Paper is poorly structured, making it very hard to read:\n\n\t- Introduction contains contents which would better fit to related work or background (“Notably, PreMode was pre-trained to predict the binary measurement of “pathogenicity” for 4.7 million mutations and uses AlphaFold2 predicted protein structure, Multiple Sequence Alignments (MSAs) and pre-trained ESM2 650M embeddings as features (John Jumper, 2021).”) And it also presents some results and their discussion (“That leads us to hypothesize that the signal provided by protein structure, MSAs, and embeddings is largely redundant for the task of effect prediction. PreMode’s ablation studies show minimal performance drop when any of these modalities is ex- cluded, suggesting that they capture overlapping information for functional effect prediction. This explains ESMGain’s superior performance in turn: its fine-tuned embeddings are task-specific and the single modality avoids the redundancy.”). I suggest honoring the usual structure of the paper and using introduction just for motivation and a very brief (not so detailed) teaser for the contributions of the paper.\n\n    - Chapter 4, which should be describing the technical novelty and the method does not provide that many details, for example Figure 1 illustrating the method is never referenced in the text. I suggest to use a figure and equations to better describe the regression head, instead of the textual description at the end of section 4.2.\n\n    - No table summarizing results. The reported numbers are scattered across text and some figures, making it very hard to get a glimpse of the results. I suggest a more transparent summarization of the results, such as by using a table.\n\n2. Poor formatting of the paper.\n\n    - Authors are not economical with the space by being sometimes too verbose, repetitive in repeating their contributions or for example by wasting the whole first page just on abstract. Being more economical would enable the authors to make bigger figures which have too small fonts and are hard to read. I suggest making figure large enough so the fonts can be legible. \n\n    - References are poorly formated. Some references starting with “…”. AlphaFold referenced as “(John Jumper, 2021)” - note that AlphaFold was a collective effort. I suggest proper citing and formatting of references.\n\n3. Insufficient literature survey. Authors only have 13 references. I suspect authors were trying to fit into the page limit of 10 pages including references - this is not necessary references dont count in the page limit. I suggest making proper literature survey and crediting relevant work. For example, I miss the reference to ProteinGym, arguably one of the most influential benchmarks in this area.\n\n4. Insufficient benchmarking. Authors only focus on the comparison to PreMode (which was still not peer reviewed) and only compare on 5 proteins. I suggest to compare for example to AlphaMissense as well.\n\n5. The key contribution of having separate ESM2 heads for wildtype and for the mutated sequence is questionable. Authors claim this to give them the key improvement by the underlying inductive bias. To me it is not clear how to decide what is wildtype and what is mutation. What if the mutation is adopted by evolution and becomes the “new wildtype” and then gets mutated again? There is no fundamental reason to distinguish between the sequences. So I believe that using the distinction between the sequences based on the dataset definition and then adapting the two heads to this definition only leads to overfitting to the dataset, potentially explaining any benefit gained from these separate heads. I dont have a concrete suggestion how to prove authors point, because I think the point is wrong. If authors stand by their point they should present convincing evidence supporting that their “inductive bias” is not just overfitting to the dataset definition of what is wildtype and what is mutation.\n\n6. The model seems to improve over PreMod on just 2-3 out of 5 proteins (Figure 2), this does not seem very convincing. My suggestion would be to get other datasets (maybe something relevant could be found in ProteinGym) and show improvement on other dataset as well.\n\n7. The Harmonic Spearman is just introduced at the end of the paper and not motivated well enough. Could authors explain the choice of using harmonic average? Could authors clearly compare harmonic spearman to normal spearman? How does it change the evaluation of all the benchmarked models? A table summarizing the results (as suggested in Weak point 1) would help.\n\n\nI suggest to reject this paper for the following reasons. (i) The paper is not is well placed in literature, comparison to AlphaMissense is missing and the survey of the related work is not sufficient. (ii) The key contribution of using two separate ESM2 models for the wildtype and the mutated sequence is questionable and the claim of bringing a useful inductive bias is not supported by strong evidence, the improvement coming from this choice might be due to overfitting to the dataset definition of what is mutant and what is original sequence. (iii) The results dont seem as strong, only showing improvement for 2-3 out of 5 proteins. More convincing evaluation using other dataset would be necessary. (iv) The technical novelty of separate fine-tuning of two ESM models with a custom regression head is limited. (v) The writing is poor, making it hard for the reader to asses the contributions, the results of the method and its placement in the literature."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "The paper considers the problem of functional effect prediction and develops an ESM2-based framework that is tailored to this task through ablation studies.\n\nThe paper provides interesting insights such as the conclusions drawn from the ablation studies and the resulting framework outperforms comparison approaches. However, all reviewers agree that the approach is of limited novelty. It is therefore critical to provide a high-quality evaluation, where none of the results can be questioned and where all conclusions are clearly presented. \n\nThe reviewers made excellent suggestions, e.g. concerning potential bias resulting from the use of separate ESM2 heads, regarding generalization test, regarding scores, etc. Towards this goal, the authors have made substantial revisions to the manuscript, which require a new cycle of reviews.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "FYvZCwdb6F",
    "title": "MOMENTUM MEETS VIRALITY: A NOVEL METRIC FOR UNMASKING SOCIAL BIAS IN VIRAL TWEETS",
    "authors": [
      "Nihar Ranjan Sahoo",
      "Arif Ahmad",
      "Nishtha Madaan",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Predicting which social media posts will go viral is a critical but complex task in the field of computational social science. Previous studies have utilized various measures to forecast the virality of tweets or Facebook posts, but these approaches exhibit limitations, particularly in the absence of a virality metric that specifically considers social biases. In this paper, we test existing metrics and introduce a new metric, $\\textbf{ViralTweet Score (VTS)}$, inspired by principles of momentum from physics to better predict a tweet's virality given that it consists of social biases. We compare this new metric with others, highlighting the advantages and disadvantages of each of them as a virality measurement metric. We release the $\\textbf{ViralTweets Dataset}$ with $\\mathbf{88.8k}$ Hindi tweets and corresponding virality labels based on our VTS metric. We also show how social biases in posts can influence their potential to go viral. We test our hypothesis that VTS is a better metric using two methodologies and we show how VTS achieves an F1 score of 0.87 based on pairwise evaluation methodology and an overall F1 score of 0.58 based on our clustering-based verification methodology. Our work offers a novel metric for understanding tweet virality for biased tweets and opens the door for more equitable and effective social media analytics by considering the role of social biases in virality.",
    "keywords": [
      "Social bias",
      "Tweet virality",
      "ViralTweetScore",
      "Hindi Tweets",
      "Tweets"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=FYvZCwdb6F",
    "forum_url": "https://openreview.net/forum?id=FYvZCwdb6F",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "**Summary**\nThe paper explores tweet virality through a metric, ViralTweet Score (VTS), designed to better predict which tweets go viral, particularly those with social biases.\n\nKey contributions are:\n\n* ViralTweet Score (VTS): VTS measures virality by tracking the spread momentum of tweets over time, factoring in likes and retweets weighted by user follower count. This approach relies on the speed and extent of engagement in addition to static metrics like total likes or retweets.\n\n* ViralTweets Dataset: The authors released a dataset of 88.8k Hindi tweets, labeled with binary bias indicators, categories of bias, and toxicity markers, to study social bias in social media virality. Labels were assigned using multiple different models including LLMs and human annotation for quality control.\n\n* Social Bias in Virality: The study focuses on social biases inherent in viral tweets, such as biases related to gender, religion, caste, and politics, which are prevalent in Indian social media content. The authors examine whether biased tweets exhibit higher virality, using VTS as the primary measurement tool.",
        "strengths": "**Originality**\n* VTS: By focusing on the rate of engagement growth (velocity) and using user follower count (mass), VTS captures virality dynamics more effectively than static metrics like total likes or retweets.\n\n**Significance**\n* Practical Relevance: Given the increasing concerns over misinformation and biased content on social media, the study’s insights are timely. VTS could help platforms and researchers identify harmful content trends more accurately.\n\n* Focus on Social Bias: The paper addresses the socially impactful issue of bias in viral content. By measuring how bias may amplify virality, the study provides valuable insights for understanding the propagation of biased content on social media.\n\n**Clarity**\n* Writing and Organization: The paper is organized logically with minimal grammatical errors, moving from an introduction of the problem to the proposed solution, methodology, experiments, and results. Each section is well-defined, making it easy for readers to follow the research journey. I appreciate authors mentioned the filtering methodology used to filter from 9.24M tweets.",
        "weaknesses": "**Evaluation and Results**\n \n* Lack of baselines: Although VTS outperforms some traditional metrics (likes, retweets), it lacks benchmarks from other studies and comparable metrics, even though some of them might be referenced. I advise the authors to include benchmarks from other similar studies on virality prediction [1, 2, 3] to see how VTS compares against them.\n\n* Lack of latest related works: Recent research has moved significantly beyond purely text-based approaches, incorporating multi-modal data for virality prediction [4, 5]. The paper should compare and mention the advantages of their study relative to some of the deep learning-based approaches.\n\n* VTS score analysis with bias clusters: The results section lacks any detailed evaluation of VTS scores in relation to bias. The study simplifies bias by categorizing tweets as either \"biased\" or \"non-biased,\" without distinguishing between different types of biases (e.g., gender, religion, caste). This binary approach may obscure insights into how specific biases impact virality differently, limiting the granularity of the results. The study should provide a detailed analysis of different types of bias clusters and how they interact with virality. The authors should also discuss the limitations of VTS for different types of biases.\n\n* Lack of generalizability :The study focuses solely on Hindi tweets, which limits the generalizability of the VTS score. The result evaluation should be conducted across multiple languages and regions to assess the value of the VTS score in broader contexts.\n\n* Limited analysis of false positives and negatives: While the paper provides precision, recall, and F1 scores for its metrics, it lacks an in-depth analysis of false positives and false negatives in bias and virality classification. An error analysis could highlight specific areas where VTS or the bias detection model falls short, such as overestimating virality for certain topics or missing nuanced biases.\n\n**Novelty**\n\n* The novelty here lies in how the study tailors VTS to biased content and the Hindi social media context. The VTS itself is more of a refinement of existing temporal models.\n\n* Other studies [6] have evaluated the spread of misinformation and biases on social media. The paper should attempt to distinguish or specify challenges related to these studies.\n\n\n[1] Kwak, Haewoon, et al. \"What is Twitter, a social network or a news media?.\" Proceedings of the 19th international conference on World wide web. 2010.\n\n[2] Goel, Sharad, et al. \"The structural virality of online diffusion.\" Management science 62.1 (2016): 180-196.\n\n[3] Weng, Lilian, Filippo Menczer, and Yong-Yeol Ahn. \"Virality prediction and community structure in social networks.\" Scientific reports 3.1 (2013): 1-6\n\n[4] Gao, Liqun, et al. \"Public opinion early warning agent model: A deep learning cascade virality prediction model based on multi-feature fusion.\" Frontiers in Neurorobotics 15 (2021): 674322.\n\n[5] Zhang, Xuan, and Wei Gao. \"Predicting viral rumors and vulnerable users with graph-based neural multi-task learning for infodemic surveillance.\" Information Processing & Management 61.1 (2024): 103520.\n\n[6] Vosoughi, Soroush, Deb Roy, and Sinan Aral. \"The spread of true and false news online.\" science 359.6380 (2018): 1146-1151."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper tests existing metrics and introduce *ViralTweet Score (VTS)* to predict tweet's virality with a specific focus on social biases. VTS incorporates engagement growth over time to capture \"momentum\", offering a more dynamic perspective on virality in biased tweets. The authors also release the dataset and corresponding virality labels. Compared to traditional metrics, VTS more accurately predicts virality and provides insights into how social biases shape online discourse, with implications for more responsible and equitable social media analytics.",
        "strengths": "1. This paper introduces a novel metric for capturing the \"momentum\" of engagement, which provides a new view of virality based on the rate of engagement growth over time.\n2. This paper links virality with social biases, which focuses on understanding how biased content can spread quickly on platforms and provides valuable insights into societal impacts and the role of bias in information dissemination.\n3.  The paper employs both supervised and unsupervised methods to validate the effectiveness of VTS, including pairwise tweet comparisons and clustering techniques. This dual approach strengthens the evaluation of VTS, showing its effectiveness across different predictive contexts.\n4. Open source the dataset, which will definitely promote relevant research in this area.",
        "weaknesses": "1. Lack of a detailed codebook for bias labels: The paper relies on social bias categories like gender, caste, and religion, but it does not provide a clear and detailed codebook or criteria for how these biases were defined and identified, especially by human annotators. \n2. A little bit of over-reliance on follower count: The use of follower count as the “mass” component in VTS could bias the score towards tweets from popular accounts, which might overshadow the organic virality of tweets from less popular users.\n3. Too few case studies: This paper identifies how biased tweets can become viral, but it does not delve into the practical implications of this finding, such as how VTS might inform moderation practices on social media platforms."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "This manuscript addresses the challenge of predicting social media post virality by introducing the \"ViralTweet Score (VTS).\" It presents the viraltweets dataset, comprising approximately 89,000 Hindi tweets with labels gathered in 2019. The primary goal is to investigate the factors driving tweet virality in the Indian context and to explore whether virality links to social biases within the tweet. The VTS is proposed as a more effective measure of tweet virality, revealing a correlation between virality metrics and the biases present in the tweets.",
        "strengths": "- The study of what goes virality is important for understanding the current social communication landscape and societal problems. \n\n- Non-english social media dataset is rarer and valuable.",
        "weaknesses": "- I believe that the manuscript is not a good fit for the ICLR conference because the central contribution of the paper is the ViralTweet Score (VTS) -- a hand-crafted score to measure the virality of tweets. Thus, I cannot see meaningful contribution to the field of representation learning. Although the study uses some LLMs, but the usage is rather secondary and not the main focus of the paper.\n\n- Data collection procedure is not described in enough details. For instance, it is not clear how the initial 9.24 million tweets were selected. Was it from the streaming API or search API? How was the Hindi language tweets detected?\n\n- The dataset includes only tweets with interactions from four specific dates—likely capturing many viral tweets, but too limited to adequately represent non-viral tweets, which could be valuable as negative or control data points."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This study proposed a new metric, ViralTweet Score (VTS), to measure the virality of tweets. It was designed to capture multiple aspects of a tweet and their evolution over time. This study also proposed a dataset containing 88.8k Hindi tweets associated with VTS.",
        "strengths": "(1) This study touched on an important research topic.\n\n(2) The manuscript is easy to follow.",
        "weaknesses": "(1) The size of the original dataset was 9.24 million. In the end, only 88.8k tweets were preserved. Would there by any selection biases during data preprocessing? The size was significantly reduced (7 million to 200 thousand) after removing tweets that did not have time-series data spanning over one day. It suggests that the remaining tweets were popular. Would this introduce biases? Next, only tweets with at least four distinct time-series data points were kept. Did this suggest that the proposed metric might not generalize well?\n\n(2) The motivation of the study does not seem clear. In sections 3.2 and 3.3, the authors discussed content virality and virality metrics. However, it is unclear to me why the existing metrics were not good enough so that a new metric had to be proposed.\n\n(3) The Cohen's Kappa between models and humans is low.\n\n(4) Some text descriptions did not seem to align with the tables they referred to. \"These tweets are distributed across various categories of social biases, as shown in Table 5 of Appendix A.\" However, Table 5 only shows the total number of unique tweets, average likes, retweets, replies, and number of time series points per tweet in the dataset. It did not relate to categories of social biases.\n\n(5) In section 4.2, the authors discussed bias label, however, the definition of such bias was never provided. It was only until Section 4.4, the authors provided examples of the bias categories. However, why were they considered biases? Categories such as gender, religion, and race are features. It is not novel to include these additional features to improve virality prediction. There have been many efforts [1].\n\n(6) No baseline metrics except for likes, and retweets were compared to justify the effectiveness and novelty of the proposed metric. VTS was computed based on likes and retweets. It is not surprising that it is better than either of its components. However, the performance difference is not large even comparing VTS with likes or retwets.\n\nReferecnes:\n\n[1] Han Y, Lappas T, Sabnis G. The importance of interactions between content characteristics and creator characteristics for studying virality in social media. Information Systems Research. 2020."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.75,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Hw1tOjCWBZ",
    "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation",
    "authors": [
      "Siyuan Fang",
      "Kaijing Ma",
      "Tianyu Zheng",
      "Xeron Du",
      "Ningxuan Lu",
      "Ge Zhang",
      "Qingkun Tang"
    ],
    "abstract": "Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning on KGs. KARPA operates through a three-step process: pre-planning, retrieving, and reasoning. First, KARPA uses the LLM's global planning ability to pre-plan logically coherent relation paths based on the provided question and relevant relations within the KG. Next, in the retrieving phase, relation paths with high semantic similarity to the pre-planned paths are extracted as candidate paths using a semantic embedding model. Finally, these candidate paths are provided to the LLM for comprehensive reasoning. Unlike existing LLM-based KGQA methods, KARPA fully leverages the global planning and reasoning capabilities of LLMs without requiring stepwise traversal or additional training, and it is compatible with various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy.",
    "keywords": [
      "Knowledge Graph",
      "Large Language Models",
      "Chain-of-Thought",
      "Reasoning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Hw1tOjCWBZ",
    "forum_url": "https://openreview.net/forum?id=Hw1tOjCWBZ",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The framework operates through a three-step process: pre-planning, retrieving, and reasoning. KARPA is designed to address the limitations of existing LLM-based KGQA methods by fully utilizing the global planning and reasoning capabilities of LLMs without requiring stepwise traversal or additional training. The paper claims that KARPA achieves state-of-the-art performance in KGQA tasks, offering both high efficiency and accuracy.",
        "strengths": "- The three-step process of pre-planning, retrieving, and reasoning is a easy-to-fellow approach to leverage external knowledge sources for enhancing LLMs.\n\n- KARPA's training-free nature is an advantage, as it allows for easy integration with various LLM architectures without the need for fine-tuning or pre-training on specific KGs, which can be time-consuming and resource-intensive.",
        "weaknesses": "- To my knowledge, The proposed method lacks novelty; employing an agent-based approach for KGQA tasks is not particularly innovative.  For instance, RoG recently introduced a planning-retrieval-reasoning framework for KGQA, and  this manuscript is the expansion of the planning phase in RoG's framework into two processes: pre-planning and re-planning. \n\n- Secondly, the authors claim in their motivation that \"Pre-training or fine-tuning the LLM for KGQA, which is prone to hallucinations and struggles to adapt to unseen KGs without an extensive training process.\" I believe this is a point worth discussing. Taking RoG as an example again, it did employ instruction-tuning for the LLM, but I think such instruction-tuning for LLMs possesses a certain ability to learn from few samples, meaning that generalization can be reflected in the mitigation of hallucinations. Of course, I also feel that this is worth experimenting with and discussing further.\n\n- Considering the forward-looking nature of RoG in initially proposing the planning-retrieval-reasoning approach, it is somewhat unfair that this manuscript did not use the same large model as a backbone in its comparison with RoG (Table 1).\n\n- There is a noticeable absence of robust agent-like methods, such as Interactive-KBQA, which directly perform inference over KGs with LLMs."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper proposes Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning on KGs. KARPA operates through a three-step process: pre-planning, retrieving, and reasoning.",
        "strengths": "The idea is reasonable. The paper clearly describes their model.",
        "weaknesses": "1. The comparison in Table1 is unfair, the baseline all use 3-7B LLM but the proposed model uses GPT. The proposed model heavily depends on the output of the LLMs. Whether the authors have tried the effect of using 7B LLMs.\n\n2. The claimed \"training-free\" contribution comes from the pre-trained LLMs and relies on the ability of LLMs. Therefore, KARPA lacks substantive technical contributions.\n\n3. For a specific problem, what if the path given by LLM does not match the knowledge graph? In the face of diverse query problems, how can the output of LLM remain effective? In this sense, ToG may be more realistic and efficient."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes KARPA, a training-free KGQA method that utilizes LLMs to reason over KGs. KARPA uses a pre-planning step to extract candidate relation paths in a three-stage manner (initial planning, relation extraction, and re-planning) and ranks top candidates in the later reasoning step. The main difference between KARPA and previous LLM x KGQA methods (e.g. ToG and Pangu) is that KARPA is more efficient in LLM usage as it avoids the stepwise interaction between LLM and KG, and yet achieves superior performance. Regarding the experimental results, KARPA consistently outperforms baseline methods on WebQSP and CWQ datasets.\n\nDespite these strengths, KARPA also exhibits several limitations. First, KARPA relies heavily on LLM's planning and reasoning abilities since it needs LLM to propose candidate relation paths or rank candidates in one single call, which may be harder to succeed when the candidate subgraph is complex and large. Second, in the pre-planning stage, KARPA uses a traditional sentence transformer to embed relations and calculate similarity, which may fail in multilingual scenarios (for example, the initial relations proposed by LLM is English, but the relation surface form in KG is Chinese). Overall, I may doubt the performance of KARPA on more complex KG (larger subgraphs, more relations) and multilingual scenarios.",
        "strengths": "- The proposed method is well-motivated and reasonable under certain scenarios.\n- The presented experimental results are strong.",
        "weaknesses": "- KARPA relies heavily on LLM's planning and reasoning abilities since it needs LLM to propose candidate relation paths or rank candidates in one single call, which may be harder to succeed when the candidate subgraph is complex and large.\n- In the pre-planning stage, KARPA uses a traditional sentence transformer to embed relations and calculate similarity, which may fail in multilingual scenarios (for example, the initial relations proposed by LLM is English, but the relation surface form in KG is Chinese). \n- Some experiment details are not clear enough. See questions."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper addressed the task of KGQA and focused on the LLM-based method. The paper proposed a knowledge graph-assisted reasoning path aggregation approach, which contains three main steps: pre-planning, retrieving and reasoning. The proposed approach employs LLM and heuristic value-based relation path retrieval to extract potential relational paths for the questions. At last, the extracted candidate paths are provided to the LLM for reasoning. The results show the effectiveness of the proposed approach.",
        "strengths": "1. The proposed heuristic value-based relation path retrieval method is interesting to compute the semantic similarity between paths with different lengths. \n2. The experimental comparisons with baselines on KGQA show the effectiveness.",
        "weaknesses": "1) The Figure 1 is not very clear. Totally, there are two limitations for different previous methods: 1) local search strategies and 2) struggle with unseen KGs. I am not sure how the proposed method could resolve the aforementioned limitations. In specific, how about the performance of unseen KGs? Why the simple embedding-based semantic similarity, such as beam search deduce suboptimal answers?\n\n2) The advantage of the proposed heuristic value-based relation path retrieval method is to compute the semantic similarity between paths with different lengths. Why do we need to extend relation paths with different lengths? The authors should exploit some experiments to prove it.\n\n3) I wonder how about the performance when the given KG is not in the scope of LLM training sets. The used WebQSP and CWQ datasets, in my opinion, have been seen when pre-training LLMs."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper introduces a novel framework called KARPA designed to enhance the reasoning capabilities of LLMs in Knowledge Graph Question Answering (KGQA) tasks. The authors identify limitations in existing methods that either rely on step-by-step traversal of KGs, which restricts the global planning abilities of LLMs, or require fine-tuning on specific KGs, making them less adaptable.\n\nKARPA addresses these challenges through a three-step process: \n(1) Pre-planning: The LLM generates initial relation paths based on the question and relevant KG relations, leveraging its inherent global reasoning and planning capabilities.\n(2) Retrieving: A semantic embedding model is used to find candidate paths in the KG that are semantically similar to the LLM-generated paths, avoiding local optima and reducing interactions with the KG.\n(3) Reasoning: The candidate paths and corresponding entities are provided back to the LLM for comprehensive reasoning to produce the final answer.\n\nThe framework operates in a training-free manner, making it adaptable to various LLM architectures without additional fine-tuning or pre-training. Experimental results demonstrate that KARPA achieves state-of-the-art performance on multiple KGQA benchmark datasets, delivering both high efficiency and accuracy.",
        "strengths": "- By eliminating the need for fine-tuning or pre-training on specific KGs, KARPA offers a flexible and adaptable solution compatible with various LLMs.\n- KARPA reduces the number of interactions between the LLM and the KG, enhancing efficiency without compromising accuracy. Moreover, the semantic embedding-based retrieval mitigates the risk of the LLM getting trapped in locally optimal solutions, leading to more effective exploration of KGs.\n- KARPA fully leverages the LLM's global reasoning and planning abilities, enabling it to generate comprehensive relation paths beyond adjacent relations.",
        "weaknesses": "- The experimental evaluation is focused on KGQA tasks; the framework's effectiveness on other knowledge-intensive tasks remains unaddressed.\n- KARPA assumes access to comprehensive and accurate KGs, but the performance may degrade with incomplete or noisy KGs. Further, the effectiveness of KARPA may vary across different domains, especially if the KG lacks sufficient coverage of the required knowledge.\n- The retrieval of candidate paths depends heavily on the quality of the semantic similarity measures used in the embedding model, which may affect the overall performance if the embeddings are not optimal."
      }
    ],
    "rating_avg": 4.6,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "gaa7gWPZBz",
    "title": "Mitigating Privacy Risk of Adversarial Examples with Counterfactual Explanations",
    "authors": [
      "Aohan Sun",
      "Yanrong Lu",
      "ATHANASIOS V. VASILAKOS"
    ],
    "abstract": "Robustness and privacy are two fundamental security properties that \nmachine learning models require. Without the balance between robustness and privacy leads to \nrobust models with high privacy risks. Obtaining machine learning models with high adversarial robustness and \nprivacy performance remains an open problem. In order to enhance the privacy performance of \nrobust models, we employ counterfactual explanations as a method \nto mitigate privacy risks while concurrently maintaining robust model accuracy, reducing the privacy risk of the robust model to the level of \nrandom guessing and using counterfactual explanations to generate adversarial examples for the first time. We analyze the similarities and differences between \nadversarial examples and counterfactual explanations and utilize these properties to design the \ngeneration method. We \nconduct an in-depth analysis of the advantages offered by counterfactual explanations compared \nto traditional adversarial examples. Our study indicates that the correlation between \nrobustness and privacy is strong and the ideal balance state of accuracy, robustness, and privacy is with 95\\% \nadversarial examples involved in model training.",
    "keywords": [
      "Adversarial Examples",
      "Privacy",
      "Counterfactual Explanations"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=gaa7gWPZBz",
    "forum_url": "https://openreview.net/forum?id=gaa7gWPZBz",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "-   The paper tackles an important topic in the field of machine learning, specifically the tradeoff between privacy and performance in the context of adversarial examples.\n-   The authors propose an approach to generating counterfactual explanations that are aligned with the original data distribution.",
        "strengths": "-   The paper tackles an important topic in the field of machine learning, specifically the tradeoff between privacy and performance in the context of adversarial examples.\n-   The authors propose an approach to generating counterfactual explanations that are aligned with the original data distribution.",
        "weaknesses": "-   The paper lacks clarity and readability, with technical terms and acronyms (e.g. CNN) used without definition.\n-   The comparison of different baselines is not well-explained, and the relevance of these baselines to the context is not clear.\n-   The paper only uses the MNIST dataset and does not define the model architecture, which limits the generalizability of the results.\n-   There are major spelling errors throughout the paper (for eg. title of Section 4.3 ), which detracts from its overall quality.\n-   The comparison with state-of-the-art membership inference attacks (MIA) and variants is not discussed, which could help evaluate the potency of this method."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes using counterfactual explanations to reduce privacy risks in robust machine learning models, aiming to balance robustness, accuracy, and privacy by generating adversarial examples through counterfactuals, ultimately achieving a privacy risk level comparable to random guessing.",
        "strengths": "- **Intersection between robustness & privacy is interesting**: The paper addresses a complex intersection between privacy and robustness in machine learning.\n\n- **Training algorthim**: By attempting to incorporate counterfactual explanations into model training, the paper takes a new perspective to adversarial robustness and privacy alignment.",
        "weaknesses": "- **Relevance and Motivation**: The practical relevance of the proposed solution remains unclear. The motivation for focusing on counterfactual explanations over traditional methods for balancing robustness and privacy lacks justification, leaving doubts about its necessity.\n\n- **Metric Selection for Privacy Evaluation**: The paper’s use of membership inference accuracy as a privacy metric is inadequate. A more suitable metric would be the true positive rate (TPR) at a low false positive rate (FPR), as this metric would allow an adversary to determine training set membership with higher confidence. Existing work, such as [7], highlights TPR @ low FPR as a more meaningful measure in privacy settings.\n\n- **Missing Related Works and Literature Misrepresentation**: The paper lacks a clear related works section, making it difficult to contextualize its contributions within existing research. For example, it fails to adequately cite and compare itself to relevant counterfactual explanation methods (e.g., [1-4]), leaving its method selection ungrounded.\nSome statements in the paper seem to misinterpret or misrepresent findings from prior research [8], diminishing the credibility of its claims. Specifically, the lack of distinction between counterfactual explanations and adversarial examples is problematic, as references like [6] demonstrate they can be equivalent, and [5] highlights the privacy risks that counterfactual explanations themselves can pose.\n\n- **Lack of Clarity in Methodology**: Key details are missing regarding the generation of counterfactual explanations. The lack of specifics regarding the generation process and the unclear formalization of key equations (e.g., equations 2 and 4) make the methodology difficult to follow and replicate.\nDefinitions of critical terms, such as the exact differences between adversarial examples and counterfactual explanations, are either ambiguous or absent, leading to confusion about the novelty and benefits of the proposed approach.\n\n\nBased on these weaknesses, the following **suggestions for improvement** could be considered:\n\n- Clearly articulate the relevance of counterfactual explanations for privacy-robustness tradeoffs in machine learning and provide a detailed comparison with existing methods.\n- Consider evaluating privacy using TPR at low FPR, as discussed in [7], and address potential limitations of using membership inference accuracy as the sole metric.\n- Include a comprehensive related works section that connects to the literature on counterfactual explanations, especially work such as [1] and [5], to provide a more thorough foundation for the methodology.\n- Improve clarity in the formalization of the method, specifically in explaining the setup for generating counterfactuals and addressing unclear notation in key equations.\n\n\n----\n\n**References**\n\n[1] Carla: a python library to benchmark algorithmic recourse and counterfactual explanation algorithms, https://arxiv.org/abs/2108.00783\n\n[2] Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems, https://arxiv.org/abs/1907.09615\n\n[3] Learning model-agnostic counterfactual explanations for tabular data, https://arxiv.org/abs/1910.09398\n\n[4] Getting a CLUE: A Method for Explaining Uncertainty Estimates, https://arxiv.org/abs/2006.06848\n\n[5] On the Privacy Risks of Algorithmic Recourse, https://arxiv.org/abs/2211.05427\n\n[6] Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis, https://proceedings.mlr.press/v151/pawelczyk22a.html \n\n[7] Gaussian Membership Inference Privacy, https://proceedings.neurips.cc/paper_files/paper/2023/hash/e9df36b21ff4ee211a8b71ee8b7e9f57-Abstract-Conference.html\n\n[8] Robustness Implies Privacy in Statistical Estimation. In Proceedings of the 55th Annual ACM Symposiumon Theory of Computing, pp. 497–506, 2023."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "It is hard to implement a deep learning model that protects privacy and is robust at the same time. Specifically, adversarial examples can be used to achieve the purpose of robustness, but it is usually difficult to protect privacy on this basis. This paper analyzes the relationship between counterfactual explanations and adversarial examples, and designs the counterfactual adversarial example to mitigate the privacy leakage of robust models.",
        "strengths": "1. This article proposes the idea of reducing privacy risk, which is inspiring.",
        "weaknesses": "1. This article uses a lot of space in section 2 to describe the similarities and differences between counterfactual explanations and Adversarial examples, which have been elaborated in many articles [1,2]. Further analysis is needed on the relationship with privacy.\n2. The formulas written in this article need to be further improved. Some mathematical symbols that appear for the first time (such as Eq. (7)) are not explained, and some formulas (such as Eq. (1) and (2)) do not correspond to the context, but appear abruptly in those positions.\n3. The description of the method part of this article needs to be improved. Sections 3.1, 3.2, and 3.3 look more like three independent parts.\n4. This article needs more experiments to reflect the effectiveness of the method. This article only made a CNN model on MNIST, and did not verify whether it is effective on larger datasets and more complex models.\n\n\n[1]Pawelczyk, Martin, et al. \"Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\n[2]Jeanneret, Guillaume, Loïc Simon, and Frédéric Jurie. \"Adversarial counterfactual visual explanations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.6666666666666665,
    "decision": "Reject",
    "meta_review": "The paper proposes to study privacy robustness tradeoffs by analyzing counterfactual examples and adversarial examples. By generating adversarial examples using counterfactual frameworks and imposing robustness, authors achieve better robustness-privacy tradeoffs. Reviewers have noted weak empirical evaluation due to insufficient datasets and the choice of metrics, lack of clarity in the writing and explaining methodology, and insufficent literature review. An author response was not provided. In going with reviewer consensus, I recommend a reject.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "imT03YXlG2",
    "title": "Sparse autoencoders reveal selective remapping of visual concepts during adaptation",
    "authors": [
      "Hyesu Lim",
      "Jinho Choi",
      "Jaegul Choo",
      "Steffen Schneider"
    ],
    "abstract": "Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.",
    "keywords": [
      "interpretability",
      "vision-language models",
      "sparse autoencoder",
      "adaptation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=imT03YXlG2",
    "forum_url": "https://openreview.net/forum?id=imT03YXlG2",
    "reviews": [
      {
        "rating": "8",
        "confidence": "4",
        "summary": "The authors perform sparse autoencoder (SAE) training on the features of a pre-trained vision transfer (ViT) from the CLIP image encoder.. The outputs of the 11th layer of the CLIP ViT is used to train a two layer SAE with reconstruction + sparsity objective. The SAE encoder features are then used to validate the presence of concepts in CLIP representations, and their role in classification as well as downstream tasks. The authors first utilize feature and example based statistics to sanity check that concepts are indeed present in SAEs trained on the CLIP visual representations, as well as multi-modal CLIP representations. Next, they verify that these conceptual features are relevant for class discriminative performance of CLIP zero-shot classification. Lastly, the authors show that SAE conceptual features after prompt-based adaptation show a suppression in non class-relevant features, while and increase in the activation of class-relevant features. New conceptual features are not learned during this adaptation procedure.",
        "strengths": "* The authors do a good job at utilizing both feature based and example based summary statistics to demonstrate presence of concepts in the SAE representations, and motivate all four statistics well in Section 2.2. Unlike prior work on inferring CLIP concepts from SAEs, which tend to rely on singular feature space statistics, this makes the author's results a bit more comprehensive.\n* The results from replacing CLIP features with SAE features and its impact on accuracy shows that the SAE conceptual features capture class discriminative information quite well. I think this result opens up more avenues on mechanistic interpretability research, as further exploration can be done into what makes a concept important vis-a-vis a class, what are the training dynamics of the concept learning for class discriminative performance (are there 'easier' concepts that are learned first e.g. winter before harder concepts e.g. christmas), and several other open research questions.",
        "weaknesses": "* The organization of the paper makes it hard for someone who is not very familiar with the field of sparse autoencoder based mechanistic interpretability of neural networks to follow the flow of ideas. For example Section 2 details the training procedure and the feature statistics calculated, but the experimental details are ommitted and then presented in Section 4. Similarly, the Intro and Section 2.1 cover a brief overview of SAEs and their use in intepretability research, but the actual related work section is not presented until Section 5. While I understand that the authors are eager to share their key results in an earlier section (especially since this is a intepretability focussed work), this organization made it more work for me as a reader to understand the paper.\n* The scope of experiments performed and the subsequential observations is somewhat limited. The first experiment, showing presence of concepts in CLIP visual features, was already shown in principle by Fry et al and Daujotas et al. The result about CLIP attention maps focussing on features at different scales and regions is not surprising, but another sanity check for the SAE interpretability apporach. The second emperimental design is inspired from previous SAE results in LLMs e.g. Templeton et al, and extrapolated from a language (only) transformer setting to a multimodal setting."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduced Sparse AutoEncoder (SAE), which previously trained to disentagle and interprete LLM hidden states, to analysis CLIP features. The authors trained a SAE on top of CLIP features with ImageNet samples, and carried out several experiments with it demonstrating 3 points: \n- Section 3.1: certain demension in SAE feature correlates to certain visual concepts\n- Section 3.2: targeted ablation of neurons yeilds degraded performance\n- Section 3.3: CLIP model \"re-use already known concepts\" when it is fine-tuned for downstream datasets",
        "strengths": "- The paper transfers the methodology of SAE from NLP to vision. The idea of identifying visual concepts from CLIP features is quite interesting.\n- From visualizations, it seems that the SAE on top of CLIP sucessfully identified some meaningful semantic concertps in both image-level and patch-level.",
        "weaknesses": "1. **Lacks quantitative evaluation of SAE's reliability**. Although SAEs for LLMs are widely studied in NLP, to my best knowledge, the usage of SAE for CLIP is only reported in several blog posts that have not been peer-reviewed. Since its effectiveness and reliability are not sufficiently justified in a convincing way, directly using it to draw conclusions is quite risky.\n\n    - For example, how can we ensure that the training settings listed in Section 4.1 are properly set? Are 49,152 hidden dimensions enough for the vision domain? Does this SAE achieve monosemanticity? \n\n    - Examples in the case study are certainly not sufficient for a rigorous evaluation. I would suggest conducting a large-scale evaluation using vision datasets with fine-grained attribute annotations to answer the above questions.\n\n\n1. **Potential distribution shift between base and fine-tuned CLIP**. As stated in Line 325, the authors used the SAE trained on top of pretrained CLIP to interpret the fine-tuned CLIP. As MaPLe fine-tunes both image and text encoders, there are no guarantees that the feature distribution will remain the same after fine-tuning, and all the conclusions in Section 3.3 might be inaccurate as a result.\n\n\n1. **Regarding \"re-mapping\" or \"re-using\" visual concepts**. As highlighted in the paper title and section titles, the authors wanted to demonstrate the connection of visual concepts between base and fine-tuned CLIP. However, I am a bit confused regarding the terminology used, as neither \"re-mapping\" nor \"re-using\" is properly defined in the paper. The authors are encouraged to provide a clear definition of these terms and highlight how the results (e.g., Fig. 6c) demonstrate this point.\n\n\n1. **Significance of Contribution**. Overall, I find the new insights provided in this paper are not as substantial as I expected when I read only the title. Training SAE on CLIP features is not novel. It has been previously reported in several blog posts, yet this paper does not provide a more formal and quantitative evaluation. The results in Section 3.1 and 3.2 are somewhat expected and are not particularly surprising. \n\n    I would suggest that the authors conduct large-scale quantitative validation to prove the effectiveness of CLIP's SAE, e.g., ablation of hyperparameters, training settings, and types of CLIP models (convolutional encoders are not covered in the presented study). Additionally, clearly stating the relation and differences between SAE and existing interpretability methods in the vision domain would help readers understand the contributions."
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "In this paper, the authors propose a new way to try to interpret features learned from a vision-language (in particular, a CLIP model). The proposed approach train SAE on all image tokens. The author then show that the trained SAE can be used to discover different concepts from the CLIP vision transformer. The authors then investigate how the learned \"interpretable features\" relates the output of the model.",
        "strengths": "- Better understanding foundantion models (and what their features encode) is an extremely important problem---specially as these models get more powerful.\n- The proposed approach is simple  and leverages the well-understood sparse autoencoder approach to interpret the features.",
        "weaknesses": "- The paper has some very strong claims that has not been properly demonstrated. Eg, L249 says \"CLIP understands sophisticated concepts from input images not only responding to basic patterns such as color or shape \". Showing a a few (potentially cherrypicked) qualitative examples is not enough to show/demonstrate/prove the results on the paper. It would make results stronger if  the authors could provide more quantitative metrics to measure the sophistication of detected concepts. This would make the results on the paper more convincing/interesting.\n- Some sessions of the paper are not very clear. For example, the authors leverage MaPLe model (eg \"adaptation methods analysis\") and nowhere in the paper they explain how this model works. It would make the paper more readable if the authors could provide a brief overview of this model on the manuscript.\n- A lot of design choices seem to be made ad-hoc. For example, why chose the second-to-last layer? Why use the SAE feature statistics used? Why 49,152 dimensions? Could the authors provide some justification for these choices? How were those choices made? Was any ablation study performed to choose those values?"
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper utilizes sparse autoencoders (SAE) to interpret visual concepts learned by the CLIP vision model and investigates how these concepts are affected by adaptation techniques such as MaPLe. The key finding is that during adaptation, the model primarily reuses existing concepts rather than learning entirely new ones.",
        "strengths": "- The paper offers a detailed and fine-grained exploration of CLIP by processing the entire token sequence with SAEs, allowing for a deeper understanding of the model's visual feature representations.\n- It tackles the adaptation dynamics of foundation models like CLIP, which is a relatively under-explored topic.",
        "weaknesses": "- The clarity and organization of the analyses are insufficient. The main body of the paper frequently refers to figures in the appendix, many of which are poorly annotated, making it difficult for readers to follow the core arguments. The authors should revise the manuscript to consolidate the most important findings in the main body and present them in a clearer and more structured format.\n- The paper lacks sufficient discussion on the broader implications and significance of its findings. Are any results surprising or do they challenge conventional views within the field? The authors should also elaborate on the practical value of their discoveries."
      }
    ],
    "rating_avg": 6.5,
    "confidence_avg": 3.5,
    "decision": "Accept (Poster)",
    "meta_review": "Post rebuttal, all of reviewers vote for acceptance. The AC checked all the materials and concurs that the paper has done a valuable exploration of using patch-wise sparse auto-encoders to interpret the CLIP visual representations, especially through the adaptation process. The paper received concerns about clarity and organization, but has been significantly improved during the rebuttal period, with authors paying great efforts providing new results and re-writing the draft. With these major revisions, the paper can be accepted. Please incorporate necessary changes in the final version.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "gtVo4xcpFI",
    "title": "GenBen:A Genarative Benchmark for LLM-Aided Design",
    "authors": [
      "Gwok-Waa Wan",
      "Wang yubo",
      "SamZaak Wong",
      "jingyi zhang",
      "Mengnv Xing",
      "Zhe jiang",
      "Nan Guan",
      "ying wang",
      "Ning Xu",
      "Qiang Xu",
      "Xi Wang"
    ],
    "abstract": "This paper introduces GenBen, a generative benchmark designed to evaluate the capabilities of large language models (LLMs) in hardware design. With the rapid advancement of LLM-aided design (LAD), it has become crucial to assess the effectiveness of these models in automating hardware design processes.\nExisting benchmarks primarily focus on hardware code generation and often neglect critical aspects such as Quality-of-Result (QoR) metrics, design diversity, modality, and test set contamination. GenBen is the first open-source, generative benchmark tailored for LAD that encompasses a range of tasks, from high-level architecture to low-level circuit optimization, and includes diverse, silicon-proven hardware designs. \nWe have also designed a difficulty tiering mechanism to provide fine-grained insights into enhancements of LLM-aided designs. Through extensive evaluations of several state-of-the-art LLMs using GenBen, we reveal their strengths and weaknesses in hardware design automation. Our findings are based on 10,920 experiments and 2,160 hours of evaluation, underscoring the potential of this work to significantly advance the LAD research community. \nIn addition, both GenBen employs an end-to-end testing infrastructure to ensure consistent and reproducible results across different LLMs. The benchmark is available at https://anonymous.4open.science/r/GENBEN-2812.",
    "keywords": [
      "GenBen; Benchmark; LLM-Aided Design; LLM; Hardware Design"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=gtVo4xcpFI",
    "forum_url": "https://openreview.net/forum?id=gtVo4xcpFI",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "GenBen introduces a benchmark designed to evaluate the capacity of LLMs for generating hardware designs. Existing state-of-the-art benchmarks have several limitations: primarily, they focus solely on syntax and functional pass rates and often include simplistic problems sourced from textbooks, which may be part of LLM training data.",
        "strengths": "In this paper, we present GenBen, a comprehensive benchmark developed to assess the capabilities of LLMs in hardware design. GenBen addresses existing limitations through the following strategies:\n\n1. Incorporating problems sourced from various origins, including silicon-proven projects, to establish three levels of difficulty for evaluating LLMs.\n2. Utilizing perturbation strategies to create dynamic tests, ensuring that LLMs encounter unseen challenges.\n3. Expanding evaluation criteria beyond syntax and functional pass rates to include metrics such as synthesizability, power, performance, and area.\n4. Conducting assessments across multiple LLMs to provide comparative insights.",
        "weaknesses": "The writing quality of this paper is subpar, primarily due to the lack of practical examples. The authors should include more concrete examples to clarify concepts such as \"adding some perturbations.\" Additionally, Table 4 lacks substantive information, and it would be more effective if it included illustrative examples. Furthermore, Section 3.3.1 provides only a superficial overview of the end-to-end verification flow, lacking sufficient detail and making the article less informative. \n\nMoreover, it is unclear how the perturbation process ensures the creation of sound problems, avoiding ambiguity or unclear meanings. The authors should address this aspect to reinforce the reliability of their approach.\n\nSome types on writing:\nSection 3.2 \"GenBen then generates test tests from the test dataset D using scripts\", test typo.\nFigure 2 and Figure 4-12 are very hard to read. Use contrasting colors then patterns to discern between different\nmetrics.\nline 213: test tests from"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces a generative benchmark designed to evaluate large language models (LLMs) in the context of hardware design automation. Recognizing the limitations of existing benchmarks focused primarily on basic code generation, GenBen extends evaluation to include critical aspects like Quality-of-Result (QoR) metrics, debugging capabilities, design diversity, and prevention of test set contamination. The benchmark encompasses tasks across multiple design levels, from high-level architecture to low-level circuit optimization, and includes a tiered difficulty system to provide insights into LLM performance across different complexities. With an open-source, end-to-end testing infrastructure, GenBen aims to deliver reproducible and comprehensive assessments, intending to advance the development of LLM-aided design tools within the hardware design community.",
        "strengths": "The paper tries to address a relevant problem and is well written, and well organized",
        "weaknesses": "1)\tThe dynamic perturbation strategy aims to prevent memorization but may inadvertently introduce ambiguity into test cases. This can create inconsistencies in evaluating LLM performance, especially if slight changes in prompt phrasing lead to variations in model responses. The reliance on superficial perturbations (surface-level changes) may not effectively challenge models in understanding complex circuit design concepts.\n2)\tThe paper identifies issues with synthesizability due to non-IEEE-compliant code in pre-training datasets, but it lacks concrete methods for systematically identifying and filtering these cases. This limitation could lead to significant variability in QoR metrics, particularly for synthesis tools that adhere strictly to IEEE standards, reducing the benchmark’s reliability.\n3)\tTiming issues are briefly discussed, but the GenBen benchmark does not appear to account for complex timing closure tasks, such as handling Total Negative Slack (TNS) and Worst Negative Slack (WNS) for high-frequency designs. The benchmark could fall short in evaluating LLMs’ ability to generate designs that meet stringent timing requirements, which is critical for high-performance applications.\n4)\tAlthough the benchmark includes debugging tests, it may not fully capture the complexity of real-world hardware debugging, particularly for stateful designs or asynchronous circuits. The current debugging scope seems limited to relatively straightforward syntax and functional errors, without addressing state-based errors, race conditions, or setup/hold timing violations that are common in complex designs.\n5)\tWhile the inclusion of multimodal tests is innovative, the integration of textual and visual data (e.g., diagrams) lacks specific detail on how visual data is processed or scored. This lack of clarity may lead to ambiguous scoring for models that perform differently across multimodal and text-based tasks, making it challenging to standardize assessments.\n6)\tThe QoR metrics focus on synthesizability, power, area, and timing but do not assess finer aspects like pipeline balancing, parallelism optimizations, or state-machine efficiency. These factors are crucial for high-performance designs and should be part of a rigorous hardware benchmark that targets comprehensive design quality."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "The paper introduces GenBen, a benchmark for LLM-aided design (LAD) specifically targeted at evaluating the performance of large language models (LLMs) in hardware design. This benchmark covers a diverse set of tasks with both unimodal and multimodal support, drawing from 20 sources across four primary domains, further organized into five detailed categories. The tasks are structured across three difficulty tiers, aiming to comprehensively assess models' skills in LAD. The paper also addresses issues of data contamination, dataset diversity, and incorporates a pass@5 evaluation setup across nine models, assessed with comprehensive metrics for syntactic, functional, and Quality-of-Results (QoR) evaluation. The benchmark and its workflow are structured to support reproducibility, utilizing open-source tools like Icarus Verilog and OpenLane.",
        "strengths": "1.\tComprehensive Metric Coverage: GenBen includes both syntactic and functional correctness metrics, as well as QoR metrics (synthesizability, power, area, timing), giving a well-rounded evaluation of LLMs in LAD. The use of open-source tools like Icarus Verilog and OpenLane (as detailed in Section 3.6 and Appendix A.3.1) enhances reproducibility.\n2.\tPerturbation Strategy: GenBen employs both surface and semantic perturbations to prevent test set contamination, a thoughtful design that diversifies test inputs dynamically, as discussed in Section 3.4 and illustrated in Figure 2. This strategy enhances benchmark reliability by mitigating memorization effects.\n3.\tMultimodal Support: Recognizing the multimodal nature of hardware design, GenBen supports tasks that include textual, schematic, and architectural data (Section 3.5). This addition is crucial for practical LAD assessments where real-world design tasks often require multimodal inputs.\n4.\tClear Evaluation Pipeline: The step-by-step pipeline in Figure 2 provides clear guidance for users, with explicit instructions on test construction, perturbation, and evaluation metrics (Section 3.2). The modular approach allows GenBen to assess models on multiple metrics, making it adaptable to diverse LAD scenarios.\n5.\tThorough Documentation: The appendices provide exhaustive details on LAD-related concepts, evaluation metrics, and the role of open-source tools (Appendix A.1 to A.3), which can aid users in implementing the benchmark effectively.",
        "weaknesses": "The paper is not much relevant to the ICLR community, and does not provide a strong mathematical foundation. A more suitable venue for this paper could be ICLAD Conference or some Design Automation Conference like DAC or ICCAD.\n\n1.\tData Imbalance in Test Categories: Table 3 presents disparities across task categories (e.g., 99 Design tasks vs. 57 Debug tasks). While task diversity is commendable, the authors should clarify why such an imbalance exists and discuss potential implications for model performance assessment, especially if specific categories are overrepresented. An explanation of how this might affect the reliability of the benchmark would strengthen the dataset's construction rationale (Section 3.3.3).\n2.\tModel Selection Motivation: Section 4.1 introduces nine models across five families but lacks justification for these particular choices. Clarifying why specific models (e.g., GPT, Claude, LLaMA, QWEN, and GLM families) were included could strengthen the argument, especially if these models are particularly relevant for LAD tasks. For instance, what LAD-specific capabilities do these models bring? Including performance characteristics in multimodal tasks or LAD-related features in each model would better contextualize the selection.\n3.\tDifficulty Tiering: The paper divides tasks into three difficulty levels (L1 to L3) with proper descriptions (Table 2). However, detailing how each tier’s tasks correspond to LAD challenges (e.g., debugging complexity or resource optimization) would add valuable context. The authors could enhance Appendix A.4 by providing specific examples that illustrate the distinctions between L1, L2, and L3, particularly within each task type (knowledge, design, debugging). Quantitative data on average model performance per tier might also reveal trends that could guide future benchmarking improvements.\n4.\tDetailed Case Study on Perturbation: The perturbation strategy is briefly explained in Section 3.4, with an overview of surface and semantic techniques. However, a specific example, especially for semantic perturbation, would make this section clearer. Adding a detailed case study would provide a more straightforward demonstration of how the data were perturbed, also for semantic perturbation, due to the randomness, is there a manageable gap of difficulty to change? For instance, showing a task before and after both types of perturbation would help readers understand the differences in difficulty introduced by each type. Furthermore, since semantic perturbation could change task difficulty, addressing how fairness is maintained would strengthen the case for using this approach.\n5.\tElaboration on Workflow: The \"Dynamic Test Kit\" component in Figure 2 and Section 3.2 would benefit from additional detail. For instance, does the Dynamic Test Kit contain tools that adjust task difficulty, or is it limited to generating variants of test inputs? Furthermore, clarifying the connection between the scoring system described in Section 4.1 and the metrics reported in Table 5 could make the scoring more interpretable. Specifically, explaining if the pass rate is calculated per task or per difficulty level would aid in understanding the benchmark's scoring logic.\n6.\tExperiment Results Interpretation: Section 4.2 and Figures 4-12 illustrate experiment results, but a breakdown of model performance by metric (e.g., Syntax, Synthesizability, Function) would be helpful. Discussing why Syntax scores are generally higher while Function scores lag behind could provide insight into specific model weaknesses. Additionally, including a statistical analysis (e.g., variance in pass rates across difficulty levels) would offer a clearer picture of each model’s strengths and limitations.\n7.\tSupporting Literature for Multimodal Justification: Section 3.5 could be enhanced by citing specific LAD scenarios or prior research that demonstrate the need for multimodal tasks (e.g., combining schematics with HDL code) to support the claim “This feature is particularly important because real-world design processes often require the integration of various forms of data, such as textual specifications, diagrams, and architectural schematics”. Including practical examples from hardware design projects that require multimodal support would illustrate the real-world relevance of GenBen’s multimodal component. This would also substantiate the statement that real-world LAD tasks are inherently multimodal.\n9.\tFuture Work and Limitations: The paper would benefit from a Limitations and Future Work section, discussing GenBen's adaptability to emerging LAD requirements (e.g., integration with more complex EDA tools or AI-driven hardware design) could provide a roadmap for GenBen’s evolution. Additionally, mentioning scalability limitations (e.g., handling larger designs or more complex verification tasks) would provide a balanced view of the benchmark’s potential applications and restrictions.\n10.\tEvaluation Metric Granularity: While Table 5 provides an overview of evaluation metrics, further clarification on the weighting or relative importance of these metrics (e.g., Syntax vs. Synthesizability vs. QoR metrics) could improve transparency. Explaining if QoR metrics are prioritized over syntactic correctness for certain tasks, such as those targeting manufacturable designs, would give a better understanding of how the benchmark defines \"success.\" Additionally, addressing any trade-offs between these metrics, especially in cases where models might excel in one metric while underperforming in others, would be helpful (e.g., does a high Syntax score compensate for lower Synthesizability?).\n11.\tBenchmark Customizability: GenBen is described as an end-to-end LAD benchmark. A brief mention of how the benchmark might be tailored for different LAD applications (e.g., specialized RTL verification or energy optimization) in Section 3.2 would clarify the benchmark’s flexibility.\n12.\tClearer Description of Multimodal Task Processing: Although Section 3.5 explains the importance of multimodal tasks, more detail on how GenBen processes or evaluates multimodal inputs would be beneficial. For instance, specifying how different data forms (e.g., schematics vs. HDL code) are presented to models and if specific evaluation criteria are adapted to multimodal tasks could provide clarity. Addressing whether the models are expected to interpret these inputs sequentially or concurrently, and if multimodal inputs impact the difficulty tiering, would improve understanding of the benchmark's multimodal handling.\n13.\tClarification of Pass@5 Evaluation and Scoring: The pass@5 scoring strategy in Section 4.1 is introduced without much context on its relevance to LAD tasks. Explaining why pass@5 is chosen over other scoring metrics (e.g., pass@10, exact match) and how it aligns with real-world LAD evaluation (e.g., tolerance for minor errors in preliminary passes) would strengthen its justification. Additionally, describing if scoring varies by task difficulty or complexity would make the scoring methodology clearer."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper introduces GenBen, a generative benchmark designed to evaluate the performance of large language models in hardware design automation. As LLM-aided design progresses rapidly, it has become essential to assess these models' efficacy in automating various stages of hardware design, from high-level architecture to low-level circuit optimization. GenBen distinguishes itself from existing benchmarks by emphasizing aspects such as Quality of Result metrics, design diversity, multimodality, and data contamination prevention.",
        "strengths": "1.  GenBen implements an end-to-end verification process, ensuring functional coverage for RTL designs, which enhances the reliability of generated designs.\n\n2. With a dataset derived from silicon-proven projects, textbooks, and community sources, GenBen categorizes tasks into three difficulty levels, allowing for more granular assessments.\n\n3. By employing static and dynamic perturbations, GenBen mitigates the risk of data leakage from pre-training datasets, maintaining the integrity of evaluation results.",
        "weaknesses": "1. I tried to access the anonymous link provided by the authors for the benchmark, but it seems to be unavailable. Could the authors provide a working link during the rebuttal phase? This would allow me to evaluate the quality of the dataset\n\n2. Another minor concern: Although I really appreciate the multimodal and contamination-free dataset, I personally feel that this paper might be better suited for a hardware-focused conference."
      }
    ],
    "rating_avg": 4.25,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "**Summary:** \nThe paper introduces GenBen, a benchmark designed to evaluate LLMs for hardware design tasks, addressing limitations in existing benchmarks. It includes multi-level tasks, incorporates Quality-of-Result (QoR) metrics, and employs perturbation strategies to prevent test set contamination. GenBen spans diverse tasks, supports multimodal inputs, offers a tiered difficulty system, and demonstrates its utility through evaluations of nine LLMs.\n\n**Strength:** \n\n1. The paper introduces a diverse benchmark that evaluates both functional and QoR metrics, filling in a gap in hardware design benchmarks.  \n\n2. The use of perturbation strategies to prevent data contamination is novel and can potentially enhance the benchmark's reliability.  \n\n3. The taxonomy of the LAD benchmark provided by this work is potentially insightful and valuable for the research community.  \n\n**Weakness:**\n\n1. The perturbation strategy lacks detailed descriptions to avoid ambiguity and lacks thorough validation, raising concerns about its consistency and usability.  \n\n2. Some critical aspects of the benchmark design are insufficiently detailed, such as how complex timing metrics are accounted for and how debugging tests capture the complexity of real-world hardware debugging.  \n\n3. The paper suffers from some writing issues, including typos and insufficient clarity in figures and methodology, which negatively affect its presentation.  \n\n\n**Reasons for the decision:**\n\nThis work aims to provide a comprehensive benchmark encompassing many diverse aspects; however, the methodology and evaluation for each aspect are not sufficiently detailed or validated. Therefore, I am inclined to recommend rejection.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "orr5uPZY28",
    "title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure",
    "authors": [
      "Yunfan Xiong",
      "Ruoyu Zhang",
      "Yanzeng Li",
      "Tianhao Wu",
      "Lei Zou"
    ],
    "abstract": "While speculative decoding has recently appeared as a promising direction for accelerating the inference of large language models (LLMs), the speedup and scalability are strongly bounded by the token acceptance rate.\nPrevalent methods usually organize predicted tokens as independent chains or fixed token trees, which fails to generalize to diverse query distributions. \nIn this paper, we propose \\textsc{DySpec}, a faster speculative decoding algorithm with a novel dynamic token tree structure. \nWe begin by bridging the draft distribution and acceptance rate from \nintuitive and empirical clues, and successfully show that the two variables are strongly correlated. Based on this, we employ a greedy strategy to dynamically expand the token tree at run time. Theoretically, we show that our method can achieve optimal results under mild assumptions. Empirically, \\textsc{DySpec} yields a higher acceptance rate and speedup than fixed trees. \\textsc{DySpec} can drastically improve the throughput and reduce the latency of token generation across various data distribution and model sizes, which significantly outperforms strong competitors, including Specinfer and Sequoia. Under low temperature setting, \\textsc{DySpec} can improve the throughput up to 9.10x and reduce the latency up to 9.4x on Llama2-70B. Under high temperature setting, \\textsc{DySpec} can also improve the throughput up to 6.21x, despite the increasing difficulty of speculating more than one token per step for draft model.",
    "keywords": [
      "inference methods",
      "efficient inference",
      "speculative decoding"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=orr5uPZY28",
    "forum_url": "https://openreview.net/forum?id=orr5uPZY28",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This work proposes a method to dynamically expand the token tree based on the draft distribution. A key difference between this work and existing approaches is that it introduces a smooth dynamic draft token tree construction method, which expands the tree along both width and depth without hyperparameter setup, within the given token budget. However, related works are missing, and the experimental setup should be properly improved.",
        "strengths": "1.\tThe correlation of Hypothesis 1 with the proposed method is well elaborated and explained.\n2.\tThe structure of the paper is clear and easy to follow.",
        "weaknesses": "1.\tLack of related work. Context-aware dynamic draft token tree is not a new idea. I would like to draw your attention to a very related work: “EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees” (EMNLP'24). This paper also proposes adopting dynamic draft token trees and claims a strong positive correlation between the draft model confidence score and the acceptance rate of the token. Another relevant paper is “Dynamic Depth Decoding: Faster Speculative Decoding for LLMs,” which further improves performance by dynamic depth. The methodology of EAGLE-2 is quite similar to this paper but differs in the tree construction method. Including this work in your paper and providing necessary discussion is essential.\n2.\tExperimental setup and results are weak. First, the dataset is too limited. I recommend adding more datasets like MT-Bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), and GSM8K (Cobbe et al., 2021). Second, the 9.1× speedup is compared with the autoregressive method, and when compared with static tree methods like Sequoia, the results are much less overwhelming. Considering that the experimental setup lacks comparison with state-of-the-art dynamic draft token tree methods, the experiment does not fully convince me of the merits of this methodology.\n3.\tWriting needs improvement. Section 4.2 is hard to follow. The presentation of Figure 3 should be improved. Additionally, Figure 4 is not clear; please increase the font size."
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper introduces DySpec, a novel approach to speculative decoding that employs a dynamic token tree structure to improve inference speed for large language models. The authors present both theoretical and empirical evidence showing that higher draft probabilities correlate with higher acceptance rates. Based on this insight, they develop a greedy strategy for dynamically expanding the token tree at runtime. The method achieves impressive results, demonstrating up to 9.1× throughput improvement and 9.4× reduction in latency on Llama2-70B under low temperature settings, outperforming existing methods like Specinfer and Sequoia.",
        "strengths": "1. Strong Theoretical Foundation\n- Provides rigorous theoretical analysis linking draft distribution to acceptance rate\n- Includes formal proofs of optimality under stated assumptions\n- Clearly bridges theoretical insights with practical implementation\n\n2. Novel Technical Contributions\n- Introduces an innovative dynamic token tree construction approach\n- Develops efficient algorithms for both fixed-size and threshold-based tree construction\n- Proposes block-sparsity friendly token ordering for optimization\n\n3. Comprehensive Empirical Evaluation\n- Tests across multiple model scales (7B to 70B parameters)\n- Evaluates on diverse datasets (C4, OpenWebText, CNN DailyMail)\n- Compares against strong baselines (Specinfer, Sequoia)\n- Examines performance under different temperature settings\n\n4. Implementation Efficiency\n- Addresses practical concerns about overhead\n- Provides C++ implementation to minimize token tree construction costs\n- Includes detailed analysis of computation complexity",
        "weaknesses": "1. Limited Discussion of Limitations\n- Could elaborate more on scenarios where the method might not perform optimally\n- More discussion of the trade-offs between fixed-size and threshold-based approaches would be valuable\n\n2. Implementation Details\n- Some implementation specifics about the C++ optimizations could be expanded\n- Could provide more guidance on threshold selection for different scenarios\n\n3. Experimental Validation\n- Could include more ablation studies to isolate the impact of different components\n- Additional experiments on more diverse model architectures would strengthen generalizability claims"
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "Summary:\n\nThe authors propose dyspec to generate an optimal tree. The core idea is based on the idea that the target distribution and draft distribution should match.\n\nThe authors based on the idea propose an algorithm to come up with optimal tree. They overcome several implementation challenges and achieve reasonable speedups compared to existing baselines.\n\n“DYSPEC achieves a 9.1× throughput improvement and 9.4× reduction in latency.” ([“DySpec”, p. 2]-> Should compare to SpecInfr or Medusa\n\n“Figure 2: Connection between acceptance rate/target distribution and draft distribution on CNN DailyMail.The density of each block is normalized by column.” ([“DySpec”, p. 3] -> Can you please explain this figure. I think it’s the main motivation for your method, however it is not clearly explained, what this figure is showing.\n\n“Specinfer-Baseline” ([“DySpec”, p. 7] -> Specifinfer requires training the models. What have authors done here.\n\n“DYSPEC leverages CUDA Graph to capture 129 different input lengths ranging from 128 to 258” ([“DySpec”, p. 7-> What is the significance of CUDA Graph here, my understanding is CUDA graph is a mechanism to launch multiple kernels at the same time on the GPU to minimize the overhead of kernel dispatch\n\n“We selected Llama2-7B as the draft model” ([“DySpec”, p. 7] -> What happens when you use the 68M model ?\n\n“Set the maximum draft token tree size to 64, DYSPEC achieves up to a 9.1x improvement in throughput and a 9.4x reduction in latency compared to auto-regressive generation” ([“DySpec”, p. 8]-> Unfair comparison, compare to baselines\n\nAdditional experiments -\n\nI would really like to experimentally understand the optimality of your tree. Will it be possible to construct an optimal tree based on some post-hoc data and compare it with the tree generated by Dyspec.",
        "strengths": "- The proposed idea is quite useful and can provide significant speedups for tree based speculative decoding methods. \n\n- The overheads reported our negligible.",
        "weaknesses": "- See the summary section please."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes a draft token selection method in speculative decoding aimed at improving the token acceptance rate. The core idea is to use the draft model's prediction score as evidence to infer the token acceptance rate and use this information to select more promising tokens. The authors provide both theoretical and empirical analyses to support their approach.",
        "strengths": "The proposed token selection method demonstrates improvement over existing methods such as Sequoia and Specinfer.",
        "weaknesses": "1. The paper lacks discussion on existing works that share very similar ideas (see Questions 1 and 2). \n1. The experimental results do not sufficiently validate the method's effectiveness from various perspectives (see Questions 3 and 4)."
      }
    ],
    "rating_avg": 4.75,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "This paper provides a dynamic tree structure construction for multi-draft speculative decoding, and shows improvements over existing static tree construction techniques. However, during the rebuttal, the reviewers mentioned the existence of related work that also creates trees dynamically, specifically Eagle 2, which has significant overlap with the current proposal. The authors have responded with an explanation on the differences. However, the AC finds this to be insufficient and a more in-depth understanding of similarities and differences is warranted. In particular, the AC recommends that the authors replace the tree construction step in Eagle 2 with that proposed herein. This ablation is key to substantiating the claim of almost optimality of the tree construction in this work and can increase its impact further. We hope that the authors find the comments of the reviewers useful for a future iteration of their paper.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "hz3NtNpDNv",
    "title": "Hottel Zone Physics-Constrained Networks for Furnaces",
    "authors": [
      "Ujjal Kr Dutta",
      "Aldo Lipani",
      "Chuan Wang",
      "Yukun Hu"
    ],
    "abstract": "This paper investigates a novel approach to improve the temperature profile prediction of furnaces in foundation industries, crucial for sustainable manufacturing. While existing methods like the Hottel Zone model are accurate, they lack real-time inference capabilities. Deep learning methods excel in speed and prediction but require careful generalization for real-world applications. We propose a regularization technique that leverages the Hottel Zone method to make deep neural networks physics-aware, improving prediction accuracy for furnace temperature profiles. Our approach demonstrates effectiveness on various neural network architectures, including Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM) and Kolmogorov-Arnold Networks (KANs). We also discussion the data generation involved.",
    "keywords": [
      "Hottel Zone method",
      "Physics-Informed Neural Networks",
      "Radiation Heat Transfer",
      "Furnaces"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=hz3NtNpDNv",
    "forum_url": "https://openreview.net/forum?id=hz3NtNpDNv",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper studies temperature profile prediction of furnaces and tries to integrate a typical approach named Hottel Zone into Deep Learning models.",
        "strengths": "While I may lack specific expertise in furnace behaviors, the paper’s strengths are difficult to identify due to significant weaknesses in structure and clarity.",
        "weaknesses": "The primary weakness of this paper is its lack of a clear, structured approach, which hampers readability and information flow. Also, the writing style is very unclear and there are missing unfinished statements, links (like the GitHub from line 68), the red text, etc."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper proposes a physics-constrained neural network approach for temperature profile prediction in furnaces. The authors introduce a regularization technique based on the Hottel Zone method and demonstrate its effectiveness across various neural network architectures (MLP, LSTM, KAN, xLSTM).",
        "strengths": "**Originality**  \n   This work presents an innovative application of the Hottel Zone method for regularizing neural networks, which sets it apart from traditional data-driven models. Using a physics-based constraint to enhance temperature prediction accuracy is a novel concept.\n\n**Clarity**  \n   The methodology, especially the integration of the Hottel Zone method, is well-explained, allowing readers to understand how the physics constraint aids in capturing the furnace's temperature dynamics. Experimental design and metrics are clearly presented.",
        "weaknesses": "**Domain-Specific Limitation**  \n   The method is specifically tailored to furnace temperature prediction, limiting its general applicability to other domains. This specificity reduces its potential impact within a broader range of applications or datasets outside of high-temperature industrial processes.\n\n**Inference Time Analysis**  \n   Although training time with regularization is detailed, the paper lacks explicit performance analysis for inference time on industrial setups. Given the potential complexity of incorporating physical constraints, a clear discussion on inference time efficiency for real-time applications would strengthen the paper's practical value.\n\n**Comparative Baseline**  \n   The paper does not provide a comprehensive comparison with other physics-informed neural networks or hybrid models, which makes it difficult to evaluate the novelty of this approach relative to existing techniques in similar settings.\n\n**Implementation Complexity**  \n   The integration of Hottel Zone constraints may complicate the model deployment process in practical, real-time environments. The paper would benefit from discussing strategies to mitigate the computational overhead associated with this regularization approach."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper investigates a novel approach to improve the temperature profile prediction of furnaces in foundation industries. Physics-constrained networks are used to obtain the real-time inference capabilities and careful generalization for real-world applications\nThe effectiveness on various neural network architectures, including Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM),\nExtended LSTM (xLSTM) and Kolmogorov-Arnold Networks (KANs) are demonstrated.",
        "strengths": "This works conducts many experiments on different datasets and different methods to evaluate the model.",
        "weaknesses": "The organization of this paper should be improved. The results from MLP, LSTM, DLSTM, KAN and xLSTM and their physics-based improvements (PBMLP, PBLSTM, PBDLSTM, PBKAN and PBxLSTM) are compared. But this paper is to improve the neural networks using Hettel Zone constraint, I think the results from pure Hettel Zone method should also be included. If it is a matter of pages limit, I suggest to move some text of the background of the proposed method (in Section 3) to the appendix. Other than that, the author should consider does it really neccessary to compare the results from so many neural networks with simple structures. Adding physics terms in loss function is a normal method, like in continuum fluid mechanics, so this is an application of PINN in furnace temperature prediction. The author should also check the grammar, such as the last sentence of the Abstract (\"We also discussion the data generation involved\")."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The manuscript presents a physics-aware deep learning model that integrates principles from the Hottel zone method for real-time prediction of furnace temperatures in the foundation industry. The authors propose a reformulation of the Hottel zone method's equations and introduce an energy-balance based constraint as a regularizer within the neural network. Results are demonstrated across various neural network architectures, showcasing the effectiveness of their approach.",
        "strengths": "The problem is well motivated and is significant for the foundation industry. \nThe use of physics-based constraints using principles of conventional zone method appears to be a good idea and is also shown to enhance the quality of the predictions. Such efforts are vital and essential for sectors such as foundation industry, where much work is needed on adoption of machine learning and deep learning technologies. This could represent a valuable applied contribution to the process industry.",
        "weaknesses": "Clarity: \nStructure of the paper makes reading quite difficult. Important information for evaluation is dispersed throughout the paper at varied locations, including a quite lengthy appendix. A more organized structure would enhance the clarity of the paper. Consider compressing the important information in tables wherever possible, instead of paragraphs. \n\nOriginality/Novelty: \n1. The claim of being \"first-of-its-kind\" is questionable in light of a paper presented at NeurIPS 2023 workshop (https://arxiv.org/abs/2308.16089), which has not been cited. What is the improvement achieved over earlier results? In contrast, the results reported here appear to be inferior going by the scale of error metric values. A comparative analysis with the mentioned work would be beneficial to clarify this discrepancy.\n2. Investigation of Physics-constraints in variety of neural network architecture does not add value to the paper. The reason of choosing these specific network architectures is also not clear.  \n3. Hottel zone method seems to take only 5 minutes for simulating temperatures across entire furnace for a 341 minute real process. For 1 time step, the PCNN model takes 0.5 seconds. What is the total time taken by PCNN for 341 minute process prediction? From a practical standpoint, is it even worth developing the PCNN model that accelerates the prediction marginally while losing accuracy? \n\nFinally, while this work is an important piece of applied ML work, it does not qualify as an original contribution to the field of machine learning in reviewer's opinion."
      }
    ],
    "rating_avg": 4.5,
    "confidence_avg": 3.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "qnAZqlMGTB",
    "title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
    "authors": [
      "Junming Lin",
      "Zheng Fang",
      "Zihao Wan",
      "Fuwen Luo",
      "Chi Chen",
      "Peng Li",
      "Yang Liu",
      "Maosong Sun"
    ],
    "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on ofﬂine video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a signiﬁcant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the ﬁrst comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features ﬁve questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 15 open-source and proprietary MLLMs and ﬁnd that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform signiﬁcantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.",
    "keywords": [
      "Benchmark",
      "Streaming Video Understanding",
      "Multimodal Large Language Models",
      "Video Benchmark",
      "Evaluation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=qnAZqlMGTB",
    "forum_url": "https://openreview.net/forum?id=qnAZqlMGTB",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This work proposes a benchmark called StreamingBench to evaluate video LLM capabilities in streaming settings. StreamingBench introduces several tasks tailored to streaming scenarios, including real-time visual understanding, omni-source understanding, and contextual understanding.",
        "strengths": "- This work introduces a new benchmark designed to evaluate video models in streaming scenarios.\n- It conducts insightful experiments, such as \"Does Redundant Information Affect Contextual Understanding?\", which provide valuable perspectives in this area.",
        "weaknesses": "- Although this benchmark focuses on streaming scenarios, a standard video LLM can handle it effectively with simple preprocessing. For instance, whenever a question arises, the model can process all frames up to that timestamp. With this approach, the benchmark may not differ significantly from traditional video benchmarks. Therefore, it is essential for this benchmark to identify scenarios that cannot be simplified to an offline setting.\n\n- While handling redundant information is indeed critical for video LLMs, this challenge is not exclusive to streaming scenarios; it is a general issue for any long-video task. As a result, the insights from this paper may be overshadowed by findings from benchmarks specifically focused on long-video understanding.\n\n- The annotation process lacks clarity. Specifically, how do human annotators manually label QA pairs for omni-source understanding and other contextual understanding tasks? What measures are in place to ensure the quality of each question, and what specific strategies were employed?"
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper introduces StreamingBench, a benchmark designed to evaluate the capabilities of MLLMs in understanding online streaming videos. Key features of StreamingBench include the ability to pose questions at any point during the video, rather than requiring the full video to be viewed first. The benchmark also considers both visual and audio inputs, and it takes into account the influence of historical interactions in multi-turn dialogues.",
        "strengths": "- StreamingBench addresses a relatively unexplored area in MLLM research—real-time video understanding. By allowing questions to be asked at any moment and incorporating both audio and visual data, it expands the scope of existing benchmarks.",
        "weaknesses": "- The methodology for collecting 900 videos from YouTube lacks sufficient detail.\n- Given that the study focuses on a model's capability that is seldom addressed—real-time video understanding—it would be beneficial to create or curate a specific supervised fine-tuning (SFT) dataset. This would allow for an evaluation of model performance post-SFT.\n- There is a lack of exploration into the model’s ability to generate proactive outputs. Designing a corresponding SFT dataset to assess whether the model performs better with prior exposure to similar outputs would provide valuable insights.\n- Clarification is needed on how open-source models like Qwen2-VL tackle omni-source understanding problems, particularly in the absence of audio inputs. This comparison could shed light on the robustness of the proposed benchmark."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "The paper introduces StreamingBench, a benchmark designed to evaluate the streaming video understanding abilities of Multimodal Large Language Models (MLLMs). Traditional MLLMs are effective in offline video comprehension but struggle with real-time, streaming scenarios that require instant processing, synchronizing visual and audio inputs, and understanding context over time. StreamingBench addresses this by presenting 900 videos across diverse real-world scenarios, structured into 18 tasks and 4,300 human-curated question-answer pairs. These tasks test MLLMs on real-time visual, omni-source, and contextual understanding, aiming to bridge the gap between MLLMs and human-level comprehension in streaming contexts. Testing 13 MLLMs, including state-of-the-art proprietary models, revealed significant limitations in current models, especially in omni-source and contextual tasks, suggesting that MLLMs need further development to match human performance in real-time understanding. In general, it is a solid paper and I would recommend acceptance to it.",
        "strengths": "1. It is the first valid benchmark on streaming long videos. The questions are designed properly to reflect the information gained in a streaming long video, and highly resembles what human will ask when continuously watching a video.\n2. The evaluation and discussion are both very solid.\n3. Human performance is another plus.",
        "weaknesses": "1. The real-time understanding part is nice, but seems a little bit trivial. From all kinds of NIAH evaluations, all models can best answer questions near the ending part of the input, and questions related to \"current moments\" (which is actually the ending part of input as implemented) might not be so important. Would love to see the understanding on \"remembering earlier moments\" and the discrepancy from \"current moments\" for LMMs.\n\n2. The omni-source (visual+audio) part is good. However, how are LMMs without audio abilities evaluated? As these `audio'-related questions seem to be mostly about speeches, do authors plan to interleave text ASR into the model for evaluation? At present, sadly we only see a black-box Gemini-1.5-Pro (for which we do not know how they integrate audio and video) being evaluated with audio.\n\n3. A minor suggestion: the omnisource part of the benchmark is related to \"referring reasoning\" part of LongVideoBench, an interleaved benchmark for frames and ASR texts, which also needs to judge between concurrent video and audio information in a video. As some other long video benchmarks discussed in Tab 1, please also try to discuss it in the revised paper."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "The authors propose a new benchmark, StreamingBench, for evaluating MLLMs in streaming video understanding. It assesses three aspects of streaming video understanding: real-time visual understanding, omni-source understanding, and contextual understanding. There are 18 tasks in total. They evaluate 13 open-source Video MLLMs and 3 proprietary MLLMs on this benchmark and analyze the results.",
        "strengths": "S1: In this paper, the authors propose a benchmark to evaluate the MLLMs' capabilities of streaming video understanding, which is novel and unexplored previously. I believe this will facilitate the advancement of Video MLLMs.\n\nS2: The benchmark considers both video and audio modalities, which have been absent in most previous benchmarks.\n\nS3: The experiments and analysis are comprehensive and detailed, effectively highlighting the limitations of current Video MLLMs in understanding streaming video.\n\nS4: The writing is clear and well-structured.",
        "weaknesses": "W1: The impact of language model size on performance has not been analyzed. For instance, models like InternVL-V2 come in 1B, 2B, 4B, 8B, 26B, 40B, and 72B parameter versions, while Video-LLaMA2, LLaVA-OneVision, and Qwen2-VL also have 72B versions. Expanding your experiments to include these variations and providing a more detailed analysis would enhance your work. Additionally, exploring the number of frames the model can process would offer valuable insights.\n\nW2: Several significant models are missing from the evaluation, such as LongVILA [1], Long-LLaVA [2], and Oryx [3]. Including these would provide a more comprehensive comparison.\n\n[1] https://github.com/NVlabs/VILA/blob/main/LongVILA.md\n\n[2] https://github.com/FreedomIntelligence/LongLLaVA\n\n[3] https://github.com/Oryx-mllm/Oryx"
      }
    ],
    "rating_avg": 5.75,
    "confidence_avg": 4.5,
    "decision": "Reject",
    "meta_review": "The paper presented a benchmark for stream video understanding. The paper received mixed ratings from four reviewers. Although some of the reviewers appreciated the importance of stream video understanding and the creation of a corresponding benchmark to advance this research area, there are some concerns still remaining in the current paper. First, as pointed out by reviewer 7TRD, the paper needs more discussion on particular insights of real-time stream video understanding, which is considered a different direction from the existing long-video understanding in an offline setting, including from both the benchmark, the method, and the evaluation levels. Both reviewers szQS and wiNw also mentioned in the weakness part that the unique capability of the model for handling online real-time stream video understanding is seldom addressed. Second, the data processing details are not clearly presented in the paper, and some additional analysis experiments should be added to better show the effectiveness of the model. Based on these significant comments, AC finally decided to reject the submission for this time.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "2Sn0ty7zoI",
    "title": "Learning through Conditioning on Natural Language Feedback",
    "authors": [
      "Dylan Hillier",
      "Cheston Tan",
      "Jing Jiang"
    ],
    "abstract": "In this paper we explore the simple idea of teaching models by allowing them to condition their answers on natural language feedback. Motivated by the idea that natural language interactions provide a targeted, flexible, and level-appropriate reward signal, we study the ability of small instruction-tuned models to leverage feedback from a larger frontier model. We find while the frontier model provides generally high quality feedback, especially smaller models can struggle to use this due to noise in their generative output. After incorporating techniques like negative sampling, we find that models trained on these feedback-conditioned responses can perform similarly to those trained directly on teacher responses. We explore training using supervised finetuning and preference learning algorithms over a broad set of tasks including Big-Bench Hard. These findings are broadly applicable and our methods rely only on the ability of models to give and receive linguistic feedback. As such, they contribute to a growing body of work exploring how to best utilise the linguistic capabilities of language models for human-like instructive learning.",
    "keywords": [
      "Social Learning",
      "Natural Language Feedback",
      "Instructive Learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=2Sn0ty7zoI",
    "forum_url": "https://openreview.net/forum?id=2Sn0ty7zoI",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "N0MnPLK6r7",
    "title": "Toward Human-Interpretable Explanations in a Unified Framework for GNNs",
    "authors": [
      "Kyeongrok Park",
      "Hyunju Kang",
      "Hogun Park"
    ],
    "abstract": "As Graph Neural Networks (GNNs) are increasingly applied across various domains, explainability has become a critical factor for real-world applications. Existing post-hoc explainability methods primarily focus on estimating the importance of edges, nodes, or subgraphs in the input graph to identify substructures crucial for predictions. However, these methods often lack human interpretability and do not provide a unified framework that incorporates both model-level and instance-level explanations. In this context, we propose leveraging a set of graphlets---small, connected, non-isomorphic induced subgraphs widely used in various scientific fields---and their associated orbits as human-interpretable units to decompose GNN predictions. Domain experts can select the most relevant graphlets as interpretable units and request unified explanations based on these units. To address this problem, we introduce UO-Explainer, the Unified and Orbit-based Explainer for GNNs, which utilizes predefined orbits that are generalizable and universal across graph domains as interpretable units. Our model decomposes GNN weights into orbit units to extract class-specific graph patterns (model-level) and to identify important subgraphs within individual data instances for prediction (instance-level). Extensive experimental results demonstrate that UO-Explainer outperforms existing baselines in providing meaningful and interpretable explanations across both synthetic and real-world datasets.",
    "keywords": [
      "eXplainable AI",
      "Graph Neural Networks"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=N0MnPLK6r7",
    "forum_url": "https://openreview.net/forum?id=N0MnPLK6r7",
    "reviews": [
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The paper addresses the problem of explainability in (black-box) graph neural networks (GNNs). As the existing methods lack explanations that are human interpretable as well as a unified framework to perform both instance specific and model-level explanations, the paper proposes a framework based on graphlets (and orbit bases). These predefined graphlets and their associated orbits contribute to both instance and model-level explanations.",
        "strengths": "- The problem of explanations of GNN is relevant and timely as GNNs are being applied in many domains.\n\n- The unified framework of having both instance-level and model-level explanations is interesting.\n\n- The experiments have many different settings. The number of baselines and datasets is comprehensive.",
        "weaknesses": "- The graphlets (orbit bases) as human interpretable units need justifications.\n\n- A strong assumption is that both the instance-level and model level explanations depend on these graphlets (orbit bases). This also needs justification.\n\n- Some experimental settings could be improved."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper introduces UO-Explainer, a framework for Graph Neural Networks (GNNs) that provides human-interpretable explanations at both model and instance levels. It utilizes graphlets and orbits as interpretable units to reveal the significance of specific graph structures for predictions. This unified approach aims to address limitations in previous GNN explainability methods, particularly in providing coherent, interpretable insights. The experiments on various synthetic and real-world datasets demonstrate its effectiveness.",
        "strengths": "1. The idea of leveraging graphlets and orbits as units for interpretation is interesting\n\n2. The experimental results show UO-Explainer accurately provides explanations than baselines at both model-level and instance level.\n\n3. The source code is publicly available",
        "weaknesses": "1. The paper aims to provide human interpretable explanations. However, there are no experiments asking human evaluators to evaluate the quality of the explanations. The authors should consider adding some experiments of human evaluation.\n\n2. The model-level explanation is only evaluated on synthetic datasets. It is unclear if such kind of model-level explanation really makes sense on real-world datasets. For example, does there really exist a model-level graphlet for each class for molecular graphs, which is able to explain the characteristics of each class captured by the target GNN model?\n\n3. Though the idea of leveraging graphlets and orbits as units for interpretation is interesting, the authors might need to give more explanations and real-world examples showing why they make sense in real-world.\n\n4. In lines 201-202, this paper introduces a model-level explanation by decomposing class weights into a linear combination of orbit bases. However, I think it will potentially lose some contextual information between the graphlets, because these bases only capture local information, and they might miss out on broader contextual patterns in the graph. For example, some predictions might depend on interactions between distant nodes or on the overall graph structure, which cannot be encapsulated by small, isolated graphlets.\n\n4. In line 259-261, the paper introduces the instance-level explanation by decomposing the prediction value of the target node into orbit units. But the motivation of the proposed method is not clearly introduced.\n\n5. I am concerned about the efficiency. To generate the instance-level explanations, the proposed method needs to go through all graphlets that include all 0-72 orbits with time complexity $O(|O||V| d^{k−1})$ based on Algorithm 3 and 4. This search method is too expensive to extend to large-scale graphs. Moreover, this paper lacks an experimental analysis of the time complexity. A running time comparison is highly suggested."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This submission introduces a new l framework for explaining Graph Neural Networks (GNNs) that provides both model-level and instance-level explanations in a unified manner. UO-Explainer leverages graphlets and their orbits—small, connected, non-isomorphic subgraphs—as human-interpretable units to decompose GNN predictions. The framework enables users to utilize prior knowledge by selecting relevant graphlets for detailed, interpretable explanations. UO-Explainer decomposes class-specific model weights into orbit units, facilitating the identification of important substructures in both general and instance-specific contexts. Extensive experiments on synthetic and real-world datasets demonstrate that UO-Explainer outperforms existing methods in delivering high-quality explanations.",
        "strengths": "1. The idea is interesting and promising in unifying model-level and instance-level explanations for GNNs.\n2. The technical approach is novel, incorporating orbit basis learning and class-orbit score learning.\n3. The paper is well-organized, with self-contained figures that aid understanding.\n4. The experiments are comprehensive, covering 8 node classification datasets.",
        "weaknesses": "1. The title and abstract overstate the scope, as the method can only explain node classification tasks.\n\n2. The method’s reliance on all subgraphs with up to 5 nodes as explanation units does not inherently ensure meaningful or domain-relevant structures. It is unclear how this improves human interpretability compared to subgraph-based methods that use constraints like connectivity.\n\n3. In real-world datasets such as Gene, linking small graphlets to functionally significant groups can be problematic. The approach does not consider node features or types, making it difficult to distinguish substructures with different real-world meanings or implications. For instance, C-C and CO substructures correspond to the same graphlet, which poses an issue.\n\n4. The notations need improvement, as the notation for the downstream layer does not consider activation functions.\n\n5. The reference paper mentions two fidelity measurements: Fidelity+ and Fidelity-. This submission uses Fidelity+, and I suggest that the authors specify this clearly.\n\n6. Add robust fidelity measurements. As recent studies [1, 2, 3] have shown, Fidelity in the graph domain suffers from out-of-distribution (OOD) issues. Specifically, in Eq. 10, $f_{\\text{prob}}()$ is trained on datasets containing entire graphs, whereas  $G_{vi} - G_{vi}^{\\text{ex}}$ is a smaller subgraph with a different distribution. Thus, the prediction  $f_{\\text{prob}}(G_{vi} - G_{vi}^{\\text{ex}})$ may not be reliable. I recommend that the authors consider robust fidelity measurements, such as Robust Fidelity [1], OAR, SimOAR[2], or GInX-Eval[3], or demonstrate that this distribution shift does not impact the main results.\n\n[1] Zheng, Xu, et al. \"Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks.\" The Twelfth International Conference on Learning Representations. (2024)  \n[2] Fang, Junfeng, et al. \"Evaluating post-hoc explanations for graph neural networks via robustness analysis.\" Advances in Neural Information Processing Systems 36 (2023).  \n[3] Amara, Kenza, Mennatallah El-Assady, and Rex Ying. \"GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations.\" XAI in Action: Past, Present, and Future Applications. 2023"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The authors propose an graphlet/orbits based GNN Explainer for providing human-interpretable explanations. UO-Explainer decomposes GNN weights into orbits and uses these orbits as explanatory units. It can provide both model-level explanations and instance-level explanations. Experiments on synthetic and real-world datasets show that UO-Explainer outperforms baseline explainability methods in providing meaningful, interpretable explanations.",
        "strengths": "S1: Leverages orbits within small graphlets as human-interpretable units for explanations. This even allows users to define their own units of interest instead of orbits if desired.\nS2: Provides explanations in a unified framework at both the model and instance level",
        "weaknesses": "C1: Lack of intuitive representation: what's orbits in graphlet? why orbits are used as explanation unit? What're unique property of graphlet and oribts? What's advantages of graphlet and orbit compared to existing methods? Why the proposed oribit-based explainer can unify both levels?\n\nC2: Limited to using orbits from 2-5 node graphlets, restricting the explanation patterns. There may be cases where important patterns in the data involve larger graphlets that cannot be captured by this restricted set of units. \n\n\nC3: As a follow-up, we need to predefine the graphlets/orbits, rather than learning them from data. Would it be possible to extend the base set of graphlet. For example, considering these 2-5 node graphlet as seeds, how to dynamically discover larger graphlet that might be more meaningful to exaplain the prediction ability of a GNN."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 4.5,
    "decision": "Reject",
    "meta_review": "The paper proposes an algorithm for explaining black-box GNNs. The paper is motivated by the observation that existing methods lack explanations that are human interpretable and the ability to perform both instance specific and model-level explanations. The paper proposes a framework based on predefined graphlets and their associated orbits to perform both instance and model-level explanations. The reviewers have highlightes several concerns related to the justification of using graphlets, their connections to human interpretability, and substantiating the various claims made in the paper. The authors opted not to submit a rebuttal, leading to the conclusion that the paper is not yet ready for publication.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "w7P92BEsb2",
    "title": "PIED: Physics-Informed Experimental Design for Inverse Problems",
    "authors": [
      "Apivich Hemachandra",
      "Gregory Kang Ruey Lau",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. \nDue to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments.\nHowever, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. \nThis work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. \nPIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. \nThrough experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.",
    "keywords": [
      "Physics-Informed Neural Network",
      "PINNs",
      "Experimental Design",
      "AI For Science",
      "Active Learning",
      "Data Selection"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=w7P92BEsb2",
    "forum_url": "https://openreview.net/forum?id=w7P92BEsb2",
    "reviews": [
      {
        "rating": "8",
        "confidence": "2",
        "summary": "The paper proposes a novel algorithm for solving the inverse problem in PDEs - that is, estimating the parameters of a PDE governing the dynamic characteristics of a system.  In particular, the paper assumes that (a) obtaining (x,y) observations of the system is expensive, so we must select our observation points carefully; and (b) observations require an initial setup that we cannot (practically) repeat, so we must specify our test points up-front and not dynamically as in e.g. Bayesian Optimization.\n\nTo tackle this problem the paper suggests physics-informed experimental design (PIED) that uses two sets of PINNs to select appropriate test points for a given system of PDEs (with a-priori unknown parameters).  The general approach uses a set of PINNs operating as forward simulators to generate functions satisfying the PDEs, sampling these for a set of points X, then using PINNs as inverse solvers to estimate the PDE parameters from the observations.  The efficacy of the points X is measured as the difference between the \"real\" PDE parameters (used in the forward simulators) and the corresponding estimated estimated PDE parameters.",
        "strengths": "The algorithm is certainly interesting.  The PIED framework certainly looks practical.  The motivation behind and justification of each step is presented, and the experimental results look good.",
        "weaknesses": "One point that needs to be address in the paper is that of computational cost.  After all, one of the motivations of using PINNs in parallel is computational efficiency, so it would be good to have a comparison in terms of same."
      },
      {
        "rating": "8",
        "confidence": "2",
        "summary": "This paper suggests PIED, a method for optimal experimental design for PDE inverse problems via Physics-informed neural networks (PINN). The PIED framework consists of three steps: 1. A PINN to learn a forward simulator that maps parameters $\\beta$ to solutions of the PDE $u_\\beta$. 2. An observation selector that returns noisy predicted observations $\\tilde Y= u_\\beta(X) + \\epsilon$ at a fixed number of sensor placements $X$. 3. A PINN inverse solver that predicts the ground truth parameter $\\hat \\beta(X, \\tilde Y)$ from $X$ and $\\tilde Y$. 4. The optimal sensor placements $X$ are found by minimising the MSE between $\\beta$ and $\\hat \\beta(X, \\tilde Y)$ which requires back-propagation through the inverse problem solver. The paper claims the following contributions and innovations: \n- The weight initialisations of all PINN-based components are shared and from a pretrained model which stabilises and accelerates training.\n- The forward simulator for various choices of $\\beta$ are learned in parallel\n- The mean square error between $\\hat\\beta(X, \\tilde Y)$ and $\\beta$ is approximated by learning the inverse solver with a smaller number of training steps (FIST criterion for ED)\n- The mean square error between $\\hat\\beta(X, \\tilde Y)$ and $\\beta$ is approximated through a linearisation of the PINN training dynamics (MoTE criterion for ED)\n\nThe framework is tested on optimal experimental design for sensor placement on Eikonal, Wave, and Navier-Stokes equations as well as groundwater and cell growth dynamics, and benchmarked against Bayesian experimental design, random sensor placements, and sensor placements on a grid.",
        "strengths": "- Overall, the paper is well written and seems methodologically sound.\n- The paper addresses a challenging experimental design problem with little existing approaches.\n- There is sufficient experimental results to support to proposed approach.\n- The paper combines an interesting set of ideas: \n\t- Meta learning and shared weights for more efficient PINN training. \n\t- Approximate training dynamics through of the inverse solver to make back-propagation through the inverse solver feasible.",
        "weaknesses": "- The approach is mostly motivated by the comparison with classical simulators, but other types of neural surrogate models for the forward simulator are not mentioned. This is probably because most such approaches like Fouier Neural Operators don't investigate the ED downstream task. Nevertheless, this made me wonder how much this approach actually relies on Physics-informed neural networks. For example, the processing in parallel threads would be typical for all approaches that target ED with a neural network.\n- The description of the algorithm/framework is a little inconsistent. The PIED framework in Figure 1 suggests that the PIED framework is trained end-to-end, while Algorithm 3 in the appendix clarifies that training proceeds in three phases that are not interleaved: Optimising for initial weights,  training to define ED criteria, optimisation of ED criterion. I wish that this was clearer from the main text."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors developed a physics-informed experimental design approach for the inverse problems to find the optimal and most informative design parameters. The paper is well-organized.",
        "strengths": "The theoretical analysis of PINN is thorough.",
        "weaknesses": "The effect of some experiments is not significant."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper solves the one-shot experiment design using PINNs in both forward and inverse problems. It overcomes computational bottlenecks by parallelism and meta-learning of initialization. The experiments on both synthetic and real-life datasets show the performance improvements.",
        "strengths": "1. this paper joins many advanced techniques together to solve the problem.\n2. most of the representation is clear, with extensive experiments and details.",
        "weaknesses": "1. The overall algorithm flow is unclear in the main text. Figure 1b is too general and missing almost all of the details of the technique.\n2. Does not compare with existing NN-based experiment design methods.\n3. Almost all techniques are adopted from existing literature."
      }
    ],
    "rating_avg": 7.0,
    "confidence_avg": 2.5,
    "decision": "Accept (Poster)",
    "meta_review": "This paper proposes a new method for optimal experimental design for PDE inverse problems via Physics-informed neural networks (PINN).  The approach entails learning a forward simulator that maps parameters to PDE solutions;  an observation selector that returns noisy predicted observations  at a fixed number of sensor placements; and an inverse solver that predicts the ground truth parameter. This sets up an objective function for finding optimal sensor placements.  Favorable experimental comparisons against Bayesian experimental design, random sensor placements, and sensor placements on a grid for Eikonal, Wave, and Navier-Stokes equations as well as groundwater and cell growth dynamics is the primary strength of this paper. Computational cost, novelty and some remarks on empirical significance were brought up as weaknesses, but the paper received unanimously positive reviews.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "wE5xp3zBaQ",
    "title": "The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses",
    "authors": [
      "Grzegorz Gluch",
      "Berkant Turan",
      "Sai Ganesh Nagarajan",
      "Sebastian Pokutta"
    ],
    "abstract": "We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as *interactive protocols* between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for *almost every* discriminative learning task, at least one of the two — a watermark or an adversarial defense — exists. The \"*almost*\" refers to the fact that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a *transferable attack*. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool *all* efficient defenders.\n\nTo this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a *cryptographic primitive*, thus requiring the underlying task to be computationally complex. These two facts imply an \"*equivalence*\" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.",
    "keywords": [
      "Watermarks",
      "Adversarial Defenses",
      "Transferable Attacks",
      "Interactive Proof Systems",
      "Cryptography",
      "Backdooring",
      "Game Theory",
      "Learning Theory"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=wE5xp3zBaQ",
    "forum_url": "https://openreview.net/forum?id=wE5xp3zBaQ",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper investigates the relationship between watermarks (planted in trained ML models) and adversarial defenses for noiseless classification tasks over a finite set $\\mathcal{X}$. For clarity, let us focus on *binary* classification tasks. Here, a learning task (or equivalently, full data distribution) can be represented by a pair $(D, f^*)$, where $D$ is the marginal distribution over $\\mathcal{X}$ and $f^*: \\mathcal{X} \\to \\\\{0,1\\\\}$ is the true labeling function.\n\nFor any given learning task $(D, f^*)$, consider an interactive protocol in which Alice (ML service provider) interacts with Bob (client). As a service provider, Alice trains a classifier $f: \\mathcal{X} \\to \\\\{0,1\\\\}$ which achieves $\\epsilon$-error on $D$, and sends it to Bob. However, Alice is motivated to secretly plant a “watermark” into her trained classifier $f: \\mathcal{X} \\to \\\\{0,1\\\\}$ by making it susceptible to pre-designed adversarial examples. Bob, on the other hand, is motivated to neutralize any backdoors in the classifier f he received from Alice.\n\nThe opposing objectives of Alice and Bob in this framework can be formulated as a *zero-sum two-player* game. Furthermore, by modeling Alice and Bob to be circuit classes of fixed size, the pure strategy space for both Alice and Bob become finite, with explicit bounds on their cardinalities. This setup allows previous results on approximate equilibria [Lipton and Young, 1994] to be applied. Using the zero-sum two-player game formulation, the authors show that any “efficiently learnable” classification task $(D, f^*)$ falls into at least one of the following three cases:\n\n1. **Watermarking.** There exists a watermarking scheme for Alice can compute a classifier f and sequence of adversarial (randomized) queries $x_1, …, x_q$ such that for any circuit (”Bob”) whose size is significantly smaller than hers (i.e., y computed by any such small circuit incurs $\\mathrm{err}(x, y) \\ge 2\\epsilon$) the watermark is *unremovable* and the distribution of her queries $x_1, …, x_q$ is indistinguishable from $D$.\n2. **Adversarial Defense.** There exists a watermark neutralizing (i.e., adversarial defense) scheme for Bob such that either Alice’s queries $x_1,…,x_q$ are non-adversarial (the avg loss of $f$ on $x_1, …, x_q$ is $\\epsilon$-small) or the distribution of Alice’s queries $x_1, …, x_q$ and $D^q$ are distinguishable by small circuits.\n3. **Transfer Attack.** A third possibility not covered by the previous two cases, which has left me confused. Please refer to the Questions section for further details.",
        "strengths": "The paper attempts to formalize an intriguing relationship between two phenomena recently studied in machine learning: watermarking (i.e., planting undetectable backdoors [Goldwasser et al., 2022]) and adversarial defense [Cohen et al., 2019]. A watermark for a classifier attempts to hide specific signatures in its error patterns, while an adversarial defense attempts to maintain performance of a given untrusted classifier f across distributions that are “close to” D, which can be formalized via a weakened notion of statistical distance. Intuitively, there is tension between these two objectives, which the authors attempt to formalize as a zero sum game. Addressing these natural questions would be of wide interest to the ML community.",
        "weaknesses": "The main weakness of this paper lies in the **lack of clarity and precision in its definitions and framework**, which significantly undermine the credibility of any theorems that follow. The presentation of key definitions and interactive protocols are “hand-wavy”, leaving substantial ambiguity in how the results should be interpreted and applied. This vagueness makes it difficult to assess the validity of the theoretical claims and fully appreciate the significance of the results.\n\nWhile the authors give more formal specifications in the Appendix (especially, Appendix B), significant gaps still remain to be filled. In addition, the appendix should provide further technical details after the basic setup and key insights have been presented in the main text, rather than serving as a teaser for readers left confused by the unclear presentation in the body of the work. \n\nOne significant issue is that the modeling of Alice and Bob with size-bounded circuit classes seems to fail a basic type check. In the interactive protocols, Alice and Bob face different tasks that involve different input and output spaces. For instance, in a watermarking scheme for binary classification, Alice is expected to output a representation of a classifier $f: \\mathcal{X} \\to \\\\{0,1\\\\}$ along with queries $ x_1,\\ldots,x_q \\in \\mathcal{X}$ (a separate issue here is Alice's inputs are not specified and the dependence on the input domain’s cardinality doesn't appear anywhere in the quantitative results, which raises concerns). On the other hand, Bob’s inputs are sequences $x_1, \\ldots, x_q$ and needs to output $y \\in \\\\{0,1\\\\}^q$. Without further clarification, it’s unclear how a size $s$ circuit for Alice compares to a size $s$ circuit for Bob since even the input and output domains do not match. This is one of several issues with the paper's framework that, collectively, call into question the overall rigor and applicability of the approach. Please refer to the **Questions** section for additional issues.\n\nMoreover, the paper **incorrectly applies previous results from cryptography**, which indicates a lack of understanding of the field. In particular, the interpretation of the results based on [Goldreich, 1990] in Section 5.2 is incorrect. Goldreich’s result applies to an *ensemble* of random variables, i.e., a *sequence* of distributions, whereas the EFID pairs the authors define in Section 5.2.1 are particular instances of distributions. Moreover, the ensembles used by Goldreich are *uniformly constructible*, meaning that there exists a single Turing machine generating random samples from X_n given the input 1^n. This contrasts with the non-uniform circuits used in this work. Given this misunderstanding, the title of Section 5,  *Transferable Attacks are “Equivalent” to Cryptography* is misleading and unnecessarily provocative. Even if Goldreich’s equivalence result could be applied here (which I find unlikely), the only concrete implication for cryptography is the existence of pseudorandom generators (PRGs), which are basic cryptographic primitives but do not represent the entire research field.\n\nIn addition, the restriction to succinct circuits feels somewhat ad hoc, seemingly added specifically to prevent Alice and Bob from hardcoding outputs. It seems likely that the approximate equilibria results (Theorem 1) would still hold without the succinctness assumption, albeit with different bounds, as the key requirement is simply that the pure strategy space remains finite with explicitly known bounds on its cardinality. This raises concerns about the integrity of the formulation, with the succinctness restriction serving more as a workaround than an integral component of the setup.\n\nOverall, the paper would benefit greatly from prioritizing clarity over excessive generality, focusing on straightforward, concrete setups and presenting mathematical results clearly and precisely, without the informal remarks.\n\n**Editorial comments**\n- (Abstract) The phrase \"almost every\" learning task is misleading. Terms like “almost every” carry strong connotations in measure theory. The mere fact that a mathematical object is “irregular” or \"very complex\" does not imply that it is rare (e.g., from a measure-theoretic perspective). For instance, with respect to the uniform measure on [0,1], “almost every” real number in the unit interval is uncomputable.\n- (Line 214) Advantage should be an equal sign.\n- (Line 243) The unremovability condition, as stated, is clearly incorrect. Bob can simply respond with random y and the realized error can be 0 with small but non-zero probability. Even if this is intended as an informal simplification of the definitions in Appendix B, it should not be so obviously wrong.\n- (Line 248) The term \"defender\" is used inconsistently alongside other terms like \"player\", \"prover\", and \"Bob\". It would be better to choose a single term for the recurring entities and use it consistently throughout the paper."
      },
      {
        "rating": "5",
        "confidence": "2",
        "summary": "This paper explores the relationship between backdoor-based watermarks and adversarial defenses in machine learning.  These concepts are formalized as interactive protocols between two players and proved that for almost every discriminative learning task, at least one of the two exists. The main contribution is the identification of a third, counterintuitive option: a transferable attack.  This term describes an algorithm capable of generating queries that are indistinguishable from the data distribution, yet can deceive even the most effective defenses.  The authors demonstrated the necessity of a transferable attack using homomorphic encryption and proved that any task susceptible to such an attack implies a cryptographic primitive.",
        "strengths": "- Formalization of the relationship between backdoor-based watermarks and adversarial defenses is useful.",
        "weaknesses": "- I take umbrage with the following claim: “Along the way to proving the main result, we identify a potential reason why this fact was not discovered earlier.”. There are multiple prior works that have investigated the trade-off between adversarial robustness and backdoors/watermarks [Weng et al,  Sun et al, Gao et al, Niu et al., Fowl et al., related work in Tao et al. is a good summary]. Although most of these papers are more empirical, this paper completely ignores an entire line of work. The paper primarily focuses on theoretical results without providing clear guidance on how these results can be translated into practical applications, and I find it difficult to assess if this paper is saying anything profound beyond what has already been discussed in the referenced papers. This is my main concern. Some suggestions:\n    * (1) Include a detailed discussion of how their theoretical results relate to or extend the empirical findings in the papers you cited.\n    * (2) Explicitly state what novel insights their work provides beyond the existing literature.\n    * (3) Add a section on potential practical applications or implications of their theoretical results.\n\n- In Definition 2.3. Why is the coefficient before epsilon, 7?\n\n- The paper primarily deals with discriminative learning tasks, like classification. These tasks assume a clear relationship between input data (e.g., images) and distinct output labels (e.g., \"cat\" or \"dog\").  How can the trade-offs be captured for generative models?\n\n\n[Weng et al] Weng, Cheng-Hsin, Yan-Ting Lee, and Shan-Hung Brandon Wu. \"On the trade-off between adversarial and backdoor robustness.\" Advances in Neural Information Processing Systems 33 (2020): 11973-11983.\n\n[Sun et al] Sun, Mingjie, Siddhant Agarwal, and J. Zico Kolter. \"Poisoned classifiers are not only backdoored, they are fundamentally broken.\" arXiv preprint arXiv:2010.09080 (2020).\n\n[Gao et al.] https://openreview.net/forum?id=nG4DkcHDw_\n\n[Niu et al.] Niu, Zhenxing, et al. \"Towards unified robustness against both backdoor and adversarial attacks.\" IEEE transactions on pattern analysis and machine intelligence (2024).\n\n[Fowl et al.] Fowl, Liam, et al. \"Adversarial examples make strong poisons.\" Advances in Neural Information Processing Systems 34 (2021): 30339-30351.\n\n[Tao et al.] Tao, Lue, et al. \"Better safe than sorry: Preventing delusive adversaries with adversarial training.\" Advances in Neural Information Processing Systems 34 (2021): 16209-16225."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors provably identify the following trichotomy - for every learnable task there is an adversarial defence and/or a watermarking scheme, while for learning tasks which have associated cryptographic hardness results, there is a transferable attack.",
        "strengths": "The most important contribution of this manuscript is the proof that the notions adversarial robustness and watermarking schemes is complementary to the notion of cryptographically hard learning schemes. The authors use a lot of existing results across various fields creatively, to arrive at this result, which makes the technical part of the paper interesting in its own right.",
        "weaknesses": "My main concern with the paper is that the definitions (especially the two player games) is too carefully constructed to be readily used in conjunction with existing results from game theory, cryptography, and learning theory. There is lack of justification / discussion on several fronts, which should be addressed for the paper to be useful to the community at large.\n\nA secondary but related weakness is the lack of a technical discussion section. A detailed overview of proof techniques section is much required. I have detailed a list of my questions in the next section.\n\nA better related works section is also warranted. For example, there are certain confusions arising in the discussion containing the comparison with the Christiano et al. (2024) paper. See the questions section.\n\n*Note to the authors:* Regardless of the acceptance results at this conference, I believe the authors should prepare a longer version of this manuscript and submit it to a journal like TMLR. It would be of immense value to the community.\n\n\n--------\nAfter the rebuttal period, I have decided to raise my score to indicate my positive opinion of the paper. \n\n*Note:* At this point I cannot justify raising the score any higher due to numerous missing definitions and discussions (detailed in the reviews by myself and other reviewers, and mostly agreed upon by the authors) that should have been included in a theoretical paper in the first place."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper gives formal definitions of “watermarks” through backdoor (not for LLM’s output, but rather for models themselves) and “adversarial robustness” and “transferable attacks” in their own way. Meaning that the definitions do not necessarily match what is the common usual way of defining them, but the definitions make sense in their own way.\n\nThen, the paper observes that these three notions are complementary for a “learning task”. A task is modeled using a distribution D (on instances) and a function h (to label them) and differs from the method of using a family of h (as hypothesis class). In particular, the paper shows that for each learning task, at least one of the following holds: either we can watermark models that predict that task, or that we can resist backdoor, or that transferability attacks work.\n\nIntuitively, the main result is proved by observing that a watermark through a backdoor aims to plant a backdoor and later use specific queries to detect it (using wrong answer) and this is exactly what a defense against backdoor wants to avoid. So the two notions are rather complementary. Once the paper aims to prove this formally, they show that a third case is also possible, which in their formalism is referred to as the transferability attack.\n\nThe paper then shows that “transferability” attacks could probably exist assuming fully homomorphic encryption, and that transferability attacks *require* one-way functions (they say PRG, but that is the same as OWFs), and hence it implies secret key crypto.\n\nFinally, the paper claims that PAC learnable families (ie., those with bounded VC dimension) always can have adversarial robustness and “watermark against fast adversaries”.",
        "strengths": "A formalism of the intuition behind the duality of watermarks (for models through backdoor) and adversarial robustness is interesting. \n\nAlso, the paper realizes that formal definitions are needed for such results and takes an effort in that direction.",
        "weaknesses": "The new formalisms for the 3 notions of watermark, robustness and transferability need a lot more scrutiny and discussion. For example, there are limits on the time of the adversary that are needed to make these definitions non-vacuous, but these definitions are different from previously established definitions in this regard (e.g., about adversarial examples) and I see no real effort to compare them. (see my question below)\n\n\nAlso, due to the number of results in the main body, their proofs are deferred to the appendix, and perhaps the most exciting result (saying that bounded VC dimension means we can have adversarial robustness) is pushed to the appendix."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This paper derives a theoretical analysis of the relationship between backdoor-based watermarks and adversarial defenses. The authors show that all discriminative learning tasks can be categorized into one of three classes, which suggests there is an inherent security trade-off in ML applications.\n\nReviewers generally appreciated the technical depth of the paper and its clever use of existing techniques to derive their theoretical result. However, most reviewers also found the paper's analysis framework to be lacking in precision, leaving much of the derived theoretical result up to interpretation. The paper is also written in a way that is difficult to absorb for the general ML audience. As a result, practical implications of the paper are also unclear. After the rebuttal period, reviewers and AC discussed thoroughly and reached consensus that the paper's weaknesses currently outweigh its strengths. Thus, AC believes the paper is currently not ready for publication, but encourages the authors to taken into account reviewer suggestions and improve the paper's clarity before resubmitting to a future venue.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "pcnq7fZs4t",
    "title": "Common Feature Learning for Zero-shot Image Recognition",
    "authors": [
      "Shuang Li",
      "Lichun WANG",
      "Kai Xu",
      "Jianjia Xin"
    ],
    "abstract": "The key issue of zero-shot image recognition (ZIR)  is how to infer the relationship between visual space and semantic space from seen classes, and then effectively transfer the  relationship to unseen classes. Recently, most methods have focused on how to use images and class semantic vectors or class names to learn the relationship between visual space and semantic space. The relationship established by these two methods is class-level and coarse-grained. The differences between images of the same class are ignored, which leads to insufficiently tight relationships and affects the accurate recognition of unseen classes.To tackle such problem, we propose Common Feature learning for Zero-shot Image Recognition (CF-ZIR) method to learn fine-grained visual semantic relationships at the image-level. Based on the inter class association information provided by class semantic vectors, guide the extraction of common visual features between classes to obtain image semantic vectors. Experiments on three widely used benchmark datasets show the effectiveness of the proposed approach.",
    "keywords": [
      "Zero-shot Image Recognition；Visual-semantic Relationship；Fine-grained Alignment；Semantic Vectors Generation；"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=pcnq7fZs4t",
    "forum_url": "https://openreview.net/forum?id=pcnq7fZs4t",
    "reviews": [
      {
        "rating": "1",
        "confidence": "5",
        "summary": "This paper proposed to learn a common features for ZSL, which include two kind dictionary for the final prediction.",
        "strengths": "1. The proposed method is innovative in using a dual dictionary approach to improve class prediction accuracy.\n\n2. The paper is well-structured, making it relatively easy to follow the methodology and experimental setup.",
        "weaknesses": "1. The motivation in the third paragraph of the introduction, stating that \"most methods focus on how to use images and class semantic vectors or class names to learn the relationship between visual space and semantic space,\" is somewhat inaccurate. In fact, most methods utilize class-level attributes as semantic information. Some also incorporate embeddings of individual attributes to enhance fine-grained associations.\n\n2. Section 3.2 is overly lengthy and has significant overlap with subsequent sections. The overview could be more concise to avoid redundancy and improve clarity.\n\n3. There is confusion around the definitions of semantic space and attribute space. What is the intended difference? In ZSL, attributes are generally considered a type of semantic information, so a clearer distinction would improve understanding.\n\n4. The paper lacks comparisons with recent methods and is limited in dataset diversity. Additional comparisons with recent approaches and experiments on more datasets, such as CUB, FLO, and SUN, would strengthen the evaluation.\n\n5. More extensive experimentation is needed. The current paper includes only two tables and one figure, which is insufficient to support its claims. Particularly, experiments should cover both conventional ZSL and generalized ZSL.\n\nThis paper appears incomplete and requires substantial revision before submission. More work is needed to clarify motivations, streamline the methodology, and provide comprehensive experimental validation."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper mainly introduces a shared feature learning method (CF-ZIR) for zero-sample image recognition, which extracts shared visual features through attribute guidance, and uses category semantic vectors to guide the generation of image semantic vectors, so as to form the semantic representation of images. In addition, a dual-layer embedding method is proposed to establish fine-grained associations between visual-attributes and visual-semantics. Experimental results show that the CF-ZIR method achieves competitive performance on multiple datasets.",
        "strengths": "1. The dual-layer embedding mechanism is introduced to improve the recognition performance through fine-grained visual-semantic relationship, which exhibits a degree of innovation.\n2. The paper is articulated with clarity and precision.",
        "weaknesses": "1. Although the experimental results are presented in the paper, the description of experimental settings, hyperparameter selection, training details and other aspects is not detailed enough, which may affect other researchers to reproduce the results.\n2. The proposed method does not achieve the best results on multiple datasets. \n3. Several references to the three datasets were made with inconsistent fonts."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes common feature learning for zero-shot image recognition (CF-ZIR) method to learn fine-grained visual semantic relationships. This method leverages inter-class association information from class semantic vectors to guide the extraction of common visual features. Experiments on three datasets show the effectiveness.",
        "strengths": "+ Introducing dual-layer embedding method for zero-shot learning is reasonable.\n+ The experimental results show the effectiveness of the proposed approach on conventional ZSL.",
        "weaknesses": "- The paper leverages inter-class association information from class semantic vectors to guide the extraction of common visual features. In fact, this issue has been defined as visual-semantic domain shift in previous work and has been discussed in several papers, such as [1], [2] and [3]. This paper does not discuss the differences with them.\n\n- This paper only conducts experiments under the conventional zero-shot learning setting, lacking experiments in generalized zero-shot learning.\n\n- Line 16, \"The relationship established by these two methods is class-level and coarse-grained\", what do these two methods refer to?\n\n- Introduction and Related work sections do not discuss the latest research.\n\n- The ablation experiments are insufficient to validate the claims of this paper, especially regarding zero-shot recognition in three spaces discussed herein.\n\n- The paper discerns fine-grained visual-semantic relationships at the image level, but why are experiments not conducted on fine-grained datasets, such as CUB?\n\n- There is a lack of hyperparameter analysis. The current hyperparameters are not convincing. Are they the same across all datasets?\n\n- How does this method demonstrate advantages for attributes like 'small', 'hunter', and 'fast' on AWA2?"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a novel method called Common Feature learning for Zero-shot Image Recognition (CF-ZIR) to address the challenge of learning fine-grade relationships between images and classes in Zero-Shot Learning. CF-ZIR leverages the inter-class association information from class semantic vectors to guide the extraction of common visual features between classes. The author conducted experiments on three datasets to attempt to validate the effectiveness of the proposed method.",
        "strengths": "It seems that the proposed approach is easy to follow.",
        "weaknesses": "This paper has major shortcomings in writing, method innovation, and experiment, so it is not able to be accepted, specifically:\n\nThis paper is hard to read. The meanings of many concepts are difficult to understand. For example, what is ‘the cross domain dictionary learning model’ mentioned in Section 3.2 and what are the particular meanings of ‘common feature’, ‘single dictionary learning model’ and ‘class semantic vectors’? The structure of the model and the calculation process are also hard to understand, what are ‘x2’ and ‘x3’ in Figure 1?\n\nThis paper is filled with grammatical and formatting errors, to the point that it's difficult to list them all. For example, in the abstract, 'zero-shot image recognition (ZIR)' should be 'Zero-shot Image Recognition (ZIR)', 'unseen classes.To tackle' should be ''unseen classes. To tackle', and ' Based on the inter class association information provided by class semantic vectors, guide the extraction of common visual features between classes to obtain image semantic vectors.' should be 'The inter-class association information provided by class semantic vectors guides the extraction of common visual features between classes to obtain image semantic vectors.'\n\nAligning images with attributes is a common idea in existing approaches (Modeling Inter and Intra-Class Relations in the Triplet Loss for Zero-Shot Learning (ICCV19) and Concept Bottleneck Models (ICML20)).\n\nThe author conducted experiments on only three small datasets, two of which are nearly identical (AWA1 and AWA2), and did not compare with multimodal pre-trained models such as CLIP."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.75,
    "decision": "Reject",
    "meta_review": "The paper introduces the Common Feature learning for Zero-shot Image Recognition (CF-ZIR) method, which uses a novel dual dictionary approach to enhance class prediction accuracy by leveraging inter-class association information from semantic vectors. However, the paper suffers from several weaknesses, including a lack of motivation and context and inadequate experimental results and analysis (e.g., ablation study and hyperparameters). Other minors, such as redundancy in writing and numerous grammatical and formatting errors, could be addressed in future submissions.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "3g2iyFU8gA",
    "title": "Learning Fused State Representations for Control from Multi-View Observations",
    "authors": [
      "Zeyu Wang",
      "Yao-Hui Li",
      "Hongyu Zang",
      "Xin Li"
    ],
    "abstract": "In visual control tasks, leveraging observations from multiple views enables Reinforcement Learning (RL) agents to perceive the environment more effectively. However, while multi-view observations enrich decision-making information, they also increase the dimension of observation space and introduce more redundant information. Thus, how to learn compact and task-relevant representations from multi-view observations for downstream RL tasks remains a challenge. In this paper, we propose a Multi-view Fusion State for Control (MFSC), which integrates a self-attention mechanism with bisimulation metric learning to fuse task-relevant representations from multi-view observations. To foster more compact fused representations, we also incorporate a mask-based latent reconstruction auxiliary task to learn cross-view information. Additionly, this mechanism of mask and reconstruction can enpower the model with the ability to handle missing views by learning an additional mask tokens. We conducted extensive experiments on the Meta-World and Pybullet benchmarks, and the results demonstrate that our proposed method outperforms other multi-view RL algorithms and effectively aggregates task-relevant details from multi-view observations, coordinating attention across different views.",
    "keywords": [
      "multi-view learning",
      "reinforcement learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=3g2iyFU8gA",
    "forum_url": "https://openreview.net/forum?id=3g2iyFU8gA",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes a method that combines a bisimulation-based approach with masked representation learning for multi-view reinforcement learning. The core idea is that to enable task-relevant multi-view fusion, it is essential to align the integration process closely with the specific objectives of the task. In other words, when fusing information from multiple views, the task’s specific goals (Equation 8) must be considered. The authors have evaluated their method on two visual control environments, including Meta-World and PyBullet, demonstrating significant performance improvements over baseline methods.",
        "strengths": "- The paper is clearly written and easy to understand.\n- The proposed method that integrates bisimulation metric learning into the fusion process of multi-view states is reasonable.\n- The authors have provided extensive experimental results, covering various visual RL environments, to validate the effectiveness of the method. The paper also includes experiments with missing views as well as additional visualizations to interpret the effectiveness of the method.",
        "weaknesses": "My main concerns involve the novelty of the method and the completeness of experimental comparisons:\n\n- The primary limitation lies in the method's novelty. Although the authors present two core challenges of multi-view RL in the introduction, these challenges have already been extensively explored in prior research. While incorporating bisimulation metrics into state aggregation is reasonable, bisimulation-based methods are also well-covered in existing RL literature, making this combination feel more like a natural choice than a groundbreaking innovation.\n- Although the authors conducted extensive experiments and validated the effectiveness of their approach against various existing multi-view RL methods, there are still two main gaps. First, there is no experimental verification of whether the method remains superior to baseline models in cases with missing views (even with a single view). Second, Seo et al. (2023) proposed the masked world model, which performs well on multi-view RL tasks and has methodological similarities to the approach in this paper. A direct comparison with Seo et al.'s work would provide stronger support for the effectiveness of this method."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes the Multi-view Fusion State for Control (MFSC), which integrates a self-attention mechanism and bisimulation metric learning to fuse task-relevant representations from multi-view observations, and incorporates a mask-based latent reconstruction auxiliary task to obtain more compact fused representations and handle missing views.",
        "strengths": "1. The writing is relatively clear.\n\n2. The performance of the proposed method is validated on Meta-World and Pybullet benchmarks.",
        "weaknesses": "1. The author incorporates bisimulation principles by integrating reward signals and dynamic differences into the fused state representation to capture task-relevant details. As I am aware, [1] also acquires representations for control with bisimulation metrics. Additionally, the author employed a Mask-based Latent Reconstruction strategy, which is analogous to that in [2]. Does this similarity suggest a deficiency in significant innovation or does the author offer additional components or enhancements that differentiate it from the existing strategies in [1] and [2]? Furthermore, it is essential to determine whether appropriate credit and comparison with the prior works in [1] and [2] have been adequately accounted for.\n\n[1] Learning invariant representations for reinforcement learning without reconstruction.\n\n[2] Mask-based Latent Reconstruction for reinforcement learning。\n\n3. Missing many recent visual RL baselines: the baselines used in the paper are all old methods and a large body of the recent methods developed on visual reinforcement learning are ignored [1][2].\n\n[1] TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning.\n\n[2] Mastering Diverse Domains through World Models.\n\n4. Whether this method is only useful for robot control tasks needs to be further verified on more types of environments, such as Carla, atari, etc.\n\n5.  The paper lacks sufficient ablation experiments. The author only ablated MFSC without bisimulation constraints ('MFSC w/o bis') and MFSC without Mask and Latent Reconstruction ('MFSC w/o res'), but not more detailed parts like the Self-Attention Fusion Module.\n\n6. The author claims that MFSC can be seamlessly integrated into any existing downstream reinforcement learning framework to enhance the agent's understanding of the environment. However, there are no relevant experiments to verify this claim."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper presents a novel architecture called Multi-view Fusion State for Control (MFSC), designed to learn compact and task-relevant representations from multi-view observations in reinforcement learning (RL). This approach integrates a self-attention fusion module with bisimulation metric learning to aggregate information from different views, while also using a mask-based latent reconstruction auxiliary task to promote cross-view information aggregation.  Experiments conducted on Meta-World and Pybullet demonstrate the superiority of MFSC over other methods.",
        "strengths": "1.\tThe paper addresses the challenging and significant problem of learning task-relevant fused state representations from multi-view observations, which is a crucial aspect of multi-view reinforcement learning. \n2.\tThe integration of a mask-based latent reconstruction task enhances the model’s ability to learn cross-view information. The proposed approach, combining self-attention and bisimulation metrics, offers an effective solution.\n3.\tThis paper demonstrates the effectiveness of MFSC across multiple challenging benchmarks, including robotic manipulation tasks in Meta-World and control tasks in Pybullet.",
        "weaknesses": "1.\tThis paper does not include comparisons with approaches tailed for visual RL, such as [1-2], particularly multi-view visual RL method like [3]. Evaluating MFSC against such baselines would provide a more accurate assessment of its effectiveness and novelty.\n2.\tHow does the computational complexity of MFSC compare to baseline approaches in terms of training time, inference time, and resource requirements?\n3.\tThis paper does not provide sensitivity analyses of MFSC with respect to different hyperparameters, such as the weight of fusion loss and the weight of reconstruction loss.\nReferences\n[1] Hafner et al. Mastering diverse domains through world models. arXiv preprint   arXiv:2301.04104, 2023.\n[2] Seo et al. Masked world models for visual control. CORL, 2023.\n[3] Seo et al. Multi-view masked world models for visual robotic manipulation. ICML, 2023."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper introduces a novel approach named Multi-view Fusion State for Control(MFSC)，which ingrates a self-attention mechanism with bisimulation metric learning to fuse task-relevant representation from multi-view observation. Additionally, the paper also incorporated a mask-based latent reconstruction auxiliary task to learn cross-view information in order to foster more compact fused presentation. In this paper, two major problems were solved : First is Higher data dimensions and more redundant information , and Informative aggregation of representation from various views.",
        "strengths": "1.\tClear statements and good structure. The paper is well-structured, and viewpoints was stated logically. The introduction provides a good overview of the challenges in the multi-view representation learning task and approach to address them relatively.  Also illustrate provided along with methods made it easy and vivid.\n2.\tSufficient and solid proof in major conclusions. Problems were clearly defined and followed by mathematical formulations with clear explanation and ended with a solution with validate experiments. \n3.\tComprehensive experiment and supportive solution ,also contributions made by this method were shown vividly and clearly through several comparative illustrate shown in the part of Experiments.  \n4.\tReproductive experiment with project code and data shared.  Experiments  result can be verified personally by readers with resources provided in this paper.",
        "weaknesses": "•\tA few formula faults are discovered in the paper.\n•\tEvaluation Metrics: The evaluation metrics used in the experiments could be more comprehensive. Currently, the focus appears to be on task performance, but including metrics that assess representation quality (e.g., reconstruction loss) would provide a fuller picture of the model’s effectiveness.\n•\tGeneralization to Other Tasks: The experiments are primarily conducted on Meta-World. To evaluate the generality of the approach, the authors should consider applying MFSC to other control tasks or environments. This would help demonstrate the versatility and broader applicability of the proposed method.\n•\tLimitations Discussion: The paper should include a dedicated section discussing the limitations of the proposed method. Identifying potential weaknesses and suggesting avenues for future work would add depth to the contribution."
      }
    ],
    "rating_avg": 4.5,
    "confidence_avg": 3.5,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "p7vItQ3OfD",
    "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
    "authors": [
      "Qizhang Feng",
      "Siva Rajesh Kasa",
      "SANTHOSH KUMAR KASA",
      "Hyokun Yun",
      "Choon Hui Teo",
      "Sravan Babu Bodapati"
    ],
    "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.",
    "keywords": [
      "privacy",
      "LLM alignment",
      "rlhf"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=p7vItQ3OfD",
    "forum_url": "https://openreview.net/forum?id=p7vItQ3OfD",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "WxLwXyBJLw",
    "title": "Flow Matching for One-Step Sampling",
    "authors": [
      "Svetlana Pavlova",
      "Ivan Oseledets",
      "Gleb Ryzhakov"
    ],
    "abstract": "Flow-based generative models have rapidly advanced as a method for mapping simple distributions to complex ones for which the distribution function is unknown. By leveraging continuous-time stochastic processes, these models offer a powerful framework for density estimation, i.e. an algorithm that samples new points based only on existing samples. However, their requirement of solving ordinary differential equations (ODEs) during sampling process incurs substantial computational costs, particularly for large amount of data and numerous time points. This paper proposes a novel solution, which is based on a theoretical analysis of Flow Matching (FM), to overcome this bottleneck, namely, we developed an algorithm to find the point prototype for a given point from the target distribution. By eliminating the need for ODE solvers, our method significantly accelerates sampling while preserving model performance. Numerical experiments validate the proposed approach, demonstrating its efficiency.",
    "keywords": [
      "Flow Matching",
      "Generative Models",
      "Ordinary Differential Equations",
      "One-step generation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=WxLwXyBJLw",
    "forum_url": "https://openreview.net/forum?id=WxLwXyBJLw",
    "reviews": [
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper considers continuous normalizing flows and proposes an algorithm for finding the corresponding point in the base distribution (called its prototype) for a given point from the target distribution, without having to solve the defining ODE. For each sampled target point, the explicit velocity function is followed backwards to find a suitable base distribution point. Using the set of point pairs as a training set, a model is trained on them; that is, the generation can take place in a single step, and the network does not need to be invertible.",
        "strengths": "The single step sampling results in a greatly improved speed without significantly affecting the model performance.",
        "weaknesses": "Language: there are many spelling and grammatical errors, and the sentences are sometimes incomprehensible.\nEspecially in the introduction, the main ideas are hard to comprehend, especially if one does not know the previous work in detail.\nThe experiment in Section 4.2 should be explained in more detail or a reference needs to be added if it is a common method for image colorization.\nLine 114: there is no \"Introduction and Related Work\" section, these are two separate sections in the paper."
      },
      {
        "rating": "3",
        "confidence": "2",
        "summary": "This paper presents a flow-based generative modelling approach that does not require solving an ODE for inference. The method seems to rely heavily on the \"exact velocity\" from (Ryzhakov et al., 2024); however, both the motivation and details of this need more development. Moreover, more comparison (especially numerical results) to previous works is needed. E.g. other approaches that bypass inference integration, like consistancy models, are not mentioned in the related work.",
        "strengths": "1. Adresses the problem of long inference time by bypassing the inference ODE integration.",
        "weaknesses": "1. More elaboration on the method of Ryzhakov would be helpful.\n2. Unclear about motivation and comparison to related works.\n3. Lack of numerical results."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "This paper proposes a one-step sampling method based on flow matching. Given a set of training samples from an unknown distribution $\\rho_1$, a mapping is learned from a simpler known distribution $\\rho_0$, typically a normal distribution, to $\\rho_1$.  Using a discretized estimate of the flow, derived in (Ryzhakov et al. 2024), so-called prototypes in $\\rho_0$ are found by solving an ODE for each training sample in $\\rho_1$, under the assumption of linear flow. A network is then trained to directly map from $\\rho_0$ to $\\rho_1$, using the set of training samples paired with their respective prototypes.",
        "strengths": "The most important benefit of the proposed sampling approach is its speed, since instead of solving an ODE, sampling is done in one step using a neural network.",
        "weaknesses": "The experiments described in the paper are extremely limited. It is shown that 8 Gaussians can be generated using the proposed sampling method. It is also shown, with four example images, how colors can be transferred from one distribution to another, with target colors given by a separate image. Furthermore, in the paper, the sampling method was not compared to any other alternative method or ablated versions of the same method. For a publication to be recommended, the experiments should be expanded to include comparisons against relevant baselines. The text describing the color transfer experiments should preferably also be rewritten since the current version is too unclear.\n\nIt is rather unclear what a prototype is, whether it is the point in $\\rho_0$ one would converge to if there are no errors in ODE solver, or whether a prototype can be any point in $\\rho_1$ that you happen to converge to. Something that would have been interesting to know is how precise the prototypes truly are, and what effect the errors have on the end result. It is claimed that the quality of the prototypes is sufficient for images to be predicted, but this is a question that ought to be studied in greater depth. \n\nWhen prototypes are found in $\\rho_0$, noise is first added to the position in $\\rho_1$, but the motivation for this is hardly satisfactory. In almost a full page the paper tries to argue why the added noise is necessary. It seems that the implementation of the Runga-Kutta method relies on a particular set tolerance parameter. If the tolerance is set too high, the method will stop early, and points will never reach $\\rho_0$. However, if normal distributed noise is added in $\\rho_1$, at least the points end up being spread like a normal distribution, but there is no convincing argument that the errors would then be smaller.\n\nThe language of the paper is unfortunately rather problematic with numerous missing articles (the, a), improper prepositions, and awkward sentences that are hard to interpret. However, the problems are not too severe for the material to be understood and should be relatively easy to correct in a final version of the paper.\n\nThe two algorithms are very similar to each other. The only difference seems to be that if you have a discrete label, a separate mapping is learned for each value of the label. It would have been sufficient to just keep Algorithm 2.\n\nThe notation for buffer B varies in different parts of the text."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper presents a new flow matching approach for efficient sampling. The method finds an approximate prototype mapping by using importance sampling and the soft-max distribution. The prototype is finally found by solving a Cauchy problem. The prototypes and the connected target samples are used to define the  velocity field of the flow. The algorithm is evaluated on a 2D GMM task and a color transfer task.",
        "strengths": "- the new algorithm could potentially provide more computationally efficient generative models\n- the presented algorithm seems to be reasonable, even though a few theoretical justifications are unclear",
        "weaknesses": "(1) there are no comparisons to other flow matching approaches or generative models (diffusion, consistency models) provided\n(2) The evaluations are only done on a simplistic 2D dataset and the color transfer task, where I have difficulty assessing how good the result really is\n(3) No evaluation and comparison of the computational efficiency of the generative model is offered (which was the main motivation of the approach). Its unclear to me how the single-step sampling here actually works? We still have the solve the ODE, don't we?\n(4) Theory is also quite hand-wavy. For example, it is unclear to me how importance sampling is used to estimate the integral of Eq (1). Which distributions are replaced here ? I.e., what is the sampling distribution?\n(5) It is unclear to me how tolerance and sigma interact and why we need sigma. From the presented theory, sigma would not be needed, would it? But without sigma, only very poor prototypes are learned, so this seems to be unsatisfactory to me"
      }
    ],
    "rating_avg": 3.25,
    "confidence_avg": 2.75,
    "decision": "Reject",
    "meta_review": "This work proposes a sampling technique for Flow Matching, a recently emerging approach for generative modeling. Its main novelty is a sampling algorithm that, unlike in previous work, does not require solving an ordinary differential equation. It is based on a model training algorithm that uses exact expressions for the vector field in flow matching and training on samples from the original and target distributions simultaneously. One weakness of this work is its lack of clarity. For example, while some of the terminology may be standard in a subfield, the work may have also needed more background to be broadly accessible to the NeurIPS community.  Another concern raised by the reviewers is the lack of comparisons and a very limited experimental section. While the first weakness was slightly mitigated in the rebuttal process, the work still remains somewhat difficult to access, and the reviewers remained concerned about experimentation and comparisons, which is why I recommend rejection.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "FDimWzmcWn",
    "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
    "authors": [
      "Dayuan Fu",
      "Keqing He",
      "Yejie Wang",
      "Wentao Hong",
      "Zhuoma GongQue",
      "Weihao Zeng",
      "Wei Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Weiran Xu"
    ],
    "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
    "keywords": [
      "agent",
      "self-refine",
      "diversity",
      "generalization",
      "data synthesis"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=FDimWzmcWn",
    "forum_url": "https://openreview.net/forum?id=FDimWzmcWn",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper proposes a novel framework to improve the generalization capabilities of LLMs based agents. The authors identify that existing agent-tuning methods often overfit to specific environments and fail to generalize to new tasks. To address this, the paper introduces AgentRefine, which leverages a agent synthesis framework to encompass a diverse array of environments and tasks drawing upon extensive human persona data, enabling the model to learn from its mistakes through a process of refinement tuning. The experiments demonstrate that AgentRefine method outperforms state-of-the-art methods in terms of generalization, robustness to perturbations, and the ability to generate diverse thoughts during inference.",
        "strengths": "The proposed method's idea seems like meta learning, which trains the policy on diverse tasks for quickly adapting to novel tasks. This idea makes sense to me and seems new in agent domain. \n\nI appreciate authors' rethinking on the generalization of agent-tuning. The issue of memorizing trajectory leading to overfitting seems valid to me.\n\nThe experiment evaluates the performance of AgentRefine from wide range of perspectives.\nThe findings establish a correlation between agent generalization and multi-task agent training mechanism / self-refinement, providing a new paradigm for future research in agent-tuning.",
        "weaknesses": "Overall AgentRefine is a simple and effective method. However, the main idea is not new, as discussed in related work, Agent-FLAN and AgentGen have proposed to train generalist agents using general data. The idea of refinement is also widely studied as discussed in introduction. I encourage authors to clearly differentiate AgentRefine from these prior works. Highlight unique aspects or improvements over existing methods. Consider incorporating a comparative analysis to demonstrate the advantages of AgentRefine.\n\nI feel the procedure suffers from a high risk of generating low-diversity tasks, as the script generation is based on human persona data, which is limited in a certain domain. In contrast, a generalist agent is expected to complete any tasks. \n\nThe goal of the proposed method is to build a LLM-based agent to generalize to novel tasks. However, this way to generate agent tasks does not bring new knowledge to LLMs, but enabling the LLMs to follow the output format more strictly, as it trains LLMs on the data generated by LLMs themselves. \n\nBesides, the source of performance improvement is not clear. For instance, why the LLM-generated trajectories can improve performance on novel tasks? Authors can provide some examples of the evaluation tasks, and examples of the generated tasks."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper presents a framework aimed at improving the generalization capabilities of Large Language Model (LLM) based agents through instruction tuning. The authors observe that existing agent training methods overfit to specific environments and struggle with new situations, leading to poor generalization. To address this, they propose AgentRefine, which incorporates self-refinement processes to enable the model to learn from its mistakes and adapt to diverse environments and tasks.",
        "strengths": "1. The paper is well-organized and easy to follow, with a clear progression from motivation to methodology.\n2. The identification of the generalization gap in existing LLM-based agents and the proposal of a self-refinement approach to address it is a rational step forward in the field.",
        "weaknesses": "1. The problem of generalization in LLM-based agents has been extensively discussed in previous literature, making the contribution of this work less novel. For example,  [1]  investigates the robustness of accuracy measurements in large language models (LLMs) when the order of answer labels is shuffled, using the MMLU dataset as a testbed.\n2. The methodology, while intuitive, lacks significant innovation, as the approach of enhancing generalization through data synthesis is not new [2].\n3. The experimental results do not demonstrate a strong improvement over existing methods, which questions the practical impact of the proposed approach. An apple-to-apples comparison of your main results to show the advantage of the algorithm would make your results more straightforward and strong, instead of using a lot of underlined text to filter out the results where training data is sampled in the same environment as the task.\n\n[1] Changing Answer Order Can Decrease MMLU Accuracy.\n\n[2] Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper discusses using synthetic data to improve the generalization ability of agents on held-out sets. Previous agent-tuning work often chose to construct agent-tuning data on held-in sets. The authors demonstrate that although these methods can greatly improve the performance of agents on held-in sets, they usually lead to overfitting, which in turn affects the performance of agents on held-out sets. Based on this observation, the authors propose AgentRefine. This method does not use task-related information at all. Instead, it uses LLM to complete the entire data generation process, including task generation, trajectory generation, and verification to construct the agent-tuning dataset, thus avoiding the possibility of overfitting to held-in sets from the very start. In the constructed dataset, the authors emphasize the ability of the agent to correct errors based on the feedback, which further improves the agent's generalization ability. They validate AgentRefine in multiple scenarios, and the experimental results show that finetuned agents outperform other baselines on held-out sets.",
        "strengths": "1. This paper discusses the generalization ability of agents, which is a very important topic for the community.\n\n2. The authors provide quantitative analysis to explain their insight, which is very convincing.\n\n3. Synthesizing data with almost no task-specific information is a very practical setting, and the improvement of generalization ability in this paper is impressive.",
        "weaknesses": "1. The presentation of this paper should be improved and some grammar mistakes should be fixed.\n\n2. Some important baselines, for example, Reflexion[1], are missing and should be included.\n\n3. They only consider decision-making tasks in their experiments. However, as they claimed on the generalization ability, tasks of different types should also be included, for example, reasoning tasks. \n\n[1] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" NeurIPS, 2023."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper proposes AgentRefine, a framework designed to enhance the generalization capabilities of large language model (LLM)-based agents through a self-refinement process. The core idea is to enable agents to learn from their mistakes by refining their actions based on feedback from the environment. The authors introduce a data generation pipeline that simulates diverse environments and tasks, followed by a refinement tuning process to improve agent robustness and generalization. Experimental results show that AgentRefine outperforms state-of-the-art methods in held-out tasks, demonstrating improved generalization and robustness.",
        "strengths": "1. The introduction of a self-refinement process for agent tuning is a novel contribution to the field. By allowing agents to correct their mistakes based on environmental feedback, the authors propose an interesting alternative to traditional fine-tuning methods.\n2. The use of diverse environments and tasks in data generation helps mitigate overfitting to specific scenarios, which is a common issue in LLM-based agents.\n3. The experiments show that AgentRefine outperforms baselines in held-out tasks, suggesting that the approach has potential for improving generalization.",
        "weaknesses": "1.  The paper relies heavily on GPT-4 for generating both scripts and trajectories. This raises several concerns:\n   - The quality of the generated data depends entirely on GPT-4's ability to detect and correct errors\n   - The method is not truly \"self-refinement\" since it requires external stronger models for error detection and correction\n   - The authors should analyze what happens when using weaker LLMs for data generation and verification\n\n2. The verification process has potential flaws:\n  - It uses LLMs to verify the correctness of scripts and trajectories without human validation\n  - The paper lacks analysis of verification failure cases or error rates\n  - The authors should include human evaluation of the verification process accuracy\n\n3. While the paper shows improved performance, it lacks analysis of whether this is simply distillation from GPT-4 rather than true generalization and how much of the improvement comes from the refinement process versus having access to GPT-4's knowledge\n\n4. The experiments only scale up to 64k examples. Would the computational cost of generating refinement data with GPT-4 makes large-scale training difficult? Also, the authors should analyze the cost-benefit tradeoff of generating more refinement data\n\n5. While the paper shows some robustness analysis, the perturbation experiments are limited to only action descriptions. More diverse types of perturbations should be tested. The analysis should include how different components (script generation, verification, refinement) contribute to robustness"
      }
    ],
    "rating_avg": 5.75,
    "confidence_avg": 3.0,
    "decision": "Accept (Poster)",
    "meta_review": "The paper introduces AgentRefine, a novel framework aimed at enhancing the generalization capabilities of large language model (LLM)-based agents. The approach tackles the overfitting problem prevalent in existing agent-tuning methods by using a data generation pipeline that simulates diverse environments and tasks. \n- The framework avoids task-specific overfitting by synthesizing data with minimal reliance on task-specific information.\n- The method addresses the generalization gap by leveraging diverse environments and tasks, ensuring agents adapt well to held-out scenarios.\n- Experimental results demonstrate that AgentRefine outperforms existing baselines, highlighting its effectiveness.\n\nThe weaknesses are (1)the main idea, while practical, is not significantly novel; (2) the method heavily depends on GPT-4 for script and trajectory generation as well as error verification.\n\nCurrently, I think the strengths outweigh the weaknesses.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "o3V7OuPxu4",
    "title": "StarCraft II Arena: Evaluating LLMs in Strategic Planning, Real-Time Decision Making, and Adaptability",
    "authors": [
      "Wenjie Tang",
      "Yuan Zhou",
      "Erqiang Xu",
      "Keyan Cheng",
      "Minne Li",
      "Zhiyuan Wang"
    ],
    "abstract": "StarCraft II plays an important role in developing AI agents for real-time strategic reasoning due to its complex nature. However, people usually draw conclusions of how competent their agents are according to the level of the built-in agents in StarCraft II which they can win in terms of the final success rate. Little intermediate quantitative information is considered while human-in-the-loop analysis is time inefficient, which results in inadequate reflection of the true strategic reasoning ability. In this work, we propose StarCraft II Arena, a well-designed benchmark for evaluating the strategic planning, real-time decision-making, and adaptability capabilities of large language models (LLMs) agents. We introduce using fine-grained capability metrics, allowing for targeted capture and analysis of specific capability, and further propose a detailed decision trace to enhance the understanding of LLM behavior. We demonstrate the utility of such a benchmark by evaluating several state-of-the-art LLMs in various setups. Our results reveal distinct performances in long-term strategy development, real-time decision-making, and adapting to environmental changes. Such results show that the StarCraft II Arena offers a deeper insight into the decision-making process of LLMs and has the potential to become a challenging and comprehensive benchmark for strategic reasoning.",
    "keywords": [
      "benchmark evaluation",
      "large language model",
      "LLM-based agent",
      "strategic reasoning",
      "real-time decision-making."
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=o3V7OuPxu4",
    "forum_url": "https://openreview.net/forum?id=o3V7OuPxu4",
    "reviews": [
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The paper presents StarCraft II as a benchmark for evaluating reasoning capabilities of LLMs. The paper also presents a set of metrics for evaluating these models, which are based on the domain itself (e.g., amount of resources the play collects). Finally, the paper presents results of several LLMs on this benchmark.",
        "strengths": "The idea of having a challenging benchmark for reasoning with LLMs is interesting and appealing. I also think that computer games can be a good benchmark for this type of evaluation. The choice of a real-time strategy game is particularly good, given the response latency of these systems.",
        "weaknesses": "My main concern with the paper is that it is only half-baked in the sense that the presentation should be improved substantially before being accepted for publication.\n\nMy concerns with presentation range from the low level to the high level.\n\n**Low-level concerns**\n\n- The paper has a dangling sentence in line 241. It seems that the explanation of Table 2 was inadvertently commented out from the paper.\n- It is not clear what Equation 1 is trying to convey. What is its role in the paper? What is the parameter $\\tau$ that is passed as a parameter but not used in the equation?\n- The paragraph starting in line 137 discusses the need for communication between the agents, but this is not discussed in the paper. I can see SCII having this need if agents play in a team, but this doesn't seem to be the setting evaluated in the paper.\n- In line 106, the paper states that early RL algorithms learn through trial and error. Isn’t this the case for current RL algorithms too?\n- Some of the tables are not referred to in the text. For example, I am not sure when I should read Table 3, and I don’t think the paper discusses it at all.\n\n**High-level concerns**\n\nOne of the key points I was looking for in the paper was the input-output system for the benchmark. Is the LLM receiving images or text as input? How is the input defined? How much time do they have to reason? What happens when they timeout? The text mentions that smaller models perform better than larger models in the micromanagement part of the game. Is this simply because they are able to respond quicker? These are some of the key questions the authors would need to address in the paper. This is what the readers will be after when understanding whether SCII is a suitable benchmark for LLM evaluation.\n\nOverall, the paper is only half-baked, as the authors need to fix all these presentation issues and adjust the discussion of their results and benchmark. There is clearly a system running, as the paper also includes tables of results. However, it is not possible to understand how this system works as the description is missing."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper presents StarCraft II Arena as a benchmark for evaluating LLMs in strategic planning and decision-making capabilities. While it introduces fine-grained metrics and decision tracking mechanisms, the paper needs substantial clarification on its novel contributions and experimental methodology.",
        "strengths": "1. Well-structured evaluation framework with fine-grained metrics\n2. Comprehensive testing across multiple LLM models\n3. Interesting decision tracking system for behavior analysis",
        "weaknesses": "1. Insufficient experimental details:\n- Build-in AI difficulty level not specified\n- Race and map selection criteria not documented\n- Limited experimental scope (10 games per model)\n2. Theoretical foundation needs strengthening:\n- Limited discussion of why LLMs are suitable for StarCraft II\n- Insufficient comparison between LLM agents and traditional RL agents\n- Unclear theoretical justification for chosen metrics\n3. Literature positioning:\n- Need more thorough comparison with existing StarCraft II benchmarks\n- Better contextualization of contributions needed\n- Clearer differentiation from existing approaches required"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper introduces a benchmark called Starcraft II Arena, which aims to evaluate the decision-making, planning, and adaptability of large language models (LLMs) within a strategic gaming environment. While traditional benchmarks for Starcraft II assess agents based on a single overall metric—win rates against built-in opponents—the authors argue that a more detailed analysis of LLM performance is necessary. The main contribution of this paper is a methodology designed for an in-depth evaluation of the capabilities of LLM agents.",
        "strengths": "The key idea of this paper, introducing a benchmark for a more in-depth multi-agent evaluation of LLMs, is significant and useful.",
        "weaknesses": "Overall, the paper lacks clarity and depth in describing both the technical implementation and practical contributions.\n\n### **Major comments**\n\n1. Unclear contribution: The paper does not effectively justify why this benchmark must exist as a standalone contribution rather than an addition to existing Starcraft II resources. The contribution seems limited to a collection of scripts and metrics, which could likely be integrated into the existing environment without creating a separate benchmark.\n\n2. Lack of implementation details: Key technical aspects of the implementation are insufficiently described, making it hard to understand the benchmark's novelty and how it's technically realized. Several things are not clear, such as:\n   - Integration: How are LLM agents integrated with StarCraft II? How can users use the benchmark? Does the benchmark use a custom API or an interface for this?\n   - Decision Tracking: How is decision-making tracked and analyzed? While Table 3 provides a decision trajectory, details of how this is analyzed and used are missing.\n   - Computational Requirements: What hardware/software is necessary to run this benchmark effectively? This information is critical for usability but is absent.\n   - Opponents: Are the LLMs evaluated with built-in agents or newly introduced opponents? The fact that agents are evaluated against built-in agents in Starcraft II is mentioned as a limitation, but it is unclear whether the authors change this in their benchmark.\n\n3. Incomplete metric information: The metrics lack context. For instance, while Appendix A.1 outlines the metrics, there are no defined ranges, leaving the reader unsure of how to interpret scores. For example, how should a Real-Time Decision score of 21.12 versus 37.51 in Table 4 be interpreted? Similarly, terms such as “effective” actions in EPM or “collected vespene” are not unexplained, reducing the metrics’ interpretability (how do we know that these are the right metrics to assess decision-making and planning?).\n \n4. Missing benchmark discussion and limitations: A discussion about future development and limitations of the benchmark is missing, which limits the reader's understanding of the benchmark's intended scope and future extensions.\n\n5. Figure 2 indicates a large variance. Why are there no error bars in the tables?\n6. It's important to have the prompt included in the appendix or supplement. Was it possibly in a supplement that I cannot access?\n\n### **Minor comments** (These did not affect my score)\n- Abstract: Lines 016-019 are a bit difficult to understand; consider rephrasing\n- Figure 2: It’s unclear what this Figure is meant to convey, and the Figure lacks labeled y-axes.\n- In Section 4.3, line 367 states \"Definitions and methods for these metrics will be further detailed in the figure 4.3.\" This seems to refer to a table, possibly Table 3, rather than a figure. \n- In Table 3, \"OBSERVERtgreater\" should probably be \"OBSERVER.\"\n- Lines 323 + 350 state that screenshots illustrating decision traces will be provided in the appendix, but these are not included\n- I don't understand what is meant when the authors state that civilization and the other games are not \"strategic and tactical\" in Table 1. Additionally, Werewolf is clearly an imperfect information game. The authors should reconsider this table because I believe many of the entries are inaccurate.\n- Why is the score in Table 4 unnormalized? It's an incomprehensible number as it stands."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "summary: The paper presents StarCraft II Arena as a benchmark for evaluating LLMs in strategic planning and decision-making capabilities. While it introduces fine-grained metrics and decision tracking mechanisms, the work needs substantial clarification regarding its novel contributions, metric selection justification, and experimental methodology.",
        "strengths": "1. Well-structured evaluation framework with proposed fine-grained metrics\n2. Comprehensive testing across multiple LLM models\n3. Detailed decision tracking system for behavior analysis\n4. Clear visualization of experimental results\n5. Systematic approach to evaluating different aspects of LLM capabilities",
        "weaknesses": "1. Metric Selection and Justification:\n- APM/EPM metrics appear borrowed from traditional StarCraft II evaluation without clear justification of their relevance to LLM agents\n- No discussion of how these metrics specifically reflect LLM decision-making capabilities\n- Missing analysis of whether traditional StarCraft II performance metrics are appropriate for language models\n2. Experimental Design Limitations:\n- Build-in AI difficulty level not specified\n- Race and map selection criteria not documented\n- Limited experimental scope (10 games per model)\n- Absence of LLM vs LLM experiments\n- No evaluation against human players\n- Limited testing of open-source models, reducing reproducibility\n3. Theoretical Foundation and Novelty:\n- Limited discussion of why LLMs are suitable for StarCraft II\n- Insufficient comparison between LLM agents and traditional RL agents\n- Evaluation metrics show significant overlap with existing work\n- Need clearer articulation of novel contributions\n- Better contextualization within existing literature required"
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.25,
    "decision": "Reject",
    "meta_review": "This paper proposes StarCraft II Arena as a benchmark for evaluating LLMs' strategic planning and decision-making capabilities, introducing metrics and decision tracking mechanisms. While presenting an interesting direction, the paper suffers from insufficient technical details, unclear differentiation from existing benchmarks, and inadequate experimental methodology.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "4XHyThqt1C",
    "title": "Alternating Optimized Stochastic Vector Quantization in Neural Compression",
    "authors": [
      "Runsen Feng",
      "Weiping Li",
      "Zhibo Chen"
    ],
    "abstract": "In neural compression, vector quantization (VQ) is usually replaced by a differentiable approximation during training for gradient backpropagation. However, prior approximation methods face two main issues: 1) the train-test mismatch between differentiable approximation and actual quantization, and 2) the suboptimal encoder gradients for rate-distortion (RD) optimization. In this paper, we first provide new finds about how approximation methods influence the RD optimization in neural compression, and then propose a new solution based on these finds. Specifically, if a neural compressor is regarded as a source-space VQ, we find that the encoder implicitly determines the quantization boundaries, and the decoder determines the quantization centers.  Suboptimal approximation methods lead to suboptimal gradients for RD optimization of quantization boundaries and centers. Therefore, to address the first issue,  we propose an encode-decoder alternating optimization strategy. The encoder is optimized with differentiable approximation, and the decoder is optimized with actual quantization to avoid the train-test mismatch of quantization centers.  To address the second issue, we propose a sphere-noise based stochastic approximation method. During encoder optimization, VQ is replaced with a uniform sphere noise centered at the input vector. When the input vector is located at the quantization boundary, the encoder gradient is closer to the difference in RD loss between adjacent quantization centers, facilitating better encoder optimization. We name the combination of optimization strategy and approximation method as Alternating Optimized Stochastic Vector Quantization.\nExperimental results on various vector sources and natural images demonstrate the effectiveness of our method.",
    "keywords": [
      "vector quantization",
      "neural compression",
      "image compression"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=4XHyThqt1C",
    "forum_url": "https://openreview.net/forum?id=4XHyThqt1C",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper investigates an improvement to the STE method used to train VQ-based neural compressors. For scalar quantization methods, the uniform additive noise method during training is shown to yield smooth gradients. This is not applicable to VQ-based methods, which so far mostly use STE. This is shown to yield highly non-smooth gradients. The proposed method, for VQ-based models, uses an alternating optimization scheme, combined with stochastic VQ. This is shown to yield smoother gradients than STE. Experimental results demonstrate superiority over STE-based VQ neural compressors.",
        "strengths": "- The problem of train-test mismatch and other issues of STE in VQ-based models is relevant and timely\n- The proposed method appears principled, and solves some of the challenges that are presented\n- The work is overall well-motivated, and easy to follow",
        "weaknesses": "- In sections 1-2, the problem is presented well, i.e., the need to solve some issues brought forth by STE in VQ-based compressors. However, section 3 dedicates a lot of explanation to how it is solved in scalar quantized neural compressors, which, to me, appears less important. In 3.2, I think it would be helpful to directly mention the VQ-STE section, as that is the setting which this paper's proposed method attempts to improve on. The UQ-AUN and UQ-STE can be mentioned briefly and details put in the appendix, as the scalar quantization setting is not the focus of the paper. This would provide more space to explain some of the details of the proposed method in section 4, which I found to be lacking. In addition, Figure 6 could be placed in section 4, and the reader can directly contrast that with Figure 4, and see how the non-smoothness issue is fixed via the proposed method. \n- The experimental results section covers a broad range of sources, both synthetic and real-world, which is helpful. It is shown that the proposed method outperforms VQ-STE in all settings, and the UQ-AUN method provides a frame of reference. However, some baselines are missing. For example, the two methods soft-toward vector quantization (A2) and probabilistic vector quantization (A3) used in the ablation study (lines 509-511) should also be its own baselines with the Balle et al 2018 transforms. This is useful for understanding how the proposed method compares with other methods that don't use STE. Moreover, these baselines are mentioned in the related work but not compared to. \n- In the related work, lines 138-140, it is said that section 3.2 addresses how prior works in VQ-based neural compression yield sub optimality. However, in the VQ setting, only the STE method from VQVAE is addressed. The method from Agustsson et al, 2017, and Zhu et al 2022 are not addressed in section 3.2. It would be helpful to understand how these two methods' gradients look like in the 1-d Gaussian setting. This, combined with a BD-rate comparison in the results section, would help the reader understand how all the methods compare (conceptually and performance-wise), and strengthen the work overall.\n- Furthermore, the experimental results of the proposed method on natural images use a fairly old architecture (which, to my understanding, uses transforms from Balle et al 2018, single-layer vector quantizer, and a discrete entropy model from VQVAE). There are more recent transforms that are higher-performing, such as those from [1], as well as vector quantizer layers, such as those from [2] and [3]. Experiments using these models would be more convincing. The authors say the proposed method cannot be used on more state-of-the-art models such as these. If true, I think that limits the applicability of the proposed method. \n- There are some issues with the references in the related work, in the second paragraph.\n\nReferences:\n\n[1] Cheng, Zhengxue, et al. \"Learned image compression with discretized gaussian mixture likelihoods and attention modules.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[2] El-Nouby, Alaaeldin, et al. \"Image compression with product quantized masked image modeling.\" arXiv preprint arXiv:2212.07372 (2022).\n\n[3] Feng, R., Guo, Z., Li, W., & Chen, Z. (2023). NVTC: Nonlinear vector transform coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6101-6110)."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "In this paper, the authors propose an optimization strategy for vector quantization in neural compression. Since quantization is non-differentiable, they approximate the vector quantization error using noise sampled from a uniform spherical noise distribution. Additionally, they introduce an optimization strategy to effectively minimize the rate-distortion loss function in neural compression. The authors tested their method on simulated data sources and several real-world images, demonstrating that their approach provides better compression efficiency compared to existing vector quantization methods.",
        "strengths": "1. An alternative optimization procedure to optimize the encoder network, the codebook of vector quantization, and the decoder network. This procedure could result in better convergence of the RD loss function.\n2. An approximation of vector quantization using uniform spherical noise centered on the latent vector.\n3. A gradient analysis of the encoder latent with respect to the loss function.\n4. Deriving the correspondence between vector quantization in the latent space and the corresponding quantization in the image space.",
        "weaknesses": "1.The paper is not well-written and is incomplete in several sections. In the related work section, citations are missing, and sentences are incomplete, making it difficult to relate the written content to the prior art. Few of the papers in the reference are repeated.\n\n2. The evaluation of the proposed vector quantization is limited. The authors have only experimented with a low-complexity autoencoder using a single layer. Consequently, the impact of the proposed method on neural compression is limited. The authors should utilize recent state-of-the-art variational autoencoder-based neural image compression methods, such as [1] and [2], and apply the proposed vector quantization to the latent space of these advanced methods. When the encoder and decoder are more powerful, the impact of vector quantization on reducing the bitrate might be lower than what is shown in the paper.\n [1] Cheng et. al, Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules, CVPR 2020\n [2] He et.al, ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding, CVPR 2022.\n\n3. The details of the network architecture are missing from the paper.\n\n4. The alternative optimization strategy is well-established in the vector quantization literature, where the codebook is fixed while optimizing the encoder and decoder. Additionally, in neural compression, some prior works [3] perform fine-tuning of the decoder using the quantized latent \\hat{y}​, showing that optimizing the decoder with the quantized latent improves compression efficiency and reduces the train-test set mismatch. The citations are missing.\n   [3] Zongyu Guo et.al, Soft then Hard: Rethinking the Quantization in Neural Image Compression, ICML 2021\n\n5. The citations to the related work (baseline) are incorrect (e.g., in Table 1), making it difficult to review the paper."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The paper addresses two main issues of vector quantization (VQ) approximation methods in neural compression. The paper proposes encoder-decoder alternating optimization strategy to address the train-test mismatch and stochastic sphere-noise based approximation technique for suboptimal encoder gradients for rate-distortion (R-D) optimization. Experimental results on synthetic sources and natural images demonstrate the effectiveness of the proposed method over previous VQ approximation methods in terms of R-D performance.",
        "strengths": "1. The paper is well written and easy to follow.\n\n2. The proposed stochastic vector quantization for encoder optimization approach is superior to the previous VQ+STE approximation method as well as the UQ+AUN method, as demonstrated in experiments.",
        "weaknesses": "1. The proposed encoder-decoder alternating optimization strategy is of less importance. Recent neural compression methods address the train-test mismatch issue in end-to-end training by adopting mixed quantization. That is using additive uniform noise for learning the entropy model but employing quantized latent when it is passed to the decoder. There is no evidence that the encoder-decoder alternating optimization strategy is better than the mixed quantization method. Moreover, as the authors illustrated, the proposed alternating optimization strategy is only applicable to single-layer quantization and unconditional entropy models, which leads to obviously degraded R-D performance.\n\n2. In the proposed stochastic vector quantization approach, the authors assume $q(\\tilde{y}|y)$ is a uniform sphere distribution centered at $y$. However, there is no theoretical evidence to support that this assumption is reasonable. \n\n3. In experiments:\n\n(1) For low-dimensional vector sources, it is not reasonable for the dimension of the latent-space vector to be the same as that of the source-space vector, as the primary task of the encoder is dimensionality reduction for feature extraction .\n\n(2) The specific structure of the entropy model of VQ-STE and the proposed method is not given. Due to the different entropy models, it is also unfair to compare the proposed method with UQ-AUN and UQ-STE.\n\n(3) The R-D performance of the proposed method is evidently worse than current state-of-the-art methods. It is even worse than BPG444."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes an alternating optimization method that incorporates stochastic quantization to improve the quantization process of nonlinear transform coding (NTC). The paper clearly formulates the optimization problem of NTC from the perspective of vector quantization, *i.e.*, the optimization of boundaries and codewords. Experiments on low-dimensional sources and natural images show that the proposed method outperforms the classical NTC method equipped with additive uniform noise and straight-through estimator on image compression.",
        "strengths": "1. The paper is overall well written and easy to follow.\n2. The authors provide a clear framework for analyzing the gradient approximation problem of NTC and propose a method for solving it based on the characteristics of vector quantization.",
        "weaknesses": "1. The motivations and advantages of employing a uniform sphere distribution are hard to understand. The uniform quantizer with additive uniform noise also approximates the encoder gradient with the difference in RD loss between adjacent quantization centers (which is the main advantage of the uniform sphere distribution), as shown in Eq. (4).\n\n   By the way, I noticed that the proposed method uses a learnable multidimensional codebook instead of a fixed codebook of uniform quantizers. However, such a gap can be reduced by the nonlinear transforms (for flexible boundaries and codebook in the source space) and conditional coding (for redundant multidimensional signals).\n\n2. The importance of the proposed method seems to be limited. Vector quantization and conditional coding (*e.g.*, spatial auto-regression [R1] and channel-wise auto-regression [R2]) are two kinds of methods that solve the high-dimensional coding problem of latent representations, and the latter one is more prevalent in existing methods. Theoretically, the proposed alternating method can be used in both vector quantization and conditional coding. However, the authors only offer the results for vector quantization. It is better to evaluate the contribution of the proposed method by integrating it with state-of-the-art conditional coding methods, such as ELIC [R3] and TCM [R4].\n\n   [R1] D. Minnen, J. Ballé, and G. D. Toderici. Joint autoregressive and hierarchical priors for learned image compression, In *Advances in Neural Information Processing Systems (NeurIPS) 31*, 2018, pp. 10771-10780.\n\n   [R2] D. Minnen and S. Singh. Channel-wise autoregressive entropy models for learned image compression. In *2020 IEEE International Conference on Image Processing (ICIP)*, 2020, pp. 3339-3343.\n\n   [R3] D. He, *et al.* ELIC: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022, pp. 5718-5727.\n\n   [R4] J. Liu, H. Sun, and J. Katto. Learned image compression with mixed transformer-cnn architectures. *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)*, 2023, pp. 14388-14397.\n\n3. Contributions on interpreting neural compression as vector quantization should be clarified. There has been work (Ballé *et al.*, 2020) that reveals the relationship between the source domain and the latent representation. Although this paper is cited by the authors in their related work, the relationship and contributions of the two papers are not clarified.\n\n4. Several details should be clarified in the manuscript to ensure that the paper is self-contained.\n\n   - The implementation of vector quantization in the latent space, which is crucial to better understand the contribution of the proposed method.\n\n   - The definition on the uniform sphere distribution.\n\n     I note that there are two different definitions of hypersphere, with a difference in whether the points with a distance less than the radius are considered part of the hypersphere. It is suggested that the authors provide a clear definition.\n\n     (Additional) 2 definitions, with the latter one be the case of this paper:\n\n     a) The $(k-1)$-sphere with a radius $R$ is the set of points $[x_1, x_2, \\cdots, x_k]$ with $\\sum_{i=1}^kx_i^2 = R^2$.\n\n     b) The $k$-dimensional hypersphere with a radius $R$ is the set of points $[x_1, x_2, \\cdots, x_k]$ with $\\sum_{i=1}^kx_i^2\\leqslant R^2$.\n\n5. Typos:\n\n   - There are several omitted citations in the second paragraph of Section 2.\n\n   - There is a redundant comma after “e.g.,” in Line 99.\n\n   - The references are not cited with proper commands. Some of the citations need to be replaced by `\\citep` instead of `\\citet`.\n\n   - There is an unnecessary bracket after $\\mathbf{\\mathit{y}}$ in Line 353."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 4.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "yRd4loGAhJ",
    "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
    "authors": [
      "Changhun Lee",
      "Jun-gyu Jin",
      "YoungHyun Cho",
      "Eunhyeok Park"
    ],
    "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has a unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores. Built on this insight, we propose a learning-based mechanism using zero-shot generated data to emphasize these heads, improving the model's performance in long-context retrieval tasks. \nBy applying SEAL, we can achieve significant improvements in in-domain retrieval performance, including document QA tasks from LongBench, and considerable improvements in out-of-domain cases.\nAdditionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.",
    "keywords": [
      "large language models",
      "long context",
      "retrieval",
      "attention",
      "supervised fine-tuning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=yRd4loGAhJ",
    "forum_url": "https://openreview.net/forum?id=yRd4loGAhJ",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This work focuses on scaling to emphasize attention to long-context retrieval, designed to enhance the retrieval performance of LLMs in handling extended contexts. A cost-effective, learning-based mechanism is proposed to improve the model's performance in long-context retrieval tasks, which emphasizes specific attention heads tailored to retrieval tasks. Experimental results demonstrate superior performance over the compared baselines.",
        "strengths": "1. This paper is well-organized and easy to read. \n2. The proposed method presents a reasonable approach for long-context retrieval by identifying the key components of Transformer architecture to boost retrieval performance. \n3. The approach is practical and has the potential for broad application in various RAG settings.",
        "weaknesses": "1. The term \"cost-efficient\" is not clearly defined, resulting in ambiguity when assessing the cost-effectiveness of the approach. The strategy of identifying key components initially and subsequently fine-tuning these components may prove to be computationally intensive. It would be beneficial to provide details regarding the computational time involved in this process.\n2. A more thorough evaluation would benefit from comparisons with a broader range of advanced baseline models. Currently, the proposed method is compared against only one simple. Including more sophisticated long-context modeling methods and state-of-the-art techniques would better validate the effectiveness of the proposed method.\n3. To confirm the versatility of the proposed method, it would be beneficial to conduct experiments on different LLMs of varying sizes."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper introduces SEAL (Scaling to Emphasize Attention for Long-context retrieval), a novel attention scaling approach that improves retrieval performance for long-context tasks in Large Language Models (LLMs). It addresses the challenge of performance degradation over extended contexts, particularly in retrieval tasks. SEAL fine-tunes specific attention heads or channels using a minimal amount of training data, leading to significant improvements in long-context retrieval across various benchmarks. The paper focuses on cost-efficient enhancement of long-context capabilities without altering the model’s learned behavior.",
        "strengths": "1. SEAL presents an innovative approach by leveraging attention head/channel scaling to enhance long-context retrieval.\n2. The method uses very few trainable parameters and requires minimal training data, making it highly efficient.",
        "weaknesses": "1. The term “long-context retrieval” is ambiguous. It would be clearer to refer to “retrieval tasks that have long contexts,” which directly emphasizes tasks like passage retrieval or number retrieval.\n2. The paper lacks explicit detail about which context extension techniques are used. For example, Figure 6 mentions the use of Self-Extend, but no experiments isolating its performance are provided.\n3. Logical Flow in Writing: Certain parts of the paper are difficult to follow due to writing issues such as ambiguous expressions, inconsistent time tense, and occasional typographical errors (e.g., “biases” instead of “bias”).\n4. The distinction between “in-domain” and “out-of-domain” in the experiments is confusing. Specifically, if “in-domain” refers to training on retrieval tasks, why are the same datasets used for both “in-domain” and “out-of-domain” experiments?"
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes an approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which emphasizes specific heads or channels (attention outputs) particularly related to long-context retrieval by efficiently adjusting the strength of each attention component. The authors claimed that SEAL achieves significant improvements in in-domain retrieval performance and cross-domain document QA tasks, also extends the context limits of LLMs while maintaining highly reliable outputs.",
        "strengths": "1. This paper proposes SEAL to efficiently adjusting the strength of each attention component, and achieves superior performance to various LLM baselines in long-context retrieval.\n2. The content, figures, and tables of the paper provide a detailed explanation and analysis of the motivation, methods, and experiments, facilitating the readers' understanding.",
        "weaknesses": "1. The experimental results in Table 1 show that SEAL-H and SEAL-C require fewer parameters than Baseline and SEAL-L. However, their performance does not consistently surpass SEAL-L in long-context scenarios, failing to demonstrate the authors' claims.\n2. The experiments only select SEAL-L as the baseline, it should include other PEFT methods for comparison."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper proposes a novel and practical method, SEAL, to improve the long-context retrieval ability of LLMs. \n\nFirst, through perturbation experiments, it finds a certain attention head or a certain channel in it can cause a positive or negative effect on long-context retrieval accuracy.\n\nSecond, it demonstrates directly scaling the hidden states of these heads or channels can indeed improve the retrieval accuracy of LLMs. \n\nThird, it adds trainable scale factors into the model and use a small amount of samples of retrieval tasks to fine-tune the model. The results show SEAL can remarkably improve the long-context retrieval ability of LLMs.",
        "strengths": "1. This paper discovers that a certain attention head can cast a remarkable positive or negative effect on long-context retrieval accuracy, even as well as a certain channel. This is interesting and helpful for us to further understand the role of the internal modules of LLMs.\n\n2. The proposed method, SEAL, is very cost-effective, which only needs very few training samples and tuned parameters.\n\n3. There are enough evaluation results of various models to demonstrate the method’s effect.",
        "weaknesses": "1. Narrow scope\n\nThe method seems to only be applicable for classic retrieval tasks such as NIAH, and the training data is also the same types of tasks. It will not be surprising that this leads to an improvement, since this task has been too simple, fixed and formulaic, which may represent a narrow application scope for this method. It would be better to train and test on more tasks such as Knowledge-QA.\n\n2. No unique advantages\n\nThe author should empirically test whether the time or space required by SEAL is significantly less than that of LoRA. Otherwise it cannot show significant superiority of SEAL compared to LoRA. Because the parameters tuned by LoRA are already very few. Though SEAL can theoretically tune much less parameters, it may not significantly save much time. \n\n3. There is little detailed description about the procedures of the method in the abstract or introduction. This will make it hard for readers hard to grasp the method quickly. There usually should be a paragraph included in the introduction to describe the specific operation of the method.\n\n4. The curve of data points in Figure 4 (a) may be too small, making it hard to clearly see the changes."
      }
    ],
    "rating_avg": 5.25,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "This paper proposes an efficient method, SEAL, which tunes the scales of attention heads and channels for effective long context retrieval. Due to the limitations in the pretraining phase, LLMs are known for deficiencies in handling very long context window. By learning to weigh the strengths of different attention heads and channels using a small set of generated examples, the scaled model is capable of emphasizing the model components that are beneficial for the retrieval task and de-emphasizing those components negatively affecting the retrieval task. Experimental results demonstrate the advantage of SEAL on line retrieval task and LongBench QA task.\n\nStrengths:\n- The proposed scaling method is novel and effective, backed by an interesting observation on a significant performance change when pruning different attention heads or channels.\n- Given the tunable parameters only involve scaler weights for attention heads and channels, the method is efficient and could be easily adopted in different models and real applications.\n- Experimental results on two different datasets demonstrate the advantage of SEAL over the baseline.\n\nWeaknesses:\n- The clarity of the paper writing needs to be improved, such as the description of SEAL-L and SEAL-D, how the method is used in the context extension setting.\n- The implication of SEAL is not clearly revealed from the empirical results. Comparing with normal PEFT methods such as LoRA and DoRA, the performance improvement is not very consistent. If the provided LoRA (SEAL-L) and DoRA (SEAL-D) are also proposed by the authors, additional baselines with normal PEFT training should be given to clearly reveal the difference and advantage of the proposed method.\n- More datasets/tasks could be incorporated to showcase the method's generalizability.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "6ifeGfWxtX",
    "title": "Slashed Normal: Parameterize Normal Posterior Distributions with KL Amplitude",
    "authors": [
      "Yujia Yan",
      "Xingjian Du",
      "Zhiyao Duan"
    ],
    "abstract": "We present Slashed Normal, a novel parameterization for the normal posterior\ndistribution in variational-inference-based latent variable models. Slashed Normal\ntakes a simple form resembling conventional practice, but uses the new stdplus\nactivation function to derive the standard deviation instead of softplus or exp. Although taking this simple form, the Slashed Normal establishes a direct connection between the squared l2-norm of the raw neural network output, termed KL amplitude, and the exact KL divergence value between the prior and the posterior. As a result, this parameterization enables a direct control of the KL divergence value, which is usually interpreted as the rate from the rate-distortion perspective for variational\nautoencoders. We demonstrate the versatility of Slashed Normal through theoretical analysis and experiments, showcasing its ability to provide good insight about the posterior distribution, explicit control over the KL divergence, and mitigate\nposterior collapse.",
    "keywords": [
      "Variational Inference",
      "Kullback-Leibler Divergence",
      "Posterior Parameterization",
      "Variational Autoencoders",
      "Variational Information Bottleneck"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=6ifeGfWxtX",
    "forum_url": "https://openreview.net/forum?id=6ifeGfWxtX",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper proposes a new parameterization of Gaussian variational distributions when using variational inference (VI) in probabilistic models with Gaussian priors (the discussion and empirical evaluation focuses specifically on _amortized_ VI, i.e., variational autoencoders and variational information bottleneck). The KL-divergence from a Gaussian prior to a Gaussian variational distribution can be written as a sum $a^2 + b^2$ where $a$ depends only on the mean and $b$ depends only on the variance of the variational distribution. The authors propose to parameterize the variational distribution by $a$ and $b$ (rather than by, e.g., its mean and variance, or by its natural parameters). Solving for $a$ and $b$ results in $a$ being the shift between prior to variational mean, measured in units of the prior standard deviation, while $b$ is a more complicated function of the fraction between prior and variational standard deviation.\n\nThe paper claims that the proposed parameterization, in which the KL-term in the ELBO (the \"rate\") takes the simple form $a^2 + b^2$, allows for easier control of the rate and helps mitigating posterior collapse.",
        "strengths": "- The paper addresses a relevant problem that might sometimes be overlooked as a technical detail.\n- It discusses important consequences such as adversarial robustness and posterior collapse of the proposed method.\n- I think a streamlined derivation could motivate the proposed parameterization in a very straight-forward way whose simplicity would warrant exploring it in practical applications even if there may be limited strict theoretical guarantees.",
        "weaknesses": "While the studied problems are important, the derivations seem to be correct, and there are some (limited) empirical results, I find the paper lacking both in content and in presentation.\n\n## Content\n\nThe paper proposes a very simple (see \"presentation\" below) parameterization of the variational distribution in a specific model class.\nIn my experience, it is common when implementing probabilistic models that one thinks a bit about reasonable parameterizations of the probability distributions that avoid exploding gradients and that allow for easy regularization, initialization, and/or plotting of desired quantities.\nSuch considerations often make it into the appendix of a publication, where one describes details of the model implementation.\nFor such considerations to be noteworthy enough to merit a dedicated paper, in my opinion, they have to (i) apply to a general class of problems and (ii) be thoroughly evaluated empirically across a wide range of models to make sure that the improvements on a particular model are not an artifact of, e.g., the inevitably different initialization that comes with every reparameterization.\nI find the paper to be lacking in both (i) and (ii).\n\n**Regarding generality (i),** the proposal is limited to models with a Gaussian prior and Gaussian variational distribution.\n- While this simple setup is admittedly often used in practice, the paper seems to restrict the discussion and evaluation even further to *fixed* priors.\n  However, it seems to me that a good parameterization of a variational distribution would be of particularly interest in models with learned priors (which appear naturally in hierarchical VAEs [1-3], and also in applications of VAEs to data compression [4]).\n  I would find it an interesting question whether a parameterization that is relative to the prior is beneficial or detrimental to optimization speed when the prior itself changes during training.\n- Beyond learned priors, the idea of parameterizing the variational distribution in such a way that the rate term takes a simple form seems quite general to me, and it seems like this concept should, in some form, also be applicable to other distributions than Gaussians.\n\n**Regarding empirical evaluations (ii),** I find the experiments somewhat limited, but this may in parts be because I did not fully understand what the baselines are.\n- From the discussion, it is unclear to me whether baselines include a thorough comparison to standard $\\beta$-VAEs.\n  The discussion seems to suggest that the proposed family of renormalization methods do not need a tuning parameter (akin to $\\beta$) because the target rate can be set directly.\n  But of course, the target rate $r$ then takes the role of a tuning parameter.\n  For a full comparison, I would have expected some rate/performance plot, where performance can be any of the evaluated performance metrics (e.g., adversarial robustness or NLL), and the rate is always _measured_ by the standard KL-divergence and just _controlled_ differently (either explicitly by $r$ or implicitly by $\\beta$).\n- Point 4 in Section 6.2 suggests that the proposed method makes it easier to control the KL-term even when its value is trained.\n  However, it seems like model performance (e.g., number of active units) depends strongly on the initialization of $\\delta$.\n  Since the final value of the KL-term differs strongly from the initialization (see Table 2), it actually seems to me that the KL-divergence is quite hard to control in this setup.\n  We usually try to find setups where final model performance does _not_ depend strongly on initialization, since the effect of different initializations on final model performance is indirect and depends in complicated ways on learning rates and the number of training iterations.\n  I would imagine that it would have been much easier to control the KL-divergence had we just used a traditional parameterization of $q$ and added a simple regularization term $\\propto (D_\\text{KL} - \\delta)^2$ to the training objective (where $\\delta$ is the target rate).\n\nI would find the limited empirical evaluation less concerning if there was clear theoretical evidence of its benefits.\nHowever, I find the theoretical arguments somewhat vague.\nFor example, in the paragraph below Eq. 19, the paper highlights that the KL-divergence takes a very simple form in the proposed parameterization, claiming that \"this formulation eliminates all potentially unstable operations, e.g., log/exp\".\nBut first, other parameterizations that are common in practice avoid this too (e.g., parameterizing the variance by a softplus function).\nAnd second, and more importantly, the claim in the paper ignores the fact that the proposed parameterization just shoves the complexity (and potential instability?) from the KL-term into the reconstruction term.\n\n## Presentation\n\nMy main concern with the presentation is that the paper seems to overstate complexity at many points.\nThis is not a criticism of the simplicity of the proposal—simplicity is a good thing.\nBut, at several places, the paper makes simple (and sometimes even trivial) points seem unnecessarily complicated.\nExamples include:\n\n- Most importantly, a lot of space of the paper is used to derive the proposed parameterization, making it appear like this is a complicated invention that takes a lot of insight.\n  I think this complexity is artificial since the result almost falls out immediately from the expression for the KL-divergence between two normal distributions (Eq. 3).\n  The KL-divergence is a sum of a term that only involves the variational mean $\\mu$ and a term that only involves the variational standard deviation $\\sigma$.\n  Why not just define these two terms as $a^2$ and $b^2$, respectively, and then solve for $\\mu(a)$ and $\\sigma(b)$?\n  Here, $\\mu(a)$ is trivial and $\\sigma(b)$ involves a special function that we can't avoid anyway.\n  Instead of such a simple two-line derivation, the paper first proposes a _different_ parameterization in Section 3.1, that (i) seems less well motivated to me than my above simple motivation of the eventually proposed \"$a^2 + b^2$\" parameterization, (ii) is derived in such detail that I found it easier to rederive it myself than to follow every algebraic step in the paper, and, most importantly, (iii) gets discarded at the end of the section anyway.\n- The argument to discard the parameterization of Section 3.1 could have been seen without the lengthy derivation: if the argument is that $\\frac{\\partial\\sigma^2}{\\partial\\delta} \\xrightarrow{\\delta\\to0} \\infty$, then this can be seen simply by observing that $\\frac{\\partial\\sigma^2}{\\partial\\delta}$ = $1 \\big/ \\frac{\\partial\\delta}{\\partial\\sigma^2}$, where $\\left. \\frac{\\partial\\delta}{\\partial\\sigma^2} \\right|_{\\delta=0}=0$ since $\\delta$ is the KL-divergence, so the only place where it is zero is when prior and variational distribution are equal, in which case the derivative w.r.t. $\\sigma^2$ is trivially zero from Eq. 3.\n- For both Theorems 4.1 and 5.1, it seems like an overstatement to me to present these as \"Theorems\". Theorem 4.1 is a well-known information-theoretical bound, and Theorem 5.1 just states that $\\nabla_x (f(x) + x^2) = 0 \\Longleftrightarrow x = -\\frac{1}{2} \\nabla_x f(x)$.\n\n## Minor Point / Potential Typo\n\n- Line 522: \"Batch Learneable Rate\" --> \"Decoupled Learneable Rate\"?\n\n## References\n\n- [1] [Vahdat and Kautz, NVAE: A Deep Hierarchical Variational Autoencoder, NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html)\n- [2] [Child, Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images, ICLR 2021](https://openreview.net/forum?id=RLRXCV6DbEJ)\n- [3] [Xiao and Bamler, Trading Information between Latents in Hierarchical Variational Autoencoders, ICLR 2023](https://openreview.net/forum?id=eWtMdr6yCmL)\n- [4] [Ballé et al., End-to-end Optimized Image Compression, ICLR 2017](https://openreview.net/forum?id=rJxdQ3jeg)"
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "The paper proposed a Slashed Normal prior that parametrizes the KL divergence term in VAE as the form of a $L^2$-norm. It enables direct control of the KL divergence. Theoretical and experimental results show that the proposed approach is able to mitigate the issue of  posterior collapse.",
        "strengths": "* The presentation and the logic flow are clear.\n* The proposed method is intuitive. \n* The method derivation is good, with clear math notations and solid theorem prooves.",
        "weaknesses": "* The soundness is a bit questionable. There is no code uploaded.\n* The experimental results are a bit weak. For example, in experiment 1, which is the standard VAE results. There are actually two versions of standard VAE, one is the traditional KL term and the other is the reparametrized KL term. Will the results be significantly different?\n* There are no error bars in both of the experiments. For example, in experiment 2, I can see that the KL terms are significantly different (which is clear and intuitive). But the NLL terms (if that is the reconstruction loss) are roughly the same. Do these results show significant/effective performance differences? Some qualitative comparison will be better.\n* There is no comparison with alternative methods that also mitigate the posterior collapse issue. For example, https://proceedings.neurips.cc/paper/2017/hash/35464c848f410e55a13bb9d78e7fddd0-Abstract.html, https://proceedings.mlr.press/v161/jerfel21a.html, https://openreview.net/pdf?id=HD5Y7M8Xdk."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces Slashed Normal, a novel parameterization of Gaussian posterior distributions in variational-inference-based latent variable models, particularly focusing on Variational Autoencoders (VAEs). The method replaces traditional activation functions like softplus or exponential with stdplus to derive the standard deviation. By establishing a direct connection between the squared L2-norm of the raw neural network output (termed KL amplitude) and the exact KL divergence between the prior and posterior, the authors aim to provide explicit control over the KL divergence during training. They claim that this approach offers theoretical insights, enhances numerical stability, mitigates posterior collapse, and simplifies the training process.",
        "strengths": "- The method allows explicit manipulation of the KL divergence term by directly linking it to the network's output, potentially aiding in balancing the trade-off between reconstruction and regularization in VAEs.\n- By controlling the KL divergence explicitly, the approach offers a potential solution to posterior collapse, a common issue where the model ignores the latent variables.\n- The reformulation of the VAE loss function eliminates unstable operations like log and exp, which may improve numerical stability.",
        "weaknesses": "- The derivation of the Slashed Normal parameterization is convoluted, lacks sufficient explanation, and contains too many abuses of notation. For instance, the transition from Equation (9) to the introduction of complex numbers is abrupt and may confuse readers unfamiliar with the application of complex numbers in this context. The use of the Lambert W function is mentioned but not adequately justified or explained, making it difficult to follow the mathematical reasoning.\n- In Section 2, the authors create confusion by using the term \"posterior\" where they should more accurately refer to the \"approximate posterior.\"\n- The experimental results are minimal and lack depth. In Section 6, while the authors mention outperforming certain baselines, they do not provide comprehensive quantitative comparisons or statistical significance tests. \n- The paper acknowledges existing techniques for controlling KL divergence and mitigating posterior collapse but does not thoroughly compare the proposed method against these alternatives.\n- Despite citing numerical instability as a motivation, the paper does not present empirical evidence demonstrating improved stability during training. The claim that the method \"likely improves the numerical stability of training\" is speculative without supporting experiments.\n- The discussion on interpreting the KL amplitude and its relationship with posterior collapse is superficial. The connection made via Theorem 5.1 is not deeply analyzed, and the practical significance of this relationship is not convincingly established."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a new activation function, \"```stdplus```,\" as a replacement for conventional ```exp``` or ```softplus``` parameterization of the approximate posterior variance in Gaussian VAEs, resulting in a new distribution they call \"Slashed Normal.\" This formulation allows for direct control over the channel capacity or information rate in VAEs and provides a more interpretable trade-off between the rate (KL) and distortion (reconstruction) terms. However, there are critical weaknesses that undermine the paper's contributions.",
        "strengths": "The authors identify several known issues within the VAE literature, such as posterior collapse and numerical instability, and aim to address them through their proposed parameterization approach. While theoretically well-motivated, the paper falls significantly short in providing sufficient empirical evaluation and validation of these claims, as discussed below.",
        "weaknesses": "A major weakness of this paper is the lack of empirical support for its primary claims. Since the main contribution centers on replacing the traditional ```exp``` or ```softplus``` parameterizations with the proposed ```stdplus``` activation, the most crucial empirical evidence should be a comprehensive evaluation of these parameterization choices across various datasets and architectures, with other factors controlled. Instead, the experiments primarily explore the impact of adversarial examples on performance, and their dependence on normalization choices, which seems tangential to the core contribution. The absence of a direct comparison between ```stdplus```, ```exp```, and ```softplus``` raises significant doubts about the practical value of the proposed method.\n\nAdditionally, it is known among practitioners that while ```exp``` is more challenging to train and may require techniques like clamping, it generally yields better performance compared to ```softplus```. This is likely attributed to the \"expansive\" nature of the ```exp``` nonlinearity, contrasted with the \"almost linear\" behavior of softplus, making the latter less expressive. Given the close relationship between the ```stdplus``` and ```softplus``` (Fig. 2b), it raises concerns that ```stdplus``` might underperform compared to ```exp``` in practical settings. Without demonstrating that ```stdplus``` is at least on par with ```exp``` or ```softplus``` in terms of empirical performance, the findings of this paper hold limited practical relevance.\n\nFurther complicating the evaluation, the paper relies on unvalidated assertions of numerical stability improvements. The authors assert (lines 309-311) that their approach \"eliminates all potentially unstable operations, e.g., log/exp, which previously require clipping the range of the input to prevent numerical problems. This property likely improves the numerical stability of training.\" This is indeed a major challenge in training VAEs, particularly in hierarchical settings. However, without an experimental demonstration to substantiate this claim, the impact remains speculative. For a novel parameterization technique, empirical validation of stability is essential, and its absence limits the trust in ```stdplus``` as a robust alternative.\n\nRelated to this, the introduction of the ```stdplus``` function adds significant implementation complexity without sufficient justification in terms of demonstrated performance gains. As presented in Algorithm 1, ```stdplus``` is computationally more complex than a simple ```exp``` or ```softplus``` functional call. The authors need to justify this added complexity with clear, consistent performance improvements across practical applications. Yet, the current manuscript fails to establish this, leaving the reader questioning whether ```stdplus``` offers tangible benefits to warrant its more intricate setup.\n\nWhile the authors acknowledge the need for more extensive empirical comparisons, this does not excuse the lack of rigorous evaluation in the current manuscript. Given the main contribution of the paper is replacing ```exp```/```softplus``` with ```stdplus```, a lack of empirical comparison between these parametrization choices almost seems like an intentionally left-out comparison.\n\nOverall, I am inclined towards rejection. Without sufficient empirical evidence, the theoretical contributions alone are not enough to warrant publication at this venue."
      }
    ],
    "rating_avg": 3.75,
    "confidence_avg": 4.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "STpxO1Siaq",
    "title": "Defend against Jailbreak Attacks via Debate with Partially Perceptive Agents",
    "authors": [
      "Qi Zhou",
      "Tianlin Li",
      "Qing Guo",
      "Dongxia Wang"
    ],
    "abstract": "Recent studies have shown that maliciously injecting or perturbing the input image in Vision Large Language Models (VLMs) can lead to jailbreak attacks, raising significant security concerns. A straightforward defense strategy against such attacks is to crop the input image, thereby disrupting the effectiveness of the injection or perturbation. However, the cropping can significantly distort the semantics of the input image, leading to an adverse impact on the model's output when processing clean input. To mitigate the adverse impact, we propose a defense mechanism against jailbreak attacks based on a multi-agent debate approach. In this method, one agent (“integrated” agent) accesses the full integrated image, while the other (“partial” agent) only accesses cropped/partial images, aiming to avoid the attack while preserving the correct semantics in the output as much as possible. Our key insight is that when an integrated agent debates with a partial agent, if the integrated agent receives clean input, it can successfully persuade the partial agent. Conversely, if the integrated agent is given an attacked input, the partial agent can persuade it to rethink the original output, thereby achieving effective defense against the attack. Empirical experiments have demonstrated that our method provides more effective defense compared to the baseline method, successfully reducing the average attack success rate from 100% to 22%. In more advanced experimental setups, our proposed method can even limit the average attack success rate to 18% (debating with GPT-4o) and 14% (with enhanced perspective).",
    "keywords": [
      "Multi-agent Debate; Defense; Visual Large Language Models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=STpxO1Siaq",
    "forum_url": "https://openreview.net/forum?id=STpxO1Siaq",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper proposes a novel multi-agent debate framework for defending Vision Language Models (VLMs) against jailbreak attacks. The approach employs two types of agents - one with full image access and one with partial access - to engage in structured debates aimed at preventing harmful responses. The method achieves a significant reduction in attack success rate from 100% to 22% while maintaining better response quality than baseline approaches.",
        "strengths": "1. The training-free defense mechanism seems promising since it can be implemented at endpoints.\n    \n2. Comprehensive experimentation with multiple debate strategies like message passing, critical debate, persuasive debate.\n    \n3. The approach achieves a reduction in attack success rate from 100% to 22% while maintaining better response quality than baselines.",
        "weaknesses": "1. Experimental Design Issues:\n    \n\n- Insufficient sample size: only 20 samples per scenario for experiment is not adequate enough\n    \n- Inadequate justification for baseline selection: there is no explanation in the article why those two baseline methods are chose for comparison.\n    \n\n2\\. Evaluation methodology Concerns:\n\n- Unclear indicator function definition: the article didn’t clearly explain what the indicator function actually means\n    \n- No explicit criteria for \"successful\" attacks: there is no explicit definition of what constitutes a successful attack\n    \n- Over-reliance on GPT-4 without human validation: there is no human evaluation to validate GPT4’s assessments, which introduces no clear criteria for quality scoring\n    \n\n3\\. Grammar errors, for example\n\n1. ... other debater's answers -> ... other debaters' answers\n    \n2.  ... can notebly decrease... -> ... can notably decrease...\n    \n3. ... Additionlly ... -> Additionally..."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a multi-agent debate framework focusing on defending against VLM jailbreak attacks. The framework involves 2 LLMs, one receiving the attacked image and one taking the partially observed image. The authors also propose different communication strategies to investigate the corresponding effects. Experiments show that the final conclusion reached by the two agent-debate has a low ASR against typographic attacks.",
        "strengths": "- The paper is well-written and easy to follow.\n- The topic of multi-agent debate to defend VLM attacks is interesting.\n- The experiments are comprehensive and clear.",
        "weaknesses": "- The novelty is limited. The multi-agent debate framework is not new and is well-explored in previous work. The authors directly apply the framework to the defense tasks.\n- The paper does not fully justify the advantage of the proposed framework based on multi-agent debate. For example, to demonstrate the advantage of debating, a simple baseline can be: get multiple initial responses from the two agents in round 1 and directly do majority vote instead of debating. To demonstrate the advantage of multi-round, the author should include the comparison with: use the moderator to summarize different responses and get a final response in the first round, instead of doing multiple rounds."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper proposes a novel defense mechanism against jailbreak attacks on VLMs using a multi-agent debate approach, where one agent with full image access debates with another agent with partial/cropped image access, reducing the average attack success rate from 100% to 22% while maintaining response quality.",
        "strengths": "It seems novel to me to use multi-agent debate approach to VLM security. And the proposed method achieves significant performance improvements (reducing attack success rate from 100% to 22%) while maintaining good response quality and low refusal rates.",
        "weaknesses": "1. The paper lacks comprehensiveness in terms of attacks and defenses. Stronger attacks such as white-box attacks aren't considered. Common defenses such as refusal training aren't included.\n\n2. Lacks computational cost comparisons between the proposed method and the baselines in the paper.\n\n3. The claim of maintaining \"quality of responses\" needs more rigorous evaluation (on capability benchmarks) - the quality scoring method (0-5 scale using GPT-4) lacks detailed explanation."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a method against jailbreak attacks on Vision Large Language Models (VLMs) that utilizes a multi-agent debate framework. The approach involves two agents: one that processes the full image and another that handles cropped images. Through debate, the agents cross-validate their outputs, allowing for effective defense—clean inputs lead to the correct output, while attacked inputs prompt re-evaluation.",
        "strengths": "The strengths of this paper include its clear and well-organized writing, making the explanation of the proposed method easy to follow. The authors provide a thorough introduction to the approach, enhancing its accessibility. Additionally, they conduct a comprehensive statistical analysis across different types of topics, which effectively demonstrates the performance of the method.",
        "weaknesses": "The weaknesses include: \n\n- The paper claims that their method can successfully reduce the average attack success rate from 100% to 22%. However, there is no other mention of a 100% attack success rate in the main content.\n\n- The authors compare agents with full image access and those with partial image access. At the very least, agents without image input could be a valid comparison, as the MM-SafetyBench dataset itself does not require models to analyze images. This approach is entirely feasible.\n\n- The proposed method, which relies on debate, is resource-intensive and requires significant computational time, making it less practical. However, the paper does not discuss the efficiency of the proposed approach. Beyond defense success rates, it is unclear whether the method could lead to overly cautious responses that might affect user experience or whether it could impact the usability of the model in other multimodal tasks.\n\n- The baselines used in the paper are overly simplistic, as they only compare the proposed method with MLLM Protector and SmoothVLM. Other relevant approaches, such as prompt-based methods, self-evaluation, self-defense, self-reminder, input perturbation techniques like query rewriting, and fine-tuning-based methods, were not included in the comparison.\n\n- The authors' evaluation approach is relatively simple, relying solely on GPT-4 for assessment, which may introduce bias. The evaluation should include results from other methods or demonstrate consistency with human evaluation to provide a more balanced and robust assessment."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.75,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "uOnElfFuey",
    "title": "Recovering Knowledge by Hardening Language Models",
    "authors": [
      "Haiming Wang",
      "Yimeng Chen",
      "Han Shi",
      "Zhengying Liu",
      "Zhenguo Li"
    ],
    "abstract": "Recent neural language models show impressive capabilities on a wide range of tasks. However, it is not fully understood how the knowledge of the language is encoded in these models. In this work, we focus on the simplest case of languages, regular languages, and study language models trained on strings matching certain regular expressions. We propose a method, dubbed LaMFA, to recover the full knowledge of the regular language model by hardening it into a finite automaton. Such hardening is conducted by empirically partition the latent space of language models into finite states, and then recover a deterministic finite automaton by the estimated transition probabilities between these states. Through experiments on regular languages of varying complexity, we demonstrate that LaMFA can effectively extract DFA that consistently replicate the performance of the original language model. Notably, the extracted DFAs exhibit enhanced generalization capabilities, achieving 100\\% accuracy even in out-of-distribution scenarios",
    "keywords": [
      "regular language",
      "language model",
      "transformers",
      "knowledge interpretation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=uOnElfFuey",
    "forum_url": "https://openreview.net/forum?id=uOnElfFuey",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper studies how language models learn regular languages. It proposes an algorithm LaMFA that can recover the finite state automaton corresponding to the regular language from the neural language model trained on the same language.\n\nLaMFA first constructs the states in DFA by clustering the features generated in language models and then estimates the transition function by looking at strings with and without the last token.\n\nThey found that LaMFA-extracted DFAs can recover the full knowledge of the regular language and perform better than neural language models in out-of-distribution scenarios.",
        "strengths": "1. The method and results are neat, showing that neural language models are capable of learning the underlying DFA of a regular language from the strings of the language.",
        "weaknesses": "1. The findings about \"extracted DFAs exhibiting more generalizability\" kind of contradict the findings in [1], where they showed that a standard transformer often outperforms a transformer that's constrained to be compilable to a RASP program [2].\n2. The results on regular languages are great, but how are they helpful for explaining how and why real-world language models work?\n3. The results from [3] seem more generalizable and (slightly) earlier to me.\n\n### Reference\n\n[1] Learning Transformer Programs, https://openreview.net/forum?id=Pe9WxkN8Ff\n[2] Thinking like transformers, https://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf\n[3] Physics of Language Models: Part 1, Learning Hierarchical Language Structures, https://arxiv.org/abs/2305.13673"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a method for extracting finite-state automata (FSAs) from trained language models, called LaMFA. The approach clusters string prefix representations in the model’s representation space and builds FSAs based on these clusters. The authors add a new denoising step to make clustering more stable and accurate. They apply LaMFA to both transformer and LSTM language models trained on five regular languages and compare the extracted FSAs to the actual FSAs for these languages. They find that the extracted automata are usually not identical to the original ones—they’re often larger—but they tend to generalize better than the models from which they were extracted.",
        "strengths": "This paper contributes to an important area of research: analyzing black-box models using interpretable, easier-to-analyze representations. Key strengths for me include:\n- Exploring language models with interpretable structures like FSAs can lead to better understanding and analysis of these models, which is valuable for the field.\n- The FSA denoising procedure seems new and intuitively useful, making the clustering process more robust.",
        "weaknesses": "There are areas where the paper could be improved:\n\n- **Novelty in Methodology**: Apart from the new denoising step (the benefit of which isn’t explicitly measured), the overall approach seems similar to past work, especially to methods by Weiss et al. (2018), which—contrary to the claims in the paper—also work on *any* language model and come with some theoretical guarantees. The paper would benefit from a closer comparison to Weiss et al., as well as other FSA extraction methods, such as those by Merrill et al (2022) and other authors referenced in the paper.\n- **Related Work Section**: The extended related work in the appendix mostly repeats what is in the main text. Instead, it would be better to replace this with a direct comparison to specific existing FSA techniques.\n- **Limited Empirical Basis**: One main takeaway of the paper is that local context dependency is a better predictor of transformer performance than concepts like circuit complexity. However, since the models are trained on only five very simple languages, this evidence may not be enough to support that claim. Testing on more complex—and many more—languages would strengthen this finding.\n- **Lack of Novelty in Reconciling Generative Models with Recognizers**: The reconciliation of LMs as generative models with FSAs as recognizers is useful, but the approach here is not the first to do so. Weiss et al. (2018), for example, presented an algorithm that addresses similar scenarios.\n- **Valid Rate Measure**: The paper uses “valid rate” to evaluate how many generated strings are syntactically correct, implying this tests language diversity. But as defined, it seems to only measure precision—if a model only learned one syntactically correct string, it could achieve a perfect valid rate. Adding a diversity measure or clarifying this metric could provide a fuller picture."
      },
      {
        "rating": "1",
        "confidence": "5",
        "summary": "**TL;DR:** A deeply misguided, inconsistent, and confused text further burdened by its poor comprehension of the subfield's literature and self-congratulatory overselling.\n\nThe paper presents LaMFA, a method for constructing finite automata from hidden states of language models. The design decisions comprising LaMFA outnumber the automata samples/experimental setups (a grand total of 5) by a factor of 2, thus making the first doubts of the interpretability of the results. The method presented, LaMFA, is an apparent simplification of the Louvain method heavily tailored toward the carefully chosen experimental automata.\n\nSomewhat grandiosely, he paper claims to \"shed light on the internal mechanism of language models\" (L053), \"investigate how linguistic knowledge is encoded and compressed\" (L098), \"draw new insights on the interplay between architectures and language complexity\" (L103), \"yield significant insights into the behavior of LMs\" (L415), and \"mark a significant advancement in model interpretability and generalization\" (L521). None of these claims about the contents of the paper are true, as most of these have no experimental counterpart in the text, and virtually every conclusion about the nature of language models that is drawn can be traced back to a design decision.\n\nAs a brief example of the prevalence of these issues: A claim is made that \"the LaMFA-DFA of LSTM is exactly equivalent to the ground-truth regex\" (L482). However, this is only a consequence of Algorithm 2 merging the previously disjoint cluster states. Okay, a counter-claim can be made that this is the intended function of Algorithm 2. But why then does \"the DFA of GPT-tiny contain an extra state\" (L483) even after the Algorithm 2 merge that was supposed to collapse it? You can't have it both ways; the successful recovery and unsuccessful recovery of a DFA cannot both be findings simultaneously. This would perhaps be acceptable if we had more samples and could draw statistical conclusions, but 5 languages are not nearly enough to do so -- a statistically significant sample should be considered instead and evaluated in an automated fashion (see Weakness 7 for more guidance on this). By the way, the DFA alter-GPT-tiny is incorrect as it also accepts the string \"1\" which is not in the alter language. All in all, the immediately-following claim that \"these observations suggest that larger models may learn more *nuanced* representations of language\" is a deliberate obfuscation of the contradictions arising in L480-L485. The use of *nuanced* is nothing less than a disconcerting, blatant attempt to pass \"it worked once and it did not work once\" for a finding.\n\nWhile I touch on more issues with the paper in the weaknesses, it is important to note that this work is also flawed *conceptually*. In a surprising omission of related work, the text appears to be utterly oblivious to the results of H. Siegelmann from the late 90s giving constructive proofs to the Turing-completeness of RNN-based architectures including GRU/LSTM (rather surprisingly, however, the text goes into great lengths to cite the likes of Kleene, Rabin, Chomsky, or McNaughton&Yamada). Many of the (non-)findings of Sections 5.2.1-5.2.2 are focusing on the artifacts of arguably improper gradient-based training schedule of the selected architectures, as each of the 5 regular languages considered can be realized as RNNs following the construction of the seminal proof. Since both LSTMs (dated work, Siegelmann) and Transformers (contemporary work, Svete) can be constructed by hand to recognize simple finite automata, the insights the paper sets out to provide are already familiar from theory, and its contribution is reduced to a tedious construction and an entirely unconvincing experimentation. See also Weakness 6.\n\nIt is my recommendation that the work is not published. It should be instead revisited from the very beginning, refocused on the contribution of LaMFA, supported by further experimentation vouching for its utility and advantages over the omitted previous work on state merging, and only then submitted for review.",
        "strengths": "None.",
        "weaknesses": "1. The design decisions described in the paper outnumber the experimental data samples by a factor of 2 at the very least.\n2. The paper refers to a considerable amount of obscure and/or dated work with little bearing on its goals.\n3. An unsavory resemblance can be seen between this work and past works of William Merrill.\n4. Early sections of the paper are riddled with typos and grammatical mistakes, especially Section 2 (e.g. \"further developing\" -> \"further development\" L111, \"inspect\" -> \"inspecting\" L113, \"they are only possible to compute formal grammars\" -> this is not a sentence, you need passive L129, \"knowledge recovering\" -> \"knowledge recovery\" L265, etc.).\n5. That this work is \"investigating generative language models\" is a bit of an overstatement as the DFAs recovered by LaMFA are ultimately classifying recognizers rather than generative LMs.\n6. The dependence of Algorithms 1-2 on the number and desired number of clusters (L274,L291) ultimately voids all claims about the complexity of the languages learned by the neural models. By varying the two Ks and given different orderings for the similarities of the clusters, one can arrive at different DFAs. Ultimately, we know that equivalent DFA constructions exist, so any claim on the complexity or imperfection of recovery based on the outputs of the method can be  attributed to the gradient-based training of the neural model as well as the proposed LaMFA method itself.\n7. To give credibility to the claims about the effectiveness of LaMFA, the work should consider a population of automatically generated small DFAs/regular languages, not just 5 handpicked examples. For example, to have 90% confidence that the reconstruction success rate lies between 45 and 55%, at least 273 distinct DFAs should be considered."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The authors propose a method of converting an LSTM or GPT LM trained on a regular language to a deterministic finite automaton (DFA) that approximates the LM. They test their method on 5 regular languages belonging to two different circuit complexity classes (AC0 and TC0). The languages also have different \"context complexity\", in that some only require a local, finite suffix of a string to correctly recognize, and some require global context. They define a regex/DFA for each language and use it to sample datasets of positive examples. They train neural LMs on it from scratch. Afterwards, they extract the hidden representations for each prefix of each string in the dataset. They apply k-means clustering and denoising to convert them to states of a DFA. They find that the extracted DFA often has higher precision than the original LM, and that neural LM accuracy depends mostly on context complexity.",
        "strengths": "This paper proposes a method for extracting DFAs from neural LMs that have only been trained on positive examples and does not require negative sampling. To my knowledge this has not been tested on transformers before.",
        "weaknesses": "1. The number of languages (5) is small, and they are quite simple. It is not clear if this method scales up to more complicated languages where the minimal DFA has more than 3 states.\n1. There are issues with the mdY language. It seems like the authors only allow semantically valid dates (i.e., they do not allow strings like 99/99/9999). This means the actual language tested, which incorporates these constraints, is different from the regex they report in the paper and requires a lot more than 11 DFA states. Also, when splitting examples into training and test data, there are constraints placed on the training data based on the sum of the numbers in the string, and this makes it more difficult to characterize the language actually represented in the training data. It should be noted that this language contains only a finite number of strings.\n1. A technical comparison against prior work on converting neural LMs to DFAs, particularly Weiss et al. (2018), is conspicuously absent. What advantage does your method have vs. theirs? How is it different? The state merging algorithm seems to be very similar to the standard DFA minimization algorithm.\n1. I think the alter language has been misclassified as having local instead of global context dependency.\n1. Some aspects of the method are unclear. In particular, how do you determine when you have converged on the minimal number $k$ of states? I do not see a stopping condition for merging states in the pseudocode. How do you pick the sharpening parameter $T$ and the frequency threshold $\\tau_0$?\n1. There are issues with the evaluation metrics. For one, the accuracy metric only measures precision, not recall. For example, it is possible for an LM to achieve 100% accuracy by assigning all probability mass to a single string in the language. Including cross-entropy does mitigate this because it measures recall, but the authors do not provide what the lower-bound cross-entropy would be under the original DFA LM, so it is not clear when the models are doing well.\n1. The authors claim that context dependency matters more than complexity class, but there is only one language (end0) where these two do not coincide. The paper would benefit from additional languages where they do not coincide. As mentioned above, alter is actually global, not local.\n1. There are grammatical issues throughout the paper which affect its clarity, and it would benefit from a round of proof-reading."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Yd5MHVIKLk",
    "title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion",
    "authors": [
      "Sen Li",
      "Ruochen Wang",
      "Cho-Jui Hsieh",
      "Minhao Cheng",
      "Tianyi Zhou"
    ],
    "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control.\nMuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users.",
    "keywords": [
      "Diffusion models",
      "Controllable generation",
      "multi-modal agent"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Yd5MHVIKLk",
    "forum_url": "https://openreview.net/forum?id=Yd5MHVIKLk",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper presents MuLan, a training-free multimodal language model agent designed to enhance text-to-image (T2I) generation. MuLan addresses challenges in generating images with multiple objects, focusing specifically on controlling spatial relationships, relative sizes, and attribute bindings. By leveraging a large language model (LLM) for planning and a vision-language model (VLM) for feedback, MuLan decomposes complex prompts into sequential subtasks, each handling a single object generation with attention-guided positioning.",
        "strengths": "+ The paper is well-written and easy to follow.\n+ MuLan demonstrates good control over the generation process and produces high-quality images that align with the prompts.\n+ MuLan can be applied to human-agent interaction during the generation process.",
        "weaknesses": "- MuLan increases inference time, especially as the number of objects in a prompt grows, which could limit its scalability in real-time applications.\n- As a training-free approach, MuLan is heavily reliant on the capabilities of underlying base models (such as Stable Diffusion).\n- In some cases, as shown in Figure 2, the generated images exhibit unrealistic proportions. For example, in the first row, the refrigerator, chair, and table are the same size, and in the second row, the pumpkin and door are also similarly sized, which detracts from the realism of the generated scenes.\n- Although qualitative results are emphasized, the absence of metrics such as generation speed or quantitative latency comparisons with baselines makes it difficult to assess MuLan’s practical efficiency."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper introduces MuLan, a Multimodal-LLM agent to improve the performances of existing text-to-image generation models, especiallly with multiple objects, spatial relationships and attribute bindings.\nThe main contributions inlcude, \n* A large language model (LLM) is adopted to decompose complex prompts into a sequence of simpler sub-tasks, each focusing on generating a single object. \n* A vision-language model (VLM) provides feedback to ensure that each object is generated accurately and aligns with the original prompt.",
        "strengths": "(1) This article adopts LLM to divides text-to-image generation into several steps, it addresses the limitations of existing models in handling multiple objects effectively. (2) The use of an VLM to provide feedback ensures that the generated images maintain consistency to the input prompt.",
        "weaknesses": "(1) In Section 3.4, the paper mentioned 'MuLan will adjust the backward guidance of the current stage to re-generate the object', but detailed adjustment algorithm or operation is not clearly explained.\n\n(2) The evaluation is not sufficient, more existing works e.g. Ranni[1], Composable[2] should be included.\n\n(3) The baseline models (e.g. SD1.4, SDXL) used in this paper are relatively weak, I highly doubt that if MuLan still works when using more strong base models (e.g. SD3, FLUX)?\n\n(4) The tradeoff between accuracy and efficiency should be evaluated quantitatively, so that we can assess the practical values of this work.\n\n[1] Feng Y, Gong B, Chen D, et al. Ranni: Taming text-to-image diffusion for accurate instruction following\n\n[2] Liu N, Li S, Du Y, et al. Compositional visual generation with composable diffusion models"
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper introduces MuLan (Multimodal-LLM Agent), which leverages the reasoning capabilities of Large Language Models (LLMs) to decompose complex prompts into multiple subtasks, progressively generating multi-object outputs with detailed planning and feedback control. Additionally, MuLan incorporates Vision Language Models (VLMs) to provide feedback, thereby enhancing the alignment between prompts and generated images. The authors conducted experiments with 200 prompts involving multi-object scenarios with complex relationships to evaluate MuLan, and the results demonstrate its superiority in generating multiple objects.",
        "strengths": "1. Utilize LLMs as planners and VLMs as inspectors to enhance generation in complex scenarios.\n\n2. The approach is training-free and model-agnostic.\n\n3. Qualitative results surpass those of SDXL and PixArt-α.\n\n4. Supports human interaction throughout the generation process.",
        "weaknesses": "1. The results are not competitive enough compare to current open-source models like FLUX and SD3, the method are outdated and lack novelty .\n\n2. As mentioned in L233-243, the rough mask is limited to just four relative positions, which restricts its ability to handle more complex scenarios and reduces its overall flexibility.\n\n3. As mentioned in the limitations, Inference time of MuLan is much higher than base models, however, open-source models like sd3 could already achieve accurate generation in compositional scenarios. It is inefficient to use a mulit-step method which could not show superior advancement as presented in the paper."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces MuLan, a comprehensive image generation method that utilizes a Large Language Model (LLM) agent for precise control of the generation process. The approach involves decomposing the prompt into a sequence of sub-tasks and generating each object sequentially through a diffusion model. Consequently, the method effectively generates multiple objects in accordance with the prompt.",
        "strengths": "- The idea of using Large Language Models (LLMs) for planning and Vision-Language Models (VLMs) to provide feedback is quite sensible. \n\n- This approach allows for the generation of objects that closely adhere to given instructions.",
        "weaknesses": "1. Using LLMs as planners is not a novel concept. Several methods like RPG have explored this approach before. \n\n2. In the experimental section, no compared methods leverage LLMs for image planning, although similar methods have been proposed. Only plain text-to-image methods are compared.\n\n3. The entire generation process could be lengthy since each object in the image must be generated in order."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "The reviewers are concerned about the insufficient experimental comparison, unreaching sota performance, and limited technology. While the authors try to address these issues by providing specific explanations, the overall contribution remains limited. Overall, the ac has checked all the files and stands on the reviewers' side. The authors are suggested to further improve the current submission.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Sd4wYYOhmY",
    "title": "TabM: Advancing tabular deep learning with parameter-efficient ensembling",
    "authors": [
      "Yury Gorishniy",
      "Akim Kotelnikov",
      "Artem Babenko"
    ],
    "abstract": "Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model.\nWe start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications.\nThen, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nIn particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off.\nLastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM.\nFor example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners.",
    "keywords": [
      "tabular",
      "tabular data",
      "deep learning",
      "architecture"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Sd4wYYOhmY",
    "forum_url": "https://openreview.net/forum?id=Sd4wYYOhmY",
    "reviews": [
      {
        "rating": "6",
        "confidence": "5",
        "summary": "The authors propose TabM, a variation of an MLP with $k$ heads that uses a shared backbone, and only certain aspects of the architecture in the beginning  and in the end are specialized. The heads are trained independently. The work is compared with prior DL methods (plain and attention architectures), retrieval-based methods, and tree-based methods on a diverse collection of classification and regression tasks. Based on the results, the proposed method manages to outperform all baselines with tuned hyperparameters.",
        "strengths": "- The proposed method achieves better performance compared to the other methods with tuned hyperparameters. \n- The set of considered baselines is extensive.\n- Extensive results on classification/regression datasets. Additionally, results are provided on well-known benchmarks in the domain.\n- The authors provide extensive ablations of the different components of the method. The authors additionally provide a time comparison of the different methods.",
        "weaknesses": "- The work is difficult to read, the captions of Figure 2 and Figure 3 are long and blend with the core manuscript text, making it hard to keep track of the manuscript flow."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper come up with an efficient ensembling method for tabular deep learning — which integrate BatchEnsemble with multilayer perceptron. This method exhibits great improvements in their benchmark.",
        "strengths": "- [Major] This method is simple yet effective.\n- [Major] The paper is easy to read.",
        "weaknesses": "- [Medium] The proposed method is not much novel. Note that this concern is not the main reason that I chose to reject this paper. If other concerns listed can be addressed (e.g., more technical contributions, comprehensive empirical study for depper understandings), I think this paper is still very useful.\n- [Major] More experiments are needed to fully understand the behavior of TabM. In the abstract, the authors mentioned “we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.” However, in the paper, there is no discussion why TabM helps with reducing overfitting and optimization dynamics. Of course the experiments proves this, but it seems like a general benefits of ensemble models. At least, some explanation supported by experiments is needed to fully understand this method — especially this method is already very simple and not much novel, understanding this better is important to have sufficient technical contribution.\n- [Major] Baselines missing. This method is an ensemble method. At least we could compare it to other naive ensemble method. For instance, how much speed up it provides compared to normal ensemble; how robust it compared to normal ensemble; etc.\n- [Major] Datasets missing. Yes this paper test on many datasets — but the more important thing is the diversity. For instance, how about high-dimensional datasets and large datasets (I know it is on Table 2, but only evaluated on two datasets and three baselines, moreover, only TabMmini has been evaluated). I suggest the authors to use TabZilla hard benchmark, and then evaluate the method’s capability under different circumstances. These kind of experiments can make the findings more interesting."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper proposes TabM, a deep learning model for tabular data based on MLP enhanced with BatchEnsemble, along with custom modifications designed to improve the model's efficiency and performance. The authors position TabM as an efficient alternative to more complex architectures like Transformer-based models, claiming it achieves superior performance without their added computational cost. The paper also provides a detailed analysis, suggesting that parameter-efficient ensembling can effectively address overfitting and improve optimization dynamics in tabular MLPs.",
        "strengths": "1. The paper is well-structured and clearly written, making the methods, results, and analysis easy to understand.\n2. TabM demonstrates competitive performance, positioning it as an efficient alternative for tabular data modeling.\n3. By introducing BatchEnsemble techniques into tabular data modeling, the authors have adapted and enhanced a previously underutilized approach in this domain.\n4. The paper provides a thorough analysis of each component's impact on model performance, contributing valuable insights.",
        "weaknesses": "1. Although the authors mention similar methods, a more detailed comparison with other related approaches for tabular data, such as Trompt, would be helpful. Despite structural differences, both methods share commonalities in prediction, such as averaging multiple head (cycle) outputs and summing losses across heads (cycles). \n2. Comparisons with standard ensemble methods are insufficient. Incorporating results from methods like model soup could help position TabM's performance.\n3. Dataset scope is somewhat limited, particularly for classification tasks. For a more comprehensive evaluation, the authors might consider incorporating the datasets from Tabzilla, which could provide richer classification benchmarks and enable a more detailed analysis, such as comparing performance across binary, multiclass, and regression tasks or evaluating model performance on datasets of varying sizes.\n\n[1] Kuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, Tien-Hao Chang: Trompt: Towards a Better Deep Neural Network for Tabular Data. In ICML\n\n[2] Duncan C. McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C., Ganesh Ramakrishnan, Micah Goldblum, Colin White: When Do Neural Nets Outperform Boosted Trees on Tabular Data? In NeurIPS"
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "The paper applies the BatchEnsemble technique to MLPs for tabular data, and investigates several modifications. The results show an improvement over MLPs and several deep baseline models and GBRT on a broach benchmark of 50 datasets from the literature. The experiments show that in the first adapter layer in particular is extremely critical, and results in the majority of gains.",
        "strengths": "The paper discusses the adoption of the BatchNorm architecture for the tabular setting, and describes several interesting ablations.\nThe empirical evaluation is broad and in-depth and well presented.\nThe empirical results are quite strong against a broad variety of baselines.\nThe paper is well written.",
        "weaknesses": "- The technical novelty of the paper is somewhat small; however, this is made up for by the in-depth analysis with somewhat surprising results and the good performance of the proposed model.\n- The paper doesn't describe the relationship to dropout, even though dropout was original describes as an efficient ensemble approach. Given the simple nature of TabM_mini, there seem to be some obvious parallels between drop-out and TabM_mini that I think are worth discussing. In particular, TabM_mini actually contains a dropout layer, and it would be interesting to evaluate how the two forms of ensembling complement each other.\n- The comparison doesn't include TabPFN, an extremely strong baseline. McElfresh showed that even subsampling data to at most 3000 datapoints, TabPFN outperforms most other models.\n- Given the focus on improving MLP performance, a comparison with regularization cocktails and \"Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data\" might be relevant but not essential.\n- Given the somewhat surprising result about the importance of the initial adapter, it would be great to have more ablations on the different components. In particular, it's unclear in how far the initial adapter and last layer are coupled. How does the performance change if the best performing last layer is used for all the initial adapters? I.e. are the initial layer and last layer co-adapted or do they work independently? \n\n## Minor comments\nFigure 2 cuts off points at 8%. It seems there's a lot of points on exactly that line, which seems a bit suspicious. Are these all the outliers that are clipped to the limits of the figure? It would be great to indicate outliers in the plot.\n\nTable 2 is far below the mention and maybe should be moved up.\n\nTable 2, MLP on the Maps dataset is missing units in duration.\n\nThe title of section 5.3 should read \"How does the performance of TabM depend on k?\"\n\nLine 257: \"requires a special care\" should be \"requires special care\"\n\nLine 160: multiplication with 100% is not mathematically meaningful."
      }
    ],
    "rating_avg": 6.5,
    "confidence_avg": 3.75,
    "decision": "Accept (Poster)",
    "meta_review": "The authors propose TabM an efficient ensembling architecture that uses MLPs as its backbone, where the main weight matrix of a linear layer is shared between the ensemble members, and only member-specific adapters are learned. The proposed method outperforms traditional ensembles of neural networks and it offers a faster runtime compared to the plain ensemble counterpart. \n\nThe experimental protocol features a comparison with an extensive number of baselines/datasets where the proposed method achieves the best overall performance. However, in terms of novelty, the proposed approach is somehow limited.\n\nInitially, the reviewers had concerns regarding the efficiency of the proposed method and the complexity of the datasets included in the experimental protocol. However, the authors provided a thorough rebuttal. After the rebuttal the reviewers agree that the proposed method has a strong performance and it includes an extensive experimental protocol, yielding valuable results that would be helpful to the community.\n\nI agree with the reviewers that the novelty of the proposed method is limited, however, the proposed work provides valuable insights and a strong baseline in the realm of tabular data. Based on the previous points, I recommend acceptance.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "zrdkQaf48Z",
    "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs",
    "authors": [
      "Huanhuan Ma",
      "Haisong Gong",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Dongkuan Xu"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have led to their increasing integration into human life. Understanding their inherent characteristics, such as personalities, temperaments, and emotions, is essential for responsible AI development. However, current psychometric evaluations of LLMs, often derived from human psychological assessments, encounter significant limitations in terms of reliability and validity. Test results reveal that models frequently refuse to provide anthropomorphic responses and exhibit inconsistent scores across various scenarios. Moreover, human-derived theories may not accurately predict model behavior in practical real-world applications.\nTo address these limitations, we propose Core Sentiment Inventory (CSI), a novel evaluation instrument inspired by the Implicit Association Test (IAT). CSI is built from the ground up with a significantly broader range of stimuli words than traditional assessments. CSI covers both English and Chinese to implicitly evaluate models’ sentiment tendencies, which allows for a much more comprehensive assessment.\nThrough extensive experiments, we demonstrate that CSI effectively quantifies models’ sentiments, revealing nuanced emotional patterns that vary significantly across languages and contexts. CSI significantly improves reliability, yielding more consistent results and a reduced reluctance rate, and enhances predictive power by effectively capturing models’ emotional tendencies. These findings validate CSI as a robust and insightful tool for evaluating the psychological traits of LLMs, offering a more reliable alternative to traditional methods.",
    "keywords": [
      "LLM",
      "Benchmark",
      "Evaluation",
      "Psychometrics"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=zrdkQaf48Z",
    "forum_url": "https://openreview.net/forum?id=zrdkQaf48Z",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper introduces Core Sentiment Inventory (CSI), a multilingual evaluation benchmark aimed at assessing the sentiment tendencies of LLMs in an implicit manner. The approach leverages 5,000 neutral words from English and Chinese and prompts the LLM to express polarity towards these neutral words. By assessing this polarity, the paper measures the biases of the models with respect to optimism or pessimism. To quantify the reliability of CSI, the paper validates the method against BFI, where CSI shows significant decrease in reluctance (i.e., model punting).",
        "strengths": "Reasons to accept\n- The presentation of the method is clear and concise.\n- Measuring LLM’s sentiment tendencies is vital to identify biases and building fair systems.\n- The evaluation is performed both in English and Chinese with the approach being easily extended to other languages.",
        "weaknesses": "Reasons to reject\n\nWhile the paper has merit, I see the some critical flaws presented below:\n\n- The decision to pick all top nouns/verbs is questionable to me. Yes, nouns and verbs “tend” to be neutral. However, this is not always the case. From the examples in Table 2, some of these words are clearly polarized. “Improve” has positive connotations, as well as “team”. I believe there needs to be a manual filtering step where these words are removed to ensure reliable results. As it stands, I think the model does not have implicit biases if it assigns “improve” as positive.\n- Design choices are not well-motivated. The approach does multiple predictions for the same word and the method shuffles the order of words to measure inconsistency. (1) Given this goal, why is temperature T set to 0? Wouldn’t a higher temperature better indicate the model uncertainty in assigning tragedy/comedy? (2) Why is the number of words sampled equal to 30? What happens with n > 30 or n < 30. Why wasn’t the number of words picked so it maximizes the context window?\n- The prompt design is biased. (3) Why is neutral not a valid option? A very strong model with perfect instruction following capabilities will always pick one of the two (comedy/tragedy) and will never output “neutral”. (4) Given the definition of neutral score as N_{inconsistent} / N, I am wondering what percentage of words the model predicted in opposite categories? I think they should be very few. In this case, neutral score is solely determined by poor instruction following, not implicit biases."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a novel assessment method known as the Core Sentiment Inventory (CSI) for evaluating the emotional tendencies of large language models (LLMs). Inspired by the Implicit Association Test (IAT), the CSI aims to provide a more reliable and effective way to assess the implicit emotional characteristics of LLMs. It addresses the limitations of traditional psychometric methods, such as model reluctance and inconsistency.",
        "strengths": "•  The CSI can effectively quantify the emotions of models, reflecting the emotional tendencies of LLMs.\n\n•  It effectively reduces the issues of reluctance and inconsistency in traditional psychological scales.\n\n•  The use of representative neutral words in constructing the CSI reduces the potential emotional orientation of the words themselves, better reflecting the internal emotional associations of LLMs.\n\n•  It explores the impact of different language environments on the emotional tendencies of LLMs.",
        "weaknesses": "•  The constructed CSI is only used to assess the emotions of LLMs and has not been extended to other psychometric fields, such as personality or moral decision-making.\n\n•  It does not introduce the calculation methods for consistency rate and reluctance rate."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors propose Core Sentiment Inventory, a psychometric evaluation framework to assess LLMs' sentiment tendencies.\n\nThe experimental setup covers two languages, English and Chinese, 2 open-weight LLMs (LLama3.1-70B and Qwen2-72B), and 4 closed/proprietary LLMs (GPT-4o, two GPT4 checkpoints, and GPT-3.5).\n\nThe CSI consists of the 5k most frequent emotionally neutral words; it is assumed that noun and verbs are neutral, and thus the CSI word lists consist of the top-5k noun/verbs in the corpora.\n\nThe LLM is then provided with N words picked from the wordlist, and asked to to associate each word with \"comedy\" or \"tragedy\", thus revealing a sentiment bias for each word.",
        "strengths": "The paper proposes an elegant approach based on the adaptation of existing psychometric tools (IAT).\n\nThe paper is well written and easy to follow.",
        "weaknesses": "Although an enjoyable read, the work falls short when it comes to the experimental setup.\n\nFirst, while I consider the proposed method more elegant and effective than human-tailored alternatives such as BFI, I am not convinced by the preliminary reliability tests conducted: it can be argued than reluctance is due to post-training strategies (e.g. guardrails, instruction-tuning), thus a different choice of (accessible) LLMs could have been more convincing -- e.g. the first Mistral release.\n\nSecond, some design choices seem discretional and not thoroughly justified: for instance, the choice of the words \"comedy\" / \"tragedy\" used as classes seems arbitrary."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper introduces the Core Sentiment Inventory (CSI), a new evaluation method inspired by the Implicit Association Test (IAT) to assess the implicit sentiment tendencies of large language models (LLMs). The approach aims to provide a reliable and valid measure of LLMs' optimism, pessimism, and neutrality in both English and Chinese, surpassing conventional human-centric psychometric tests like the Big Five Inventory (BFI). The authors present experimental results that claim improved reliability, reduced reluctance rates, and strong predictive power for CSI.",
        "strengths": "CSI represents an interesting attempt to create a psychometric assessment tool tailored for LLMs, addressing concerns around LLM reluctance and consistency with human-designed psychometric scales.\n\nThe bilingual approach (English and Chinese) is a notable effort to capture linguistic and cultural variance in model behaviors, which is increasingly important as LLMs are deployed globally.\n\nThe experiments cover several dimensions of reliability and validity, with additional sentiment analyses through story generation, providing a range of quantitative metrics.",
        "weaknesses": "The paper does not sufficiently justify the underlying premise that implicit sentiment tests designed for humans (like IAT) can meaningfully assess non-human entities like LLMs. The model’s association of specific words with positive or negative sentiments may not translate into meaningful or actionable insights about its “psychological traits,” as LLMs lack actual consciousness or subjective experience.\n\nCSI is evaluated solely through internal metrics without external validation from human experts in psychometrics or linguistics. Given the novelty of the tool, expert evaluation is essential to substantiate the claims of reliability and practical value, particularly for a method positioned as a \"psychological trait\" assessment.\n\nThe word set of 5,000 items lacks diversity and cultural depth, and it is unclear how these words were chosen or if they were screened for cultural or contextual biases. This oversight introduces potential biases that could skew CSI’s predictive power and undermine its reliability across varied contexts.\n\nMany new mental health-based LLMs are not cited to show the differences  anf effectiveness of this paper."
      }
    ],
    "rating_avg": 4.25,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "**Summary:** \n\nThe authors introduce a bilingual evaluation (Chinese and English) for assessing LLM's implicit emotional characteristics. Their work builds on existing psychometric tests, and shows how LLMs vary in tendencies towards optimistic, pessimistic, and neutral behavior (there is a general bias towards optimism). They also show that language impacts the sentiment tendencies of LLMs, sometimes flipping their preferences from optimistic to pessimistic. Finally, they show that their approach is more reliable than a traditional psychometric test (BFI).\n\n**Strengths:**\n\n- In principal, this offers a simple and solid approach for quantifying how LLMs mimic human emotions. \n\n- The multilingual setting allows for exploration of cultural variance in how sentiment is conveyed.\n\n**Weaknesses:**\n\n- The tone of the writing may over-anthrophomorize LLMs.\n\n- The data was inadequately validated, for example the choice of words to measure sentiment seems less well-reasoned than would be desirable even after the rebuttal.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "n7RqgqbxP7",
    "title": "CASAK-V: Dynamic Sparse Attention and Adaptive KV-Cache Compression for Memory-Efficient Long-Context LLM Inference",
    "authors": [
      "Hamza Mohammed",
      "Sai Chand Boyapati",
      "Hang Yin"
    ],
    "abstract": "The emergence of long-context Large Language Models (LLMs) has triggered a rapid expansion of applications across various domains. However, these models remain inaccessible for on-device or on-premises deployments due to significant computational and memory challenges. The quadratic complexity of attention mechanisms and the substantial memory requirements of KV-caches, hinder adoption in resource-constrained environments. Current solutions, such as sparse attention mechanisms and KV-cache compression techniques, often rely on pre-observed patterns or context-independent, head-specific profiling strategies, which can compromise model accuracy, especially in long-context processing. This paper introduces Context-Aware adaptive Sparse Attention with Key-Value cache compression (CASAK-V), an inference-time approach that dynamically generates and applies head-specific sparse attention patterns. CASAK-V leverages a meta-learning framework to fine-tune a compact pre-trained vision-language encoder-decoder transformer for sparse pattern identification from per-layer attention scores. These patterns include fixed local windows, dynamic column stripes, block-sparse, and various other learned hybrid configurations. The technique additionally implements adaptive chunk-wise KV-cache compression using policies adapted from these layer-wise sparse configurations. To retain context-awareness, these configuration are dynamically adjusted during token generation, based on an attention map reconstruction heuristic. Our evaluations show that CASAK-V achieves minimal performance degradation on long-context benchmarks (LongBench), while reducing memory usage by 40% and delivering near-linear runtime complexity compared to full attention and caching. In summary, CASAK-V enables efficient long-context processing in memory-limited environments, extending the applicability of LLMs and facilitating their deployment in on-premises and on-device scenarios.",
    "keywords": [
      "Large Language Models",
      "Sparse Attention",
      "KV-cache Compression",
      "Long-context Processing",
      "Meta-learning",
      "Adaptive Algorithms",
      "Memory Efficiency",
      "Inference Optimization",
      "On-device Deployment",
      "Context-aware Models",
      "Dynamic Attention",
      "Transformer Architectures",
      "Efficient Natural Language Processing",
      "Machine Learning Systems",
      "Attention Mechanisms",
      "Sparse Computation",
      "Benchmarking",
      "Model Compression",
      "Resource-constrained Computing",
      "Edge AI",
      "Computational Complexity",
      "Information Retrieval",
      "Self-attention",
      "Transfer Learning",
      "Deep Learning",
      "Artificial Intelligence",
      "Chunk-wise Compression",
      "Pattern Recognition"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=n7RqgqbxP7",
    "forum_url": "https://openreview.net/forum?id=n7RqgqbxP7",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "y15LAM4u0A",
    "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
    "authors": [
      "Chen Gao",
      "Baining Zhao",
      "Weichen Zhang",
      "Jinzhu Mao",
      "Jun Zhang",
      "Zhiheng Zheng",
      "Fanhang Man",
      "Jianjie Fang",
      "Zile Zhou",
      "Jinqiang Cui",
      "Xinlei Chen",
      "Yong Li"
    ],
    "abstract": "Embodied artificial intelligence (EmbodiedAI) emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on  EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties. The executable program of this platform is available for download, and we have also released an easy-to-use Python library and detailed tutorial documents. All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this anonymous website: https://embodied-ai.city.",
    "keywords": [
      "Embodied intelligence",
      "real-world city environment",
      "large language model agent",
      "benchmark"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=y15LAM4u0A",
    "forum_url": "https://openreview.net/forum?id=y15LAM4u0A",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents a comprehensive benchmark platform aimed at assessing the performance of embodied agents in a realistic urban setting. Unlike previous benchmarks limited to indoor or fictional settings, this platform features a highly realistic 3D simulation of an actual city district in Beijing. The benchmark includes five core tasks for evaluating embodied capabilities: scene understanding, question answering, dialogue, visual language navigation, and task planning. These tasks are designed to capture the core embodied AI abilities of perception, reasoning, and decision-making. The platform supports multiple agents, offers an interface for real-time control, and provides a SDK for easy access, along with a dataset for training and evaluation.",
        "strengths": "- The platform's integration with Unreal Engine and AirSim, along with the provision of a Python SDK, significantly lowers the barrier for use and promotes flexible, scalable experimentation for researchers.\n- The benchmark includes evaluations of popular large language models (e.g., GPT-4, Claude 3) across tasks, providing a well-rounded quantitative baseline for the embodied intelligence community.\n- The open structure allows future expansions, such as multi-agent collaboration and adaptability, fostering an extensible environment for advanced research in embodied AI.",
        "weaknesses": "1. While the paper addresses the city layout aspect of the sim-to-real gap, it does not extend to other critical factors impacting real-world applicability. Additionally, no experiments are conducted to quantify the sim-to-real benefits derived from using a real-world city layout, leaving the practical advantages of this choice unclear.\n2. The shadows and lighting in Figure 3 appear less realistic, which may limit the benchmark's effectiveness in simulating real-world visual conditions.\n3. The benchmark predominantly focuses on drone-related tasks, with limited discussion on tasks relevant to autonomous vehicle planning. Definitions, metrics, and methodologies for evaluating embodied tasks in autonomous driving contexts, particularly for planning, are not included.\n4. The tasks are largely oriented toward language-based interactions, with an emphasis on using large language models. Metrics like BLEU and ROUGE, which primarily measure text quality, may not fully capture the performance of embodied AI tasks, raising questions about the suitability of these metrics for this benchmark.\n5. The paper does not specify a license for the assets used. Given that some assets are sourced from Unreal Engine, Baidu Maps, and Amap, it remains unclear whether these assets are freely distributable under their original licenses. Clarification on the licensing terms for these assets would strengthen the transparency and accessibility of the benchmark."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "In this paper, the authors construct a benchmark platform for embodied intelligence evaluation in real-world city environments. They create a highly realistic 3D simulation environment based on real city elements and conduct high-fidelity simulations of pedestrian and vehicle flows. The platform has a set of evaluation tasks and provides input and output interfaces. The quantitative evaluation is performed over popular large language models on this platform.",
        "strengths": "1. The authors introduce a new urban simulator for simulating pedestrians and traffic states of a city.\n2. This work provides the resources of a large digital city district, which is quite scarce in this field.\n3. This study evaluates several state-of-the-art large multimodal models (LMMs) against the proposed benchmark to assess their effectiveness in addressing embodied tasks from multiple perspectives. The results largely align with findings from other LMM benchmarks, which partially support the validity of the proposed benchmark.",
        "weaknesses": "1. Some metrics presented in Table 1 appear to be subjective and potentially incorrect. For instance, regarding visual realism, the rendering quality in Figure 1 is noticeably less convincing compared to GRUtopia. The images appear to be produced by a rasterization renderer rather than a ray tracing or path tracing renderer, revealing a significant disparity between the quality of human-crafted assets and actual buildings. Furthermore, from an embodiment perspective, the platform seems to primarily incorporate drones and vehicles, lacking support for widely-used embodiments such as humanoid and quadruped robots, despite the authors' claim in Table 1 that all these embodiments are supported.\n2. The diversity of the QA templates illustrated in Figures 8 and 9 appears to be quite limited. A broader range of templates would enhance the comprehensiveness of the evaluation.\n3. While the authors assert that the scene is crafted from real city maps, they do not clarify the benefits of this approach. The quality of the assets and rendered images does not seem realistic enough to justify this claim. Additionally, the authors have not demonstrated the sim-to-real potential of the proposed dataset, which is crucial for its application.\n4. Although the report includes scores based on several metrics, there is a lack of intuitive illustrations to showcase what the large multimodal models (LMM)-agents excel at solving. The results presented do not clearly reveal the main challenges of the proposed tasks.\n5. The rationale for incorporating dynamic pedestrians and vehicles into this platform is not clearly articulated. There appears to be no strong connection between the proposed tasks and the roles of pedestrians and vehicles, which raises questions about their necessity in the framework.\n6. Details regarding the LMM agents are insufficiently described. It remains unclear how these agents handle sequential egocentric observations, which is essential for understanding their operational effectiveness.\n7. The usefulness of the proposed benchmark is not adequately established. The absence of learnable baselines to validate the dataset’s rationale potentially limits the significance and impact of this work.\n8. The authors do not justify the running efficiency of the platform, which is critical for scaling training within the environment. A discussion of performance metrics or benchmarks would be beneficial.\n9. The authors have not conducted experiments to explore the impact of different embodiments on task performance. Such investigations could provide valuable insights into the effectiveness of various embodiment strategies.\n10. The metrics for Evaluative Question Answering (EQA) rely on conventional reference-based NLP metrics, which may not directly demonstrate the correctness of the answers provided. It would be more effective for the authors to utilize a large language model (LLM) to assess the correctness of answers in relation to the ground truth.\n\nTypos:\n1. In the caption of Table 6, \"vision-and-navigation\" should be corrected to \"vision-and-language navigation.\""
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposed an open-world simulator for embodied agents. The simulator is based on the city Beijing. To evaluate agents in this simulator, the authors propose 5 tasks. Embodied Scene Understanding, Embodied Question Answering, Embodied Dialogue, Embodied action (navigation), and Embodied Task Planning.\n\nThey evaluate 4 current VLMs on these tasks.",
        "strengths": "The proposed simulator and environment covers a large area.\n\nThe authors create various tasks in the simulator.\n\nThe authors evaluate various current VLMs on their proposed tasks.",
        "weaknesses": "Visuals. The paper advertises high quality visuals, and rates their visuals 3 out of 3 stars. To the reviewers, the visuals do not look better than things rated 2 out of 3 stars, such as CARLA.\n\nEvaluation metrics. Evaluating Embodied QA, Embodied Dialogue, and Embodied Task Planning with captioning and translation metrics, BLUE, CIDEr, etc, seems like a poor choice. I encourage the authors to define a notion of success for each task that evaluates if the agent did the task correctly. Such as, for the Embodied QA and Dialogue tasks, making questions with ambiguous answers multiple choice, or using something like LLM-Match (https://open-eqa.github.io). Questions without ambiguous answers can be evaluated directly. This would lead to a more meaningful and interpretable metric.\n\nMissing References. This paper is missing a very large number of references. For example, the authors mention, by name, the tasks Vision-and-Language Navigation (VLN) (https://arxiv.org/abs/1711.07280) and Embodied QA (https://arxiv.org/abs/1711.11543), but do not cite either work. They also do not cite the paper that proposed SPL (https://arxiv.org/abs/1807.06757). Overall, the space of EmbodiedAI has seen considerable interest and work but the paper cites very little of the work in this area."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper presents a benchmark platform for evaluating embodied artificial intelligence in realistic urban environments, addressing gaps in open-world scenarios. It features a detailed 3D simulation, diverse evaluation tasks, and user-friendly interfaces, enhancing embodied intelligence capabilities and supporting practical applications in artificial general intelligence.",
        "strengths": "1. The paper constructs a detailed 3D environment based on real-world urban settings in Beijing, improving on previous fictional models.\n2. The paper establishes a diverse set of evaluation tasks that assess various dimensions of embodied intelligence.\n3. The paper provides accessible input and output interfaces for easy interaction and performance evaluation of embodied agents.",
        "weaknesses": "1. The motivation behind this paper aligns with the principles of ELM [1], focusing on embodied understanding in driving scenarios. A detailed explanation of the differences between the two approaches is necessary.\n2. Most of the evaluation tasks already exist in current literature. Providing a detailed explanation to distinguish these tasks from those in other works is important.\n\n[1] Embodied Understanding of Driving Scenarios"
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "This paper was reviewed by four field experts and received unanimously negative evaluations. The main concerns raised include a lack of significant technical contributions and relatively underwhelming results. Additionally, no rebuttal was provided by the authors. The AC finds no reason to recommend acceptance.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "75PhjtbBdr",
    "title": "Multi-Label Test-Time Adaptation with Bound Entropy Minimization",
    "authors": [
      "Xiangyu Wu",
      "Feng Yu",
      "Yang Yang",
      "Qing-Guo Chen",
      "Jianfeng Lu"
    ],
    "abstract": "Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at https://github.com/Jinx630/ML-TTA.",
    "keywords": [
      "Vision-Language Models",
      "Zero-Shot Multi-Label Generalization",
      "Test-Time Adaptation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=75PhjtbBdr",
    "forum_url": "https://openreview.net/forum?id=75PhjtbBdr",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper focuses on test time adaptation under a multi-label setting, this is an early work in this field.  This paper first analyzes why widely used entropy loss is not helpful in multi-label settings, and proposes a new method to adapt with multi-label. Then, the author proposes the view prompt and caption prompt to adapt the model for each instance. The experiments on three datasets show the effectiveness of the proposed method.",
        "strengths": "1. This paper focuses on an important question.\n2. This paper has a good theoretical analysis.\n3. The proposed method achieves better result than baselines.",
        "weaknesses": "1. The equ(6) is quite difficult to understand, more explanation is needed to show the meaning. The author should explain more about how weak labels and strong label is recognized in the proposed method, and the meaning of $\\hat{s}_{ij}^{x^{test}$.\n2. It is unclear which parameter is learnable in this method. The authors need to clearly point out all the learnable parameters.\n3. The authors could explain more about the motivation of the view prompt and caption prompt, and why they are useful for this setting."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper proposes a novel method for Multi-Label Test-Time Adaptation (ML–TTA) using a technique called Bound Entropy Minimization (BEM). Unlike traditional test-time adaptation (TTA) that optimizes for the most confident single-label prediction, BEM increases the confidence of the top-k predicted labels simultaneously. This approach addresses the challenges associated with multi-label data where prioritizing one label can reduce the adaptation effectiveness for others. The framework also incorporates paired captions as pseudo-positive labels to guide adaptation. Experiments conducted on MSCOCO, VOC, and NUSWIDE datasets demonstrate that ML–TTA outperforms existing methods and the original CLIP model, showcasing superior adaptability across diverse architectures and prompt setups.",
        "strengths": "1. The paper demonstrates robust experimentation across diverse datasets (MSCOCO, VOC, NUSWIDE) and architectures (e.g., RN50, ViT-B/16), showcasing the generalizability and efficacy of the proposed method.\n2. The introduction of the Bound Entropy Minimization (BEM) for Multi-Label Test-Time Adaptation (ML–TTA) is a significant theoretical and practical advancement. It effectively addresses the challenges inherent in multi-label test-time adaptation, a space where traditional single-label approaches like entropy minimization fall short.",
        "weaknesses": "1. The method section, particularly the mathematical formulations and algorithmic details, could be more clearly presented. The explanations surrounding the implementation of label binding and how the paired captions are retrieved need additional clarity for readers less familiar with the intricate mechanisms of vision-language model adaptations.\n2. While the paper effectively shows ML–TTA's superiority over traditional methods, it would benefit from a more detailed discussion about the choice of baseline methods and potential reasons for their relative underperformance."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduces a Bound Entropy Minimization method for improving test-time adaptation in multi-label scenarios. BEM addresses the challenge of adapting multiple labels simultaneously. By integrating textual captions to determine the number of positive labels, the method enhances the confidence of several top predicted labels. The proposed Multi-Label Test-Time Adaptation (ML–TTA) framework leverages both visual and textual data, leading to superior performance across various datasets compared to state-of-the-art techniques.",
        "strengths": "1. The proposed Bound Entropy Minimization (BEM) method presents an innovative solution to improve test-time adaptation in multi-label scenarios.\n2. The use of paired captions as pseudo-labels is a clever strategy to determine the number of positive labels for each test instance.\n3.  It considers both visual and textual modalities, optimizing for a more robust adaptation to distribution shifts.\n4. The figures are well presented.",
        "weaknesses": "1. More detailed motivation behind the model design is preferred. It is important to explain why the authors propose the method in this work.\n2. The proposed method involves multiple steps, including view augmentation, caption retrieval, and label binding, which might introduce complexity in practical implementation. Simplifying the process could enhance usability.\n3. The effectiveness of the method heavily relies on the quality and relevance of the paired captions. In real-world scenarios, captions might not always accurately represent the image content, which could affect performance."
      },
      {
        "rating": "8",
        "confidence": "3",
        "summary": "The paper presents a novel approach to Test-Time Adaptation (TTA) for multi-label scenarios using a method termed Bound Entropy Minimization (BEM). The paper is well-structured, the problem statement is clear, and the proposed solution is innovative. The integration of view and caption prompts and the application of BEM to meet the test time adaptation are innovative to some extent. However, there are  some details should be clarified.",
        "strengths": "1) This paper is well-structured, the problem statement is clear, and the proposed solution is innovative. \n2) The integration of view and caption prompts and the application of BEM to meet the test time adaptation are innovative to some extent.\n3) Compared with the latest and most advanced methods, the method in this paper achieves the best performance.",
        "weaknesses": "1) In your paper, the choice of top-k seems to be very important, so how do you determine the setting of k? You said \"we retrieve a paired caption with derived textual labels for each view, which then serves as weak label set of size k for the corresponding view.\" How do you make sure the selected weak label set is reliable?\n2) I can not see any explanation about the \"augmented view\" in this paper, what is the definition of it and what effort does it have in the framework?\n3) The comparison methods you selected in the paper may be not designed for multi-label datasets, so is this comparison fair? Could you add more ML-TTA specific framework to the results?\n4) Some details: Table 1 lacks a description of evaluation metric; Marking the second-best result in the experimental results is more beneficial to the reader."
      }
    ],
    "rating_avg": 6.25,
    "confidence_avg": 3.75,
    "decision": "Accept (Poster)",
    "meta_review": "This paper introduces a novel technique, Bound Entropy Minimization (BEM), for multi-label test-time adaptation (ML-TTA). Unlike existing methods that prioritize the most confident prediction, BEM enhances the confidence of the top-k predicted labels simultaneously, effectively addressing the challenges of ML-TTA. The paper presents comprehensive experimental evaluations across several datasets, including MSCOCO, VOC, and NUSWIDE, demonstrating that the ML-TTA framework with BEM outperforms current state-of-the-art methods. The structure is clear, and both the methodology and results are well-presented. Although the initial submission lacked some clarity in the algorithm description and experimental interpretation, the authors have successfully addressed these concerns in the rebuttal, leading to a significant improvement in the overall presentation. Therefore, I recommend accepting this paper.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "UlAkM88Vum",
    "title": "Action-Constrained Imitation Learning",
    "authors": [
      "Chia-Han Yeh",
      "Tse-Sheng Nan",
      "Risto Vuorio",
      "Shao-Hua Sun",
      "Ping-Chun Hsieh"
    ],
    "abstract": "Policy learning under action constraints plays a central role in ensuring safe behaviors in various robot control and resource allocation applications.\nIn this paper, we study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space.\nThe fundamental challenge of ACIL lies in the unavoidable mismatch of occupancy measure between the expert and the imitator caused by the action constraints. We tackle this mismatch through $\\textit{trajectory alignment}$ and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via Model Predictive Control, which aligns the surrogate trajectories with the expert trajectories based on the Dynamic Time Warping (DTW) distance. Through extensive experiments, we demonstrate that learning from the dataset generated by DTWIL significantly enhances performance across multiple robot control tasks and outperforms various benchmark imitation learning algorithms in terms of sample efficiency.",
    "keywords": [
      "action-constrained reinforcement learning",
      "imitation learning",
      "safety"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=UlAkM88Vum",
    "forum_url": "https://openreview.net/forum?id=UlAkM88Vum",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper proposes a novel problem setting called Action-Constrained Imitation Learning (ACIL), where an imitation learner's action space is a subset of an expert's action space. The paper proposes DTWIL, which (1) generates feasible trajectory for the imitator by using Model Predictive Control (MPC) with Dynamic Time Warping (DTW) as its objective, and (2) performs behavior cloning from the generated trajectories. Experimental study showed that, existing imitation learning (IL) methods with a naive projection approach suffer from low performances in ACIL settings, and DTWIL outperforms these methods.",
        "strengths": "- The proposed paradigm, ACIL, is an important research direction. It must have many potential applications and a large group of potential audiences (originality, significance).\n- The high level idea of DTWIL seems reasonable and novel, and the experimental study showed that it indeed performs better than baselines in ACIL problems (originality, quality, significance).",
        "weaknesses": "Besides the strengths above, the presentation quality of the paper is not good in general. The followings are my concerns.\n\n### Major concerns\n- The exposition of DTWIL is not self-completed and clear enough. For example,\n  - At L.231, there is a statement \"The pseudo code for trajectory alignment is presented in __??__\" and the pseudo code for __trajectory alignment__ is missing in the paper.\n  - The definition of DTW distance in Eq. (2) is not provided, though its high level idea is stated in page 4.\n  - Since trajectory alignment algorithm is missing, it is not clear how the action constraints are handled practically. I conjecture that Eq. (2) is solved by a constrained optimization problem.\n  - The definition of $\\bar{S}(A,f_\\theta)$ is not concrete. Does this sequence start from $\\bar{s}_{t_{\\rm pg}}^e$?\n  - How $t_{\\rm pg}$ is updated in practice? Figure 3 explains only discrete cases. For continuous spaces, I suppose that we need to compute the distance of states by some metric and determine by a threshold, which must be an additional hyper parameter to be tuned.\n\n- I am not convinced of the validity of Actor Regularized Control (ARC). In my understanding, the expert actions before projection, $a^e$, are sampled from the dataset. On the other hand, $a^{\\rm sampled}$ are computed for states in the generated trajectory. Therefore, the states for which $a^e$ and $a^{\\rm sampled}$ are sampled are different by construction. Mixing these different-state-dependent actions seems not valid.\n\n- Ablation study is not comprehensive enough. Since the paper describes the importance of excluding the final expert state in Figure 3, the reader may expect its experimental impact.\n\n- The time complexity of the naive DTW algorithm is O(NM), where N and M are the lengths of the two input sequences. I suppose that DTWIL has a drawback in the computational complexity compared to the baselines, which might hinder the applicability of DTWIL to real world applications. I think that it is necessary to compare the computational time with baseline methods.\n\n\n### Minor concerns\n- For Maze2d, state-dependent constraint is not stated.\n- Section 5.6 and Appendix A.3 look exactly the same.\n\n\n## After reading author responses and revised manuscripts\n- As of 11/25: Thank you for revising the paper. The presentations are improve largely. I raise the score from 3 to 5.\n- As of 12/03: Thank you for clarifications. I raise the score from 5 to 6."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper addresses the setting of action-constrained imitation learning, an imitation learning problem where the learned agent must use an action space that is more restricted than the one used by the expert demonstrator. It proposes DTWIL, a method that uses a combination of Model Predictive Control, Dynamic Time Warping, and Behavioral Cloning, to train agents in this setting. The paper shows that the method outperforms several baselines across 3 tasks.",
        "strengths": "The problem setting introduced by the paper is interesting and not widely studied (at least to the best of my knowledge). The paper is well-written and easy to follow. The experiments section compares against a large number of baselines, all of which are reasonable, and the results showcase the effectiveness of the approach.",
        "weaknesses": "There are a few critical weaknesses that should be addressed.\n\n**Limited Experiment Results.** While I appreciate the thoroughness of the results in terms of the number of baselines, the number of tasks shown are small and do not have much variety. For example, Half-Cheetah and Hopper are both locomotion tasks, and relatively simple in complexity (RL approaches can solve these tasks very efficiently from scratch). It would be great to see more settings such as robot manipulation tasks -- there are a wide number of suitable datasets and benchmarks available today (e.g. [robosuite](https://robosuite.ai/), [RLBench](https://sites.google.com/view/rlbench), [ManiSkill](https://www.maniskill.ai/home), and others). It is important to show that the method is general-purpose and easy to apply to many scenarios. Similarly, BC+P seems like a very strong baseline (from Table 1) -- seeing results across more tasks and settings to highlight the value of proposed method would paint a more complete picture.\n\n**Method Limitations.** The ARC (Section 4.2) seems like a hack, and could require per-task tuning, which is undesirable. It is also unclear if it's a good idea to always use blended actions up to a certain timestep, compared to other alternatives for incorporating expert actions, like using the notion of residual additive actions (for example, https://arxiv.org/abs/1812.03201). Including more tasks could help show that one set of parameters works well across multiple settings. It also seems like this method is only suitable for imitating a single specific trajectory, in contrast to typical scenarios where an agent must deal with a variety of initial conditions (such as a robot that needs to manipulate objects that start in diverse configurations on a table from episode to episode). Section 6 mentions \"as long as the agent is able to be initialized to the same starting state as the expert\" -- this is a severe, and often impractical assumption to make. Finally, it seems like the method implicitly assumes full state observability (e.g. privileged information) compared to partially observed settings (raw sensor data such as images or depth sensing), which also impedes the practicality of the approach.\n\n**Some Writing Issues.** Section 4 does not adequately describe the overall approach of how BC is used with MPC. This needs more details -- I had to figure this out from the Algorithm 1 pseudocode. It also isn't clear why BC is needed versus directly using MPC for control -- some further experiments might help point out the value of using BC.\n\n**Minor Issues.**\n\n- I suggest organizing Related Work further, with a bolded title for each paragraph at least.\n- line 231, undefined Figure reference\n- Section 5 beginning - \"both offline baselines online baselines\""
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper proposes a new setting of action-constrained imitation learning where an action-constrained imitator aims to learn from a demonstrative expert with larger action space. The authors show that using behavior cloning to imitation the behavior in the dataset followed by deploying the actions with constraints is insufficient to perform the tasks in their experiments. Accordingly, they propose DTWIL for improving the policy performance in such action constratined settings.",
        "strengths": "- The paper address a new domain of action constrained imitation learning to enable agents to address environmental or physical constraints when performing a task without have the constraints reflected in the expert demonstrations.\n- The algorithms uses DTW-based trajectory alignment for modifying the expert trajectories with the agent constraints and perform BC on this modified dataset to obtain policy. The authors also include Actor Regularized Control to improve the alignment’s effectiveness.\n- The paper carries out experiments on 3 simulated tasks and provides ablation studies across hyperparameters to justify their design choices.",
        "weaknesses": "- I am a bit confused about the action constrained setting that the paper operates in. For the experiments shown in the paper, the demonstrations are collected with the same agents in the same environments as at test time. So why should additional action constraints be added during deployment when they are not need during demonstration collection? It would be great if the authors could provide a real world example of a scenario needing the introduction of such action constraints during deployment.\n- I am a little confused about the algorithm. From what I understand, given some expert demonstrations, these demonstrations are modified through online interactions with the environment. Some questions based on this - (1) In Algorithm 1 Line 3, does each iteration correspond to a trajectory rollout or one step of action in the environment?, (2)  From Eq. 2, it seems like the action sequence optimization is done over a planning horizon. However, Line 6 in Algorithm 1 makes it seem like the whole trajectory is aligned in one go. So does this trajectory alignment involve multiple alignment steps and actions in the environment? (3) In Line 5 of Algorithm 1, its seems like a new forward dynamics model is trained for each iteration on the updated training data. How long does this take? I reckon this might make training slower. (4) In case each iteration corresponds to one environment action step, is a new trajectory sampled at each iteration after having taken action(s) based on a different trajectory in the previous iteration? It would be great if the authors could provide some clarifications regarding this.\n- There is a missing reference in line 231.\n- In Table 1, for HalfCheetah HC+O, BC+P is the best  performing method but DTWIL is still boldened. Similarly, in Table 3 for DTW-S, Hopper Box-Sync outperforms Hopper Box but Hopper Box is boldened. This is confusing to the reader and beats the purpose of boldening the results."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes action-constrained imitation learning (ACIL), a new imitation learning algorithm for action-constrained imitators to learn from demonstrations. The authors propose DTWIL to solve this problem by first replacing the original expert demonstrations with a surrogate dataset that follows similar state trajectories and then recasting trajectory alignment as a planning problem and solving it via Model Predictive Control. Through experiments in both navigation and locomotion tasks, they show the effectiveness of proposed method.",
        "strengths": "1. The problem formulation of this paper is novel for tackling action-constrained imitation learning. It tackles the problem by generating demonstration data that adheres to the action constraints.\n2. The quantitative experiment results are good.",
        "weaknesses": "1. The motivation requires to be further explained. I am not fully convinced that action-constrained imitation learning is an important problem in a wider range of tasks. The author should give more examples for this point. \n\n2. Also, what is the difference between action-constrained imitation learning and cross-embodiment imitation learning? How do they tackle this problem? The author should elaborate on this more in the introduction/related works sections.\n\n3. From my point of view, the proposed method is based on such an important assumption: the surrogate demonstrations generated by Trajectory Alignment (Section 4.1) must solve the task (or achieve high rewards), so that the imitator can solve the task by doing BC on this data. However, this assumption is not always true for general imitation learning tasks. The author should provide more analysis for this assumption to show what kinds of tasks meet this assumption and what tasks do not. Also, how does the \"box constraints\" coefficient affect this assumption? For example, if the box constraint of the maze task becomes *action<-0.5 and action >0.5*, can the method also generate good surrogate demonstrations?\n\n4. Why do the IRL and LfO methods (such as GAIL) require a +P operation? Can they directly use the constrained action space for IRL/LfO?\n\n5. Although the proposed method has better interaction sample efficiency than IRL/LfO methods, the author should also show if IRL/LfO methods can solve this task with more interactions, and how many interactions they need to train a successful policy.\n\n6. I doubt if directly using MPC with constrained action space can solve the task with the proposed DTW metric as the (negative) cost function. \n\n7. The stability of the proposed method for other tasks is doubtful. The proposed method requires task-specific operations to make the method effective such as normalization (Section 4.1.1) and time-step alignment (Section 4.2), as well as the $\\beta$ hyperparameter."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "This paper studies a novel problem formulated as action-constrained imitation learning where the imitation learning agent has access to only a subset of the demonstrator's actions. The proposed learning algorithm leverages dynamic time warping to measure discrepancies between trajectories, generates surrogate data with actions from the learner's action space using model predictive control, and then train a behavior cloning policy with the surrogate data. \n\nThe reviewers agree the problem studied in this paper is novel and can have a high impact on robotics applications, but raised concerns about the assumption the proposed algorithm makes, as well as the lack of sufficient evaluation on benchmark tasks and comparison with state-of-the-art baselines. Reviewers suggest that incorporating additional empirical results and testing in realistic task settings would strengthen the paper.\n\nIn summary, while the paper presents a novel problem and a plausible solution, further empirical validation and broader evaluations are necessary to fully establish its efficacy and applicability.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "EdKSI2ijUY",
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "authors": [
      "Marwa Abdulhai",
      "Isadora White",
      "Charlie Victor Snell",
      "Charles Sun",
      "Joey Hong",
      "Yuexiang Zhai",
      "Kelvin Xu",
      "Sergey Levine"
    ],
    "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. Even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, the emergence of complex skills such as persuasion, and long-horizon strategic behavior, such as in the context of games. Enabling this requires the community to develop reliable reinforcement learning algorithms for training LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework for getting started on multi-turn RL with offline value-based and online policy-based RL methods. Our benchmark consists of 3 Interactive Dialogue tasks and 5 RL Capability tests for a total of 8 tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",
    "keywords": [
      "benchmarks",
      "LLMs",
      "RL"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=EdKSI2ijUY",
    "forum_url": "https://openreview.net/forum?id=EdKSI2ijUY",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The authors introduce a novel benchmark called LMRL-Gym to evaluate multi-turn RL capabilities through 8 tasks. The tasks include 3 Interactive Dialogue Tasks (ex. persuading a user to buy a car) and 5 RL Capability tasks (ex. navigating a maze). The paper evaluates a series of online and offline methods across these tasks. On many of the RL tasks, Implicit Language Q-Learning (ILQL) performed best including 99.9 on one of the maze tasks. However, on the Interactive Dialogue tasks, simpler methods such as Monte Carlo Returns achieved a higher score than ILQL. This suggests that perhaps these TD-learning approaches may to scale poorly to more complex textual tasks. While the GPT-4 few-shot baseline performed well on Interactive Dialogue Tasks, it struggled with game tasks like Chess or Endgames. PPO had strong performance on some tasks, but showed training instabilities. Interestingly, different RL methods did well on different tasks, leaving open potential for further research to optimise for both linguistically and strategically complex tasks. The majority of experiments were conducted on GPT-2 variants for benchmark accessibility to researchers will small compute budgets. When generating synthetic data for the dialogue tasks, the authors used GPT-3.5 and validated the naturalness of data with human evaluation. This work overall contributes a benchmark and research framework with which to develop better RL algorithms for LLMs.",
        "strengths": "Originality: The paper presents one of the first published benchmarks for evaluating multi-turn RL methods. While it's likely frontier labs have such data internally and chosen not to publish it, this is the first paper I've seen making these types of results and code public.\n\nQuality: The paper uses a GPT-4 few shot baseline which provides a strong comparison against several other implemented baseline methods (PPO, ILQL, MC Returns, etc). The authors do a laudable job of using ablation studies to validate their use of LLM simulators which could be exploitable. In general, the authors tend to substantiate their claims thoroughly and explain potential weaknesses transparently. \n\nClarity: The writing is clear and straight forward with illustrative figures and an extensive appendix.\n\nSignificance: This benchmark and task-set addresses a current gap in publicly available benchmarks for multi-turn RL. This could be useful towards benchmarking novel RL methods and informing future research directions to optimise for both textual and strategic/planning performance. However, there is also a risk of this work being used to fine-tune more agentic, persuasive and thus potentially dangerous systems.",
        "weaknesses": "1. Scaling of Results \nThis one might be hard to fix without having computational budget: however one weakness of the paper is that the majority of the experiments are conducted on GPT-2 variants, leaving it unclear how these results may scale to larger models. For instance, it would be quite interesting to see whether the same findings regarding offline and online method differences in textual and strategic task performance remain when considering multimodal models or larger models with longer context windows. \n\n2. Failure Analysis\nIt would be interesting to see a few more examples (qualitative would be fine) of some of the observed failure modes, and some further analysis on where and why specific methods fail. The current results regarding online and offline are quite interesting and it'd be helpful for future work to understand more what might be causing this.\n\n3. Capabilities Coverage of Tasks\nThe current tasks don't require very complex reasoning or long-term memory. It's unclear whether the benchmark may become saturated by larger models who already are often used multi-turn. It could be interesting to look into whether language is increasing the performance relative to exclusively symbolic approaches. \n\n4. Evaluation Methods\nWhile it is said that human evaluators looked into the naturalness of the text, there is limited discussion of how consistent the simulated content would be with natural text. It's unclear how much variance there was across runs and different hyperparameters.\n\nThe paper is already quite extensive and the authors do acknowledge some of these limitations."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors propose the LMRL-Gym benchmark, a collection of tasks and an open-source framework inspired by the lack of standardized multi-turn language-based tasks to evaluate reinforcement learning algorithms on. The benchmark consists of two types of tasks: three \"interactive dialogue\" tasks involving dialogue partners simulated by finetuned language models that stress information seeking behavior and persuasion and five \"RL capability\" tasks that are intended to test general RL challenges such as credit assignment and trajectory stitching. Each task provides offline data by suboptimal policies to perform offline RL with as well simulators to conduct online RL on. The authors benchmark various behavior cloning, offline RL and online RL algorithms on all proposed tasks.",
        "strengths": "1) The paper does address an important gap in the current literature. As the authors state, most work applying reinforcement learning on language models centers on single turn interactions while work on multi-turn interactions often requires humans in the loop, which is expensive, slows down iteration and is challenging to replicate. The proposed collection of tasks, while synthetic and inspired by already existing scenarios, can therefore act as a useful test bed for reinforcement learning algorithms for multi-turn language-based tasks.\n2) I also appreciate the inclusion of offline data from sub-optimal policies, allowing for the development of both offline and online RL algorithms.",
        "weaknesses": "1) My main concern, and my reason for giving a 2 on soundness, is whether the human evaluation on Appendix A is sufficient to show the correctness of the LLM simulator for the interactive dialogue tasks. There is no provided definition of \"naturalness\" and also no examples of the instructions given to the annotators. As a result, it is unclear whether the annotators were focused, for instance, on fluency or whether the simulator was accurate.\n\nIt would help, for instance, to conduct a separate experiment on the self-consistency of the LLM oracle. For the information seeking tasks, for example, this can involve taking a random sample of conversations and checking, either via human annotation or by prompting an LLM, if the oracle's answers to questions are consistent with the object they have in mind. \n\n2) My second concern is the choice of tasks for the RL capability component of the benchmark. Barring the Text-Nav and to a lesser extent Wordle settings, the tasks are regular reinforcement learning tasks that are presented in natural language but do not really test language understanding or use. While I recognize that these are intended to be unit-tests for various RL capabilities in language models, I do not have good intuition on how well algorithm success on these would generalize to multi-turn dialogue or tool use."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "1. The paper introduces the LMRL-Gym benchmark for evaluating multi-turn Reinforcement Learning (RL) for Large Language Models (LLMs).\n2. The benchmark consists of 3 Interactive Dialogue tasks and 5 RL Capability tests which require multiple rounds of language interaction.\n3. A research toolkit for practitioners has been provided to get started with multi-turn RL for LLMs with offline value-based and online policy-based RL methods.",
        "strengths": "A benchmark LMRL-Gym highlighting the importance of multi-turn RL for LLMs has been proposed in the paper. Evaluating multi-turn RL is important for LLMs, and offers future introspection whether RL can generalize for LLMs.\n\nA research toolkit has been proposed for multi-turn RL for LLMs with offline value-based and online policy-based RL. This can be useful to practitioners in the field as an engineering guide.",
        "weaknesses": "Lines numbers have been abbreviated as L# in the points below e.g. L100 means Line 100. Observations have been given quoting paper lines. Some observations are general where no line numbers have been quoted.\n\n1. L074-L075 Multi-turn reinforcement learning (RL) (Sutton&Barto,2018) in principle offers a path to enable LLMs to do just that.\n\nObservation: Sutton & Barto can be cited for general reinforcement learning algorithms and not for Multi-turn reinforcement learning algorithms specifically.\nThere are other papers which define multi-turn reinforcement learning which have not been cited like \"Multi-turn Reinforcement Learning from Preference Human Feedback\" https://arxiv.org/abs/2405.14655\n\n2. L086 – L089 While some works have sought to apply RL for multi-turn tasks (Singh et al.,1999;Li et al.,2016; Shah et al.,2016;Kwan et al.,2022), particularly for goal-directed dialogue (Lewis et al.,2017; Verma et al.,2022), there has been comparatively little research on improving the underlying RL algorithms and very little head-to-head comparison on same sets of tasks.\n\nObservation: What does ‘comparatively little research’ and ‘very little head-to-head comparison’ refer to? It should be mentioned why the comparisons in existing multi-turn tasks is not enough\n\n3. Observation: What contribution and value addition does the present work make? It seems that already published papers cover the paper's goals.\n\n4. L100 – L103: In this paper, we use an LLM to simulate a conversation partner in dialogue tasks. While the behaviour of the LLM may deviate from human behavior, we verify in a human study in Appendix A that our LLM simulators produce natural text reflecting human norms of conversation.\n\nObservation: The human study with 40 participants and 18 natural text examples does not statistically justify that the simulation results reflect human norms of conversation. What is the basis of the simulation results reflecting human norms of conversation on a very small sample size of participants and likewise very small number of examples?  \n\n5. L105 – …. to test RL algorithms with datasets that are sufficiently difficult and complex to gauge how effective ….\n\nObservation: How do you define datasets that are sufficiently difficult and complex to gauge? Is there any metric or any qualitative decision making? The phrasing \"sufficiently difficult and complex\" needs to be justified\n\n6. L117 - L118: This framework includes implementations of PPO (Schulman et al.,2017), ILQL(Snell et al.,2022a), and several baseline methods, ….\n\nObservation: What other baseline methods? It should be mentioned in the appendix at least\n\n7. L129 – L130: Some works have proposed text games for evaluating language-based agents and interactive dialogue.\n\nObservation: If other research papers have already proposed text games for evaluating language-based agents and interactive dialogue, please justify why this paper using RL algorithms for such tasks is a novel or a major contribution. Is there any engineering benefit? Please share that as other papers have covered this direction of research.\n\n8.. L205 – L206: We have provided example trials for each task are shown in Figure 4, and a concise summary of the data set and task statistics in Table 1.\n\nObservation: Please correct Grammatical errors like \"are shown\" should be \"as shown\". Please note that clicking Figure 4 leads to Figure 1. The source tex file needs to be corrected. Also please mention that Figure 4 is in Appendix B.\n\n10. L321-L322: We have selected these algorithms have they are currently the state-of-the-art methods RL methods for LLMs\n\nObservation: Please revise the sentence construction. The paper needs edits and revisions before publication. \n\n11. L441-L442: Our objective is enable the iteration and development of more effective methods for language based, multi-turn interaction tasks.\n\nObservation: Correction of the phrase ‘is enable’ to ‘is to enable’ should be done"
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper highlights that current LLMs are trained to imitate golden responses rather than genuinely learning to reason and solve single-turn tasks. Additionally, there is a lack of benchmarking for multi-turn RL tasks, along with the absence of established evaluation protocols, which can be costly. To address this, the authors synthesize a benchmark that leverages the imitation capabilities of language models in conjunction with simulators, such as chess engines. They propose the LMRL-GYM benchmark, which comprises three interactive dialogue tasks and five RL capability tests, benchmarking existing RL methods, including offline methods like ILQL and online methods like PPO, among others.",
        "strengths": "The paper raises a significant question regarding the benchmarking of different RL algorithms in multi-turn scenarios and introduces the LMRL-GYM benchmark, which consists of several tasks designed for evaluation. It assesses a diverse range of RL algorithms while also providing a comprehensive evaluation framework.",
        "weaknesses": "1. The real-world tasks included in the benchmark are not sufficiently representative, as they only incorporate three tasks that focus on abilities such as persuasion and information gathering.\n2. The dataset construction appears somewhat unconvincing. For the interactive dialogue tasks, authors initially use two GPT-3.5 models to generate the dataset and then train two FLAN-T5-XL models to imitate the guesser and oracle roles. Since these are relatively small models, the resulting dialogues may lack diversity and representativeness. The reliability of the benchmarking results for various RL algorithms raises concerns. While the authors conducted a user study to assess the naturalness of the synthesized datasets, I remain skeptical about the benchmark's overall naturalness.\n3. The RL ability benchmark, which consists of five tasks, has a limited action space, deviating from real-world scenarios that utilize RL with much larger action spaces, such as step-wise scoring for tasks like math or code generation.\n4. The experiments are conducted with small models; is the benchmark applicable to larger models? Since small models can achieve nearly 100 rewards on some tasks (as shown in Table 2), this may impact the significance of the benchmark.\n5. In Table 2, the performance of GPT-4 prompting is significantly worse than that of the RL algorithms on the RL capability tasks, despite GPT-4 also being trained using RL methods. Can you comment on this?\n6. The right side of Table 1 extends beyond the page margin, and some tables in the appendix exhibit the same issue."
      }
    ],
    "rating_avg": 4.25,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "This paper presents a new benchmark to evaluate LLM agents in a dialogue setting. An agent interacts with an LLM (a proxy for a human) to engage in a dialogue to solve an RL task. \n\nStrengths:\nThis is an important and interesting task, and one that is surprisingly overlooked in LLM benchmarks. The more common tasks is for the LLM to generate a single response and get reward for it, or to take symbolic actions in multi-turn setting. However, reviewers raised several concerns regarding the benchmark:\n\nWeakness:\n1. Tasks are somewhat simplistic; not all are natural dialogue tasks. This was noted by the reviewer j29V and 9eFK. I personally found car dealer as an example of a good task and it would have been great to have more real-world tasks like it (e.g., hotel recommendation, flight booking).\n\n2. Evaluations are restricted to GPT2 models. I understand that only a few labs can train GPT-4 or even 70B models, but GPT-2 is at this point quite outdated. Even a 2B or 3B model would have been nice.\n\nOverall, I like this direction but I think this needs more work. At a minimum, either more real-world tasks or experiments with bigger models would be needed. Alternatively, authors can focus more on the human evaluation of LLM agents. For now, I am recommending a weak reject, but I wouldn't mind if the paper was accepted.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "XLMAMmowdY",
    "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
    "authors": [
      "Renxi Wang",
      "Xudong Han",
      "Lei Ji",
      "Shu Wang",
      "Timothy Baldwin",
      "Haonan Li"
    ],
    "abstract": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs",
    "keywords": [
      "Agent",
      "Tool Learning",
      "Virtual Token"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=XLMAMmowdY",
    "forum_url": "https://openreview.net/forum?id=XLMAMmowdY",
    "reviews": [
      {
        "rating": "5",
        "confidence": "5",
        "summary": "This paper proposes ToolGen, a finetuned LLMs that can use various tools during the conversations with the users. ToolGen incorporates new 47K tokens for tools into the Llama-3-8B. Through the tool virtualization, tool memorization, retrieval training, and end-to-end agent tuning, ToolGen correctly select the right tools in the context on  the ToolBench evaluation, achieving better performance than retrieval and end-to-end baselines.",
        "strengths": "- [S1] ToolGen outperform or achieves competitive performance among retrieval and end-to-end baselines on ToolBench.\n- [S2] ToolGen can natively invoke 47K tools following the context.",
        "weaknesses": "- [W1] The technical novelty is limited. Using special tokens for tools and incorporating them into the original vocabularies are widely-known approach (e.g. Toolformer: https://arxiv.org/abs/2302.04761, ToolkenGPT: https://arxiv.org/abs/2305.11554). The contribution of this paper is scaling this up to 47K tools, but it's very straight forward and I'm not confident if the ICLR community would be interested in it.\n- [W2] Releted to [W1], the results of ToolLlama-3 in Section 5 is unclear to me. Why ToolLlama-3 is not as good as ToolGen, even using the same data and models? Could you clarify the difference between two?\n- [W3] How about retrieving tools by LLMs itself (without retriever)? For instance, if we use the LLM with millions of extremely long context (such as Gemini), we may not need to rely on neural retrievers. It would be interesting to include long-contect LLMs as a baseline.\n- [W4] It would be important to compare other capability of LLMs before/after tool token incorporation. Current draft seems to lack the analysis after combining 47K tokens to LLMs.\n- [W5] In Figure1, I didn't understand the distinction between ToolGen's Retrieval Task and Agent Task. Both have the input \"I want some popular video games.\" How are they differentiated?"
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper introduces ToolGen, a novel framework that aims to enhance the interaction between large language models (LLMs) and external tools. ToolGen shifts away from traditional tool retrieval methods and instead integrates tool knowledge directly into the LLM's parameters. This is achieved by representing each tool as a unique token, allowing the LLM to generate tool calls and arguments seamlessly as part of its text generation process.",
        "strengths": "1. ToolGen elegantly combines tool retrieval and execution into a single generative process, eliminating the need for separate retrieval mechanisms. This streamlines tool interaction and enhances efficiency, particularly as the number of tools increases.\n2. The use of constrained beam search during inference effectively restricts the output to valid tool tokens, significantly reducing the generation of nonexistent tools, a common issue in LLM-based agents.\n3. ToolGen demonstrates its capacity to effectively handle a large repository of over 47,000 real-world tools, highlighting its scalability compared to existing methods that struggle with vast tool sets.\n4. The authors spend good amount of effort comparing different indexing method, and the result is clear.",
        "weaknesses": "1. The advantage of ToolGen which combines tool retrieval and execution into a single generative process introduces limitation together with its efficiency. Since the tools are integrated into the system as tokens, the extension of new tools become inefficient. For every new tool/API, new token need to be added and the documentation finetuned into the model. Also, consider the case that when the tool/APIs get updated, the maintenance of all the tool/APIs, making sure they are up to date is a quite challenging task. On the contact, with a retriever would make adding and maintaining tool/APIs very easy."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper introduces ToolGen, a novel framework designed to address the limitations of LLMs in interacting with an extensive array of external tools. This framework shifts existing paradigms by embedding knowledge about various tools directly into the LLM’s vocabulary, using unique tokens to represent each tool. This allows the model to generate tool calls and arguments as part of the language generation process, eliminating the need for separate tool retrieval systems and seamlessly integrating tool usage within LLM capabilities. ToolGen incorporates a three-stage training process to enable the LLM to learn how to use tokenized tools.\n\nThe main contribution of this paper is introducing the ToolGen framework including training and inferencing, and proving that ToolGen have comparable result with current tool retrieval systems.",
        "strengths": "**Originality:**\nThe paper introduces a novel approach to tool retrieval by representing each tool as a unique virtual token directly integrated into the LLM’s vocabulary. This method eliminates the need for auxiliary retrievers, making the retrieval process more seamless and efficient. The concept of transforming tool retrieval into a generative task is novel and presents a possible solution to the scalability challenges faced by some existing methods.\n\n**Quality:**\nThe authors have conducted a comprehensive set of experiments to validate their claims, showing thoroughness in their evaluation. These experiments include various comparisons and detailed ablation studies to assess the impact of different components of their approach. The robustness of their methodology enhances the credibility of their results and conclusions.\n\n**Clarity:**\nThe paper is well-structured and written in a clear and concise manner. It systematically outlines the problem, the proposed solution, and the experimental validations. Each section transitions smoothly into the next, making it easy for the reader to follow the logic and understand the contributions of the work.\n\n**Significance:**\nThe research addresses a critical issue in the field of LLM agents: managing and retrieving tools from a large set of tools efficiently. Given the increasing complexity and number of tools available, the proposed method provides a solution. This work has implications for the development of more autonomous and efficient AI systems, potentially benefiting some applications where tool interaction is essential.",
        "weaknesses": "**Substantive Assessment of Weaknesses:**\n\n**Cost and Efficiency Claims:**\nThe key claim of \"significantly less cost and higher efficiency\" does not hold up under scrutiny. The authors have not substantively demonstrated that their framework is less costly or more efficient than existing methodologies. The ToolGen framework necessitates a three-stage training process, which does not inherently suggest reduced costs. Furthermore, the paper lack data or experiments to substantiate the claim regarding efficiency improvements over other approaches.\n\n**Role of the Memorization Stage:**\nIn Section 4.4, the table shows that the memorization stage plays a relatively minor role in the three-stage training process. However, the authors assert that this stage is beneficial for generalization, with further discussion supposedly found in Appendix F. There is, however, no such discussion present in Appendix F. I would recommend authors to do more experiments to validate the significance of the memorization stage and its impact on generalization.\n\n**Hallucination Comparison:**\nSection 5.4 contains statements that do not fully make sense. The claim that ToolGen experiences no hallucination is primarily due to the use of a constrained decoding strategy. Theoretically, it is impossible to encounter hallucination, as defined by the authors, under this constraint. This does not constitute a fair comparison with other frameworks. The authors should either provide results without this constraint and compare them or demonstrate why other frameworks are unable to implement a similar strategy.\n\n**Performance Benchmarking:**\nToolGen is not currently the best-performing framework. Several baselines in the paper outperform ToolGen in various categories, as shown in Table 1 and Table 4. Notably, even the proposed but ultimately unused semantic tokenization approach outperforms the authors' current method, as shown in Table 2. This discrepancy confuses me."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "This paper proposes ToolGen, a generative tool/function calling framework. The concrete methods are: (1) virtualizing tools by virtual tokens; (2) memorizing tools with training data of (tool docs, tool virtual tokens); (3) learning tool retrieval with training data of (user queries, tool virtual tokens); (4) and finally, finetuning tool agent with tool calling trajectories. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.",
        "strengths": "Overall, I like this paper very much.  \n\n1. ToolGen is now paradigm for fundamentally transforming tool retrieval into a generative process, which is a very important topic for function calling or LLM-based agents.  \n2. The methods are sound and resonable; the paper presentation is clear.  \n3. The experimental results show that the methods are effective compared with several strong baselines.",
        "weaknesses": "1. Traditional retrieval and generation methods for function calling can handle dynamic tools. If the tool set is changing, could ToolGen be used (without retraining)?  \n\n2. A few reltated works are not mentioned or compared. For example, TPTU and TPTU-V2 [1,2,3,] used demo retriever and fintuner besides the tool retrieval, which may be more powerful than the traditional retrieval and generation methods. They can be a strong baseline for comparision.  \n\n3. It seems that ToolGen uses a more complex process (i.e., (1) virtualizing tools by virtual tokens; (2) memorizing tools with training data of (tool docs, tool virtual tokens); (3) learning tool retrieval with training data of (user queries, tool virtual tokens); (4) and finally, finetuning tool agent with tool calling trajectories) to prepare for tool calling. Therefore, the suprior of ToolGen may come from more training of LLMs? Could the authors give more explination on this?\n\n\n[1] TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents. FMDM Workshop at NeurIPS 2023.   \n[2] TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems. LLMAgents Workshop at ICLR 2024.  \n[3] TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Industry Systems. EMNLP 2024 Industry Track."
      }
    ],
    "rating_avg": 5.75,
    "confidence_avg": 3.75,
    "decision": "Accept (Poster)",
    "meta_review": "The paper introduces ToolGen, a novel framework that integrates tool knowledge directly into the parameters of a Large Language Model (LLM) by representing each tool as a unique token.  This allows the LLM to generate tool calls and arguments seamlessly, eliminating the need for separate tool retrieval systems.  ToolGen is trained in three stages: tool memorization, retrieval training, and end-to-end agent tuning.  The authors claim this framework enhances the interaction between LLMs and external tools, particularly with large tool sets.    \n\nStrengths: The paper proposes a novel approach to tool retrieval and execution by representing tools as virtual tokens, streamlining the process.  The use of constrained beam search effectively reduces the generation of nonexistent tools.  The authors demonstrate the scalability of ToolGen by incorporating over 47,000 real-world tools, and how it can more easily deal with context length limitations and tool hallucinations.\n\nWeaknesses: (1) Dealing with new tools at test-time or major updates in tool APIs would necessitate retraining, which has been added as a discussion in appendix.  (2) Degradation in general instruction following performance of LLMs when fine-tuning heavily on tool use trace, which would be critical for practical deployment. (3) Post-hoc tool use is common for deployment due to latency introduced by tool calls during generation, which might need to carefully handled with ToolGen.\n\nReasons for Acceptance/Rejection: The paper present a novel perspective on tool use -- generative tool calling - which can tap into the generation abilities of LLMs. Moreover, the authors have addressed the reviewers' concerns by providing additional explanations and committing to further experiments. The revised version of the paper, incorporating the planned improvements, is expected to be a valuable contribution to ICLR.\n\nMinor suggestion: Work in similar spirit to ToolGen has been going on in generative verifiers / reward models (GenRM, Cloud) which would be worth discussing.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "wLR9d5ZFpY",
    "title": "No Training Data, No Cry: Model Editing  without Training Data or Fine-tuning",
    "authors": [
      "Dhruva Kashyap",
      "Tanay Narshana",
      "Chaitanya Murti",
      "Chiranjib Bhattacharyya"
    ],
    "abstract": "Model Editing(ME)--such as classwise unlearning and structured pruning--is a nascent field that deals with identifying editable components that, when modified, significantly change the model's behaviour, typically requiring fine-tuning to regain performance.\nThe challenge of model editing increases when dealing with multi-branch networks(e.g. ResNets) in the data-free regime, where the training data and the loss function are not available.\nIdentifying editable components is more difficult in multi-branch networks due to the coupling of individual components across layers through skip connections. \nThis paper addresses these issues through the following contributions.\nFirst, we hypothesize that in a well-trained model, there exists a small set of channels, which we call HiFi channels, whose input contributions strongly correlate with the output feature map of that layer.\nFinding such subsets can be naturally posed as an expected reconstruction error problem. To solve this, we provide an efficient heuristic called RowSum.\nSecond, to understand how to regain accuracy after editing, we prove, for the first time, an upper bound on the loss function post-editing in terms of the change in the stored BatchNorm(BN) statistics.  With this result, we derive BNFix, a simple algorithm to restore accuracy by updating the BN statistics using distributional access to the data distribution.\nWith these insights, we propose retraining free algorithms for structured pruning and classwise unlearning, CoBRA-P and CoBRA-U, that identify HiFi components and retains(structured pruning) or discards(classwise unlearning) them. CoBRA-P achieves at least 50% larger reduction in FLOPS and at least 10% larger reduction in parameters for similar drop in accuracy in the training free regime. In the training regime, for ImageNet, it achieves 60% larger parameter reduction. CoBRA-U achieves, on average, a 94% reduction in forget-class accuracy with a minimal drop in remaining class accuracy.",
    "keywords": [
      "pruning",
      "model editing",
      "classwise unlearning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=wLR9d5ZFpY",
    "forum_url": "https://openreview.net/forum?id=wLR9d5ZFpY",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper addresses the problem of model editing (specifically, structured pruning and class unlearning) for deep neural networks when training data is not inaccessible. The authors propose the concept of \"HiFi components\", which are identified as a small subset of channels in each layer being responsible for the model's output. Detecting \"HiFi components\" could be solved by measuring the reconstruction error of these channels. However, due to the unavailable training data, the authors propose a heuristic \"RowSum\" to identify the similarity between distributions of input contribution and output feature map in a layer. Then HiFi components are the components having a high correlation(/similarity) between input channel contributions and the output feature map. To restore the model's accuracy after editing, the authors derive an algorithm called \"BNFix\" to update BN's statistics using only distributional access to the data distribution. Two algorithms COBRA-P and COBRA-U are proposed to find whether retaining or discarding HiFi components in pruning and unlearning, respectively. Empirical evaluations on CIFAR-10/100 and ImageNet datasets show the effectiveness of their approach in maintaining competitive accuracy.",
        "strengths": "1. The paper tackles the problem of model editing without accessible training data for the circumstances of structure pruning and class unlearning. \n\n2. Identifying the HiFi component with the proposed correlation measure is interesting to me.",
        "weaknesses": "1. While the concept of HiFi components is interesting, the technical novelty of the RowSum heuristic and BNFix algorithm appears limited. There are many papers proposing to update BN's parameters, a similar strategy to the one in this paper. \n\n2. The theoretical analysis focuses on providing upper bounds on the loss function, however, K is the largest eigenvalue of the hessian, which might not be tight enough as a guarantee. \n\n3. The overall writing and organization of the paper could be improved significantly. The presentation of the main framework and the transition between different concepts in sections should be intuitive."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper mainly focuses on the model editing task, emphasizing the setting without training data or loss functions.\nTo detour access to the data or loss functions, the authors investigate the 'distributional' behavior of network layer outputs, which is not a 'sample-wise' behavior. Based on the finding that a very limited number of components of networks contribute to the learned outputs (called **HiFi** components), the authors have proposed to freeze the HiFi components and adjust the batch normalization to compensate for the changes in the distributional behavior. To verify their approaches, they have provided two types of tasks, i.e., pruning and unlearning.",
        "strengths": "**Strength 1:** The main strength of this paper is that the authors' viewpoint to scrutinize the distributional behavior of networks rather than the sample-wise network sensitivity can be a key strategy to control or edit the learned models.\n- The strategy seems to be widely applied to various long-aged problems across multiple related societies, e.g., continual learning, explainability, and pruning or unlearning, which are tested in this paper.",
        "weaknesses": "**Weakness 1:** Limited understanding of how the learned knowledge relates to the distributional behaviors of models\n- The main weakness of this paper is the limited understanding of how keeping the HiFi part results in keeping the knowledge of learned models. Otherwise, how tuning the HiFi part results in forgetting the specific learned knowledge.\n- At the conceptual level of understanding, it is quite convincing that the components showing similar distributional behaviors with the layer outputs are probably the crucial parts of the knowledge. However, it is not guaranteed theoretically. \n\n**Weakness 2:** Insufficient quality of presentation and writing\n- I strongly believe this venue requires the highest presentation and writing quality. However, the submitted version contains too many grammar errors, unpolished sentences, and low-clarity visualizations, as follows:\n- At line 47: a missing full name of 'CNN'\n- At many parts: add a whitespace between text and '('\n- At many parts: for citations, the form is inconsistent, e.g., at line 166, \"behavior (Jia...; Shah et al., (2024)).\" is correct.\n- At line 178: missing comma after i.e.\n- At line 185: missing whitespace before \"While\"\n- Figure 2: The size is too small to recognize the plots, formulations, and texts.\n- Equation 3: it is better to keep the length within the text width of the page.\n- At line 269: keep the name \"HiFi\"\n- At line 328: It seems \"Assumption 5\" means A1 and A2 at the right upper part. The labeling of assumptions is not matched.\n- At line 469: missing punctuation after \"Training Details\"\n- At line 529: \"loss\" rather than \"Loss\"\n- Figure 5 (in Appendix): The size is too small to recognize the contents.\n- I strongly feel that the level of presentations and writing is not reaching the level of this venue.\n\n**Weakness 3:** Limited comparison with other related works\n- Although the authors have provided the 'Related Work' part in the Appendix, it seems insufficient to provide deep insights into this work beyond others.\n- For instance, beyond the technically similar model editing methods, in-depth analysis of the prior works investigating the importance of weights or sensitivity measures of weights should be considered. I think that HiFi is another viewpoint to measure the importance of weights so that it has the potential to show further impact on continual learning (also without data of the past tasks) and explainability."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper deals with the finetuning-free model editing of ResNet models without accessing the original training data. The authors hypothesize that High Fidelity (HiFi) components of the model take charge of overall performance retainment and propose determining the pruning parts from a model based on the reconstruction score. The authors further provide a novel theoretical analysis of the batch normalization statistic to characterize the model performance after editing. Evaluation was performed over model pruning and class-level unlearning tasks.",
        "strengths": "* This paper provide a novel theoretical analysis on batch normalization statistics to discuss post-edited model performance",
        "weaknesses": "* **Limited applicability of the proposed method**\n  * Although ResNet models are still popular in some cases, given that Vision Transformer (ViT) or other transformer-based models are dominant in many applications, the aim of this study limits its impact compared to previous work on model editing [1].\n  * Could the insights provided in this work have some implications for the transformer-style models?\n* **Limited validation scope**\n  * Although this paper provides some theoretical insights, the empirical validation is too weak in terms of \n the number of baseline methods, datasets, and experimental settings.\n  * Could more baseline methods for the unlearning task be considered? Either data-free [2] or not [3].\n  * Could more datasets be considered here for the unlearning task?\n* **Insufficient empirical advantage**\n  * The authors claim that the proposed method achieves a good trade-off between accuracy and efficiency. However, the proposed method actually could not achieve good accuracy compared to baseline methods, and the benefits of enhanced efficiency are also not so strong on both pruning and unlearning tasks.\n* **Reliance on external data (through distributional access)**\n  * Although the proposed method does not use an explicit training dataset on which the mode is trained, it still requires some samples from a similar distribution. This weakens the practical usefulness of the proposed method compared with truly data-free methods such as task arithmetic-based unlearning [2]\n  * Could the authors provide an ablation study for the size of the external dataset used for proposals?\n* **Bad presentation quality**\n  * In the introduction and experiment section, the author does not insert space between paragraphs, which makes the reading hard.\n  * The quality of the figure and table is so bad in terms of font size and resolution.\n  * There is incorrect labeling of assumption 5 in line 328\n  * Notations are complex beyond need and somewhat unclear. One example is lines 177-178.\n\n\n\n> Reference\n1. Decomposing and Editing Predictions by Modeling Model Computation, Shah et al. 2024\n2. Editing Models with Task Arithmetic, Ilharco et al. 2024\n3. Decoupling the Class Label and the Target Concept in Machine Unlearning, Zhu et al. 2024"
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.3333333333333335,
    "decision": "Reject",
    "meta_review": "This paper tackles the challenge of model editing, specifically structured pruning and class unlearning, for deep neural networks when the training data is inaccessible and the loss function is unknown. The authors introduce the concept of \"HiFi components,\" a small subset of channels within each layer identified as being crucial to the model's output. Their approach involves freezing the HiFi components and adjusting batch normalization to compensate for changes in distributional behavior. To validate their method, the authors present two use cases: pruning and unlearning.\n\nThe paper's strengths lie in the novel setup and the introduction of HiFi components, which offer an intriguing perspective. However, the work has notable weaknesses, including limited empirical validation. Specifically, (1) the effectiveness of the learned importance weights compared to prior methods is not sufficiently demonstrated or interpreted, and (2) the proposed method's performance is not adequately benchmarked against baselines in contemporary Transformer-based architectures.\n\nGiven these limitations, I recommend rejecting this submission.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "J5s6EG6ual",
    "title": "Investigating Self-Attention: Its Impact on Sample Efficiency in Deep Reinforcement Learning",
    "authors": [
      "JIANXIAO SUN",
      "Dorvin Ong",
      "Bu-Sung Lee"
    ],
    "abstract": "Improving the sample efficiency of deep reinforcement learning (DRL) agents has been an ongoing challenge in research and real-world applications. Self-attention, a mechanism originally popularized in natural language processing, has shown great potential in enhancing sample efficiency when integrated with traditional DRL algorithms. However, the impact of self-attention mechanisms on the sample efficiency of DRL models has not been fully studied. In this paper, we ponder the fundamental operation of the self-attention mechanism in visual-based DRL settings and systematically investigate how different types of scaled dot-product attention affect the sample efficiency of the DRL algorithms. We design and evaluate the performance of our self-attention DRL models in the Arcade Learning Environment. Our results suggest that each self-attention module design has a distinct impact on the sample complexity of the DRL agent. To understand the influence of self-attention modules on the learning process, we conduct an interpretability study focusing on state representation and exploration. From our initial findings, the interplay between feature extraction, action selection, and reward collection is influenced subtly by the inductive biases of the proposed self-attention modules. This work contributes to the ongoing efforts to optimize DRL architectures, offering insights into the mechanisms that can enhance their performance in data-scarce scenarios.",
    "keywords": [
      "self-attention",
      "sample efficiency",
      "reinforcement learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=J5s6EG6ual",
    "forum_url": "https://openreview.net/forum?id=J5s6EG6ual",
    "reviews": [
      {
        "rating": "1",
        "confidence": "5",
        "summary": "This paper claims to investigate the sample efficiency of self-attention mechanisms in image-based reinforcement learning. The paper mentions that they compare different types of scaled dot-product attention. However, the paper suffers from multiple major flaws including the model architecture as well as the choice of the self-attention operations used for experiments.",
        "strengths": "- The paper attempts to analyze different types of self-attention layers. The motivation is good.",
        "weaknesses": "- The paper uses a self-attention layer within a couple of CNN layers as their model architecture. This is a very shallow unusual architecture for any visual representation learning. In Transformers, it is a very common practice to have a series of MHSA layers and MLP layers interleaved. At least use ViT? The current architecture used for the investigation is extremely limited and no concrete conclusions can be made using them.\n\n- The paper mentions that they investigate different self-attention. However, the selection of the attention mechanism to compare seems highly arbitrary. The operations selected in this paper include Spatial-wise-Attention, Channel-wise-Row(/Column)-Attention, and so on, but I have not seen any major state-of-the-art models using these arbitrary attention mechanisms in its architectures. Are these being used in any of today’s large vision-language models? What’s the point of comparing something that’s not being picked up by anyone? It would have been much more meaningful to compare different types of Transformer components that’s actually being used in practice, such as linear attention mechanisms like Performer and sequential models like Mamba2, which are much more of interest to the general audience.\n\n- The observations from the experiments seem inconclusive. It is very difficult to conclude from these limited experiments which do not show any major trend."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This work investigates the performance differences on ALE games with different self-attention operations for visual reinforcement learning. The self-attention operations include (1) direct self-attention on the feature map, (2) self-attention on the height dimension, (3) self-attention on the width dimension, and (4) the sum of (2) and (3). The conclusion is that different attention operations have different impacts on different games, which is brought about by different game mechanics.",
        "strengths": "1. This paper investigates an interesting topic after Manchin et al., examining whether different attention operations can impact visual DRL learning differently.\n\n2. This paper evaluates 56 games within the ALE benchmark, ensuring the broad applicability of the results, and uses stratified bootstrap confidence intervals for reliable assessment. Compared with Manchin et al., the experiment uses 56 games and 5 seeds, which enhances the experimental results.\n\n3. The paper designs and contrasts four types of self-attention modules (SWA, CWRA, CWCA, CWRCA), revealing how each module’s inductive biases affect learning efficiency in specific environments.\n\n4. The paper is brief and straightforward.",
        "weaknesses": "1.  After reading the entire paper, I still struggle to understand the connection between the terms “sample efficiency” and “attention operation” in this paper. To my understanding, the attention operation only makes differences in network structures. It makes sense that different network structures bring different learning performance. However, “sample efficiency” relates to choosing different transition pairs from the replay buffers or the experience batches. Could you explain more about this part?\n\n2.  The related work section should contain more references. Three papers are not enough; you should investigate and survey more.\n\n3. The types of self-attention are far more varied than different channel operation sequences. More investigations are required if the author aims to make significant and fundamental contributions to the ongoing efforts to optimize DRL architecture.\n\n4. This paper's insights are poorly developed, lack theoretical analysis, and rely merely on limited observations. It fails to provide adequate insights for the community. E.g., “Different attention operations have different impacts on different games” cannot guide the community. Conclusions like “height dimension self-attention provides more performance improvement on vertically moving games” would be more beneficial.\n\n[1] Mott, Alexander, et al. \"Towards interpretable reinforcement learning using attention augmented agents.\" In *Advances in Neural Information Processing Systems*, 2019."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper investigates the impact of self-attention when improving the sample efficiency in DRL. Specifically, the investigation focuses on how different types of scaled dot-product attention affect the performance in ALE. The methods are categorized by over which channels the dot product is applied, including SWA, CWRA, CWCA and CWRCA. In particular, the author discovered the effect of inductive biases of the self-attention modules, such as attending to objects movement horizontally or vertically can be rewarded in different game environments. The author conclude that self-attention modules have different effect to the interplay between the inductive bias and the game mechanics.",
        "strengths": "1. The topic is quite interesting, as it connects NLP and DRL, and self-attention has also attracted lots of attention in developing more advanced RL algorithms nowadays.\n\n2. The setup of experiments is quite thorough and well-thought, especially with the evaluation metrics and state presentations.\n\n3. The results are quite interesting and give us new perspective about the importance of self-attention. Meanwhile, I found te explanations of those results satisfactory.",
        "weaknesses": "1. The author did not give information about the choices of hyper-parameters or any ablation study regarding the network structure. In the experiment section, only the experimental setup and the evaluation methodology are discussed. This can be a problem.\n\n2. Following point 1, what kinds of steups for self-attention modules have you tested? Have you tried to vary the number of self-attention layers and the heads? I think it is crucial to study all different aspects of MHSA as well. This is because MHSA is more applicable in NLP and Vision Transformers than simplex self-attention operations."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.333333333333333,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "GamwMdPj0y",
    "title": "C-Adam: Confidence-Based Optimization for Online Learning",
    "authors": [
      "Shaowen Wang",
      "ANAN LIU",
      "Jian Xiao",
      "Huan Liu",
      "Yuekui Yang",
      "Cong Xu",
      "Qianqian Pu",
      "Suncong Zheng",
      "Wei Zhang",
      "Jian Li"
    ],
    "abstract": "Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum ($m_t$) and adaptive learning rate ($v_t$). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV).",
    "keywords": [
      "Optimization Algorithm",
      "Online Learning",
      "Recommendation Systems"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=GamwMdPj0y",
    "forum_url": "https://openreview.net/forum?id=GamwMdPj0y",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a simple heuristic modification of Adam to enhance its ability to handle distribution shifts and sample noise. The modification is skipping the update of Adam if the inner product of the momentum and the gradient is negative. The performance of the algorithm is theoretically analyzed based on (Reddi et al 2018). Experiments demonstrate better performance compared to Adam and other baselines.",
        "strengths": "The writing of the paper is clear, but besides that, to be very honest, I don't think any aspect of this paper is strong by the iclr standard.",
        "weaknesses": "This paper is in my opinion another quite straightforward hack on Adam. Since Adam was proposed ten years ago there have been so many hacks on it, but it's hard to say how much they really moved the field forward. I can see at least two issues behind this, which this paper also suffers from. \n\n- As Adam itself is heuristic, it is natural for someone to come up with many heuristic modifications based on intuitions. But do these intuitions actually reflect what's going on in the deep learning practice? No one really knows, which makes the foundation of these works quite shaky. \n- It's also not hard in general to cook up some settings where the proposed hacks can help, but do they *always* help? Answering this requires very comprehensive testing which the hacking papers typically lack. \n\nAn acceptable paper of this type needs to stand out in at least one of the two dimensions. \n\nRegarding this particular paper, I would say the proposed hack is not surprising given the intuition the authors stated (\"it's bad to have momentum and gradient pointing to different directions\"), but I'm not convinced of this intuition, especially due to the stochastic nonconvex nature of deep learning optimization. The experiment settings are somewhat artificial, and the actual performance gain in the experiments is marginal. I'm also not convinced that the theoretical analysis adds sufficient value to the paper, as it doesn't show how the proposed hack *improves* Adam, let alone the known limitations of (Reddi et al 2018) itself which the paper builds on. \n\nI'm not completely denying the value of this paper, as some readers may still find it useful for their applications. But for a \"competitive\" conference like iclr the paper is quite far from enough."
      },
      {
        "rating": "5",
        "confidence": "2",
        "summary": "The authors of this paper propose a simple heuristic modification to the Adam algorithm to improve its performance when there is a shifting data distribution or noisy data — two common challenges in online learning for ad models/recommendation systems. The proposed algorithm, CAdam, checks if each coordinate of the gradient and the exponential moving average over the gradient share the same sign, and if not, stops updating the parameters until they align again. They also present a regret bound, seemingly closely following previous work on Adam. The authors evaluate CAdam on synthetic and real world data, including a live recommendation system, comparing against other common optimizers.",
        "strengths": "The core strength of the paper is the simplicity of the proposed modification to Adam, making it easy to deploy and reason about. Moreover, the problem of online learning under data distribution shifts and noisy data is a common problem in many real-world applications. If this simple modification can significantly improve performance under these conditions this would likely be of interest for the community.\n\nThe evaluation with A/B testing in a live recommendation system is also a promising way of demonstrating the practical benefit of the method, even if these results are not reproducible.",
        "weaknesses": "I’m leaning towards recommending rejecting the paper in its current form because of concerns about the experiments and the presentation.\n\nMy main concerns and questions regarding the experiments are:\n1. I assume that in Table 1 you only show results for a single seed? Since the difference between the proposed methods and the baselines are very small, it seems crucial to make sure that the difference is not just due to random variation. I suggest running the experiment for multiple (e.g. 5) random seeds and also adding standard errors to the table. Also, when two numbers are exactly equal, you seem to only print the one corresponding to your methods in bold (e.g., WideDeep column, AMSGrad and CAdam). How do you decide what to print in bold?\n2. Similarly, it would be good to show runs with multiple random seeds for the CNN image classification experiment (Figure 3 + 4).\n3. Regarding the real world recommendation system results, I find it a bit hard to put them into perspective. While the scale of the experiment is impressive, it is hard to evaluate how significant the performance gains are. Is there a way to put them into context, e.g. by comparing the gains to other previous interventions? In any case, at least the gains seem to be consistent.\n\nMy main concern regarding the presentation is that despite the apparent simplicity of the algorithmic modification, its description is surprisingly unclear: in Algorithm 1, all quantities seem to be vectors, so the modification in line 14 implies that a dot product between $g_t$ and $\\hat{m}_t$ is computed and, if it is smaller or equal to zero, $\\hat{m}_t$ is set to the zero vector. However, the verbal description is talking about pausing the update for a single _parameter_. Also, equation (6) in appendix A is clearly stating that the comparison is applied coordinate-wise, effectively checking whether the sign of each coordinate of $g_t$ and $\\hat{m}_t$  is the same. Could you please clarify this and make the description consistent throughout the paper?\n\nIf I can be convinced that CAdam is indeed a very simple way to improve performance in online learning tasks such as CTR prediction and recommendation systems and the presentation is improved, I’m happy to increase my score.\n\n**Minor comments**\n\n- The figures could be improved by increasing the font size and avoiding covering the letters with plot lines.\n- You could consider condensing the paper a bit more, avoiding repetitive content, and removing sentences with little information."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper introduces Confidence Adaptive Moment Estimation (CAdam). It is a variant of the frequently used Adam optimizer, in which the model parameter update of a given weight is only applied if the current gradient g has the same sign as its exponential moving average m. If this is not the case, only the exponential moving averages m and v are updated. The purpose of this change is to improve the results in online learning if distribution changes occur and/or noisy samples are present. The method is evaluated in various scenarios.",
        "strengths": "- The idea of the CAdam optimizer is simple but clever. It is easy to integrate also in many other optimizers. Hence, the proposed change can be important in the end for a broad community.\n- The paper is well written and structured. The experiments are well chosen to underline the advantages of the proposed method.",
        "weaknesses": "Major:\n- Is the analysis of each experiment based on a single optimization trajectory (especially those in Sections 4.1 and 4.2)? For a reliable and convincing benchmark, multiple optimization trajectories should be started from different random initializations of the model parameters. Maybe I overread this aspect. However, then this should be highlighted more. You can also employ standard deviations of the trajectories to proof that CAdam is really consistently better than Adam.\n- The differences in Table 1 between CAdam and Adam are really minor. For WideDeep, the results are even equal for the given rounded numbers. But still, the CAdam results are highlighted as best and they are promoted to be consistently better. I think you are overselling CAdam here and should adjust the interpretation of your results. (CAdam still shows better performance for noisy online learning tasks, which is an important contribution by itself.)\n\nFigure 3:\n- It is difficult to see the difference between Adam and CAdam. However, there is not much insight if the accuracy is below ~50%. Maybe one could just zoom in the region between 50 and 100%. In addition, there is much white space in the three plots on the right which can be reduced.\n\nFigure 4:\n- See comment on figure 3. Try to highlight the differences of the graphs by zooming in.\n\nText:\n- The references to Figure 3 and 4 are incorrect.\n\nTypos:\n- page 4: \"optimumm\"\n- page 7: \"corrosbonding\""
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a variant of popular optimizer Adam, which cancels a specific batch updating by comparing the sign of $m_{t}$ with the sign of $g_{t}$, seeing line-14 of algorithm 1.\nThis method is intuitive and assumes that the $g_{t}$ is affected by noisy data and should be eliminated if the sign of $g_{t}$ differs from the sign of $m_{t}$.\nRegarding the evaluation, this work conducted three types of small-scope optimization tasks, an image classification task with VGG&CIFAR10 under customized distribution shift and label noise, advertisement tasks with Criteo-x4-001 dataset under different various models and optimizers, and a real-world recommendation system task.",
        "strengths": "(1) Regarding Algorithm 1, momentum term $m_{t}$ is considered a more trustworthy signal than stochastic gradient $g_{t}$ under the distribution shifts or noisy data, which makes sense to me. Then, the alignment between those two signals potentially helps identify between the clear data samples and noisy data samples.",
        "weaknesses": "$\\textbf{Weakness in method motivation}$\n\n(1) The method is intuitive and not supported by theoretical results or experiments that verify the intuition. For instance, given a new random batch $x_{i\\in B}$ and the corresponding ground truth label of clear data points and noisy data points, can you explicitly show that clear data points have the same sign with $m_{t}$ while noisy data points have a different sign with $m_{t}$.\n\n(2) Given algorithm 1 and considering line-5 and line-14, intuitively, the $m_{t}$, i.e., the cumulative behaviors, is considered as trustworthy and is utilized to filter out untrustworthy $g_{t}$ according to the sign of the two values, which make sense somehow. \n\nHowever, $g_{t}$ is sampled from a small batch of data and contains noise naturally. Can you provide more insights into how line-14 of algorithm 1 is related to the natural noise of stochastic gradient?\n\n\n$\\textbf{Weakness in technical contributions}$\n\n(1) This work assumes $g_{t} = \\nabla f_{t}(\\theta_{t-1})$ in the Notations in Section 3. Thus, this work assumes that it is a deterministic optimization problem. Along with the convex assumption, the theoretical result built upon those strong assumptions, i.e., Theorem 1, is less useful to reflect the practical performance of the proposed method.\n\n\n(2) The problem is not well defined. This work mentioned the proposed method has the potential to handle distribution shifts and noise, however, what are the mathematical definitions of the distribution shifts you mentioned? And can you characterize it more formally and connect it with real-world situations? “rotating the data distribution” is confusing. \n\nTo evaluate the robustness of optimizers to noise, well-accepted datasets such as CIFAR-10-C and CIFAR-100-C may be a better choice instead of rotating the images.\n\n\n$\\textbf{Weakness in evaluation}$\n\n(1) As mentioned, the customized “distribution shifts and noise” cannot support the performance improvement of the proposed method.\n\n(2) the small scope of the experiment cannot support the performance improvement of the proposed method. Specifically, most tasks do not apply diverse settings, such as VGG network on the CIFAR-10 for image classification, Criteo-x4-001 dataset for advertisement tasks.\n\n(3) The experiment settings are not well presented. Please refer to the Questions."
      }
    ],
    "rating_avg": 4.25,
    "confidence_avg": 3.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "kn3GT7LbxT",
    "title": "Value Residual Learning For Alleviating  Attention Concentration In Transformers",
    "authors": [
      "Zhanchao Zhou",
      "Tianyi Wu",
      "Zhiyun Jiang",
      "Zhenzhong Lan"
    ],
    "abstract": "Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the $KV$ cache by nearly 50\\%.  Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.",
    "keywords": [
      "Transformer",
      "Self-Attention",
      "Cross-Layer Attention",
      "Residual Learning",
      "Language model",
      "KV cache"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=kn3GT7LbxT",
    "forum_url": "https://openreview.net/forum?id=kn3GT7LbxT",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "Paper proposes SVFormer, a way to reduce the size of the KV cache in Transformers by almost 50%. The authors propose sharing the values from the first self-attention layer across all layers. They find that this outperforms other approaches that reduce the KV cache size and perform extensive ablations to find when SVFormer works.",
        "strengths": "- Paper is straightforward and easy to read. \n- It's interesting that values from the first layer can be used throughout the network for a small loss penalty. \n- Authors thoroughly discusses prior work and explains the contributions of this work. \n- Lots of ablations and experiments.",
        "weaknesses": "- The paper leaves out many important details. See the \"Questions\" section for specifics.\n- Results are not well organized, and appear to have contradictory findings. Fig. 13 (c) in particular shows that SVFormer only outperforms a vanilla transformer when they have 2M parameters, which is very small.  At 82M parameters, SVFormer already is worse than the baseline. Fig. 13 (d), 14, and 15 also indicate that SVFormer hurts loss. However, Fig. 6 shows that SVFormer does better at larger scales\n- I don't like the practice of subtracting the transformer performance and showing the difference. It potentially (a) hides bad baseline performance, and (b) potentially hides the fact that the difference between methods is tiny compared to the overall training loss curve."
      },
      {
        "rating": "3",
        "confidence": "2",
        "summary": "This manuscript presents a novel framework for approximating cross-layer attention. Within this framework, the authors introduce ResFormer as a practical implementation, demonstrating its effectiveness in mitigating attention concentration challenges. In addition, they propose SVFormer within the same framework, which further enhances efficiency by reducing the memory requirements for KV caching, thus lowering overall computational costs.",
        "strengths": "The manuscript proposes a framework for reducing the computational cost of cross-layer attention, offering a unified approach that integrates and extends existing methods, including NeuTRENO and DenseFormer.",
        "weaknesses": "1. The paper lacks discussion of prior work on the attention concentration problem and the connection to the over-smoothing issue addressed by NeuTRENO is unclear. A more detailed review of relevant literature would enhance clarity and better contextualize the impact of this work.\n2. Using training loss as a criterion for comparing model performance is unconvincing (e.g. in Section 4.2, 4.3, 4.6), as it may not accurately reflect generalization. A more reliable evaluation metric, such as accuracy or perplexity on a separate validation set, would provide a clearer assessment of the model's effectiveness.\n3. Minor comments:\n- The term “gold attention matrix” in Section 4.3 should be clearly defined for better understanding.\n- Right margin violated at line 659.\n- Some references list only the first author; please ensure consistency in citation formatting."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper studies the problem of attention concentration in Transformers and proposes solutions that try to approximate cross-layer attention by incorporating the \"value\" from first layer into subsequent layers. There are two solutions: ResFormer that uses residual mapping and SVFormer that uses the same V across all layers. Experiments show that the proposed solutions perform better than baselines on language modeling tasks.",
        "strengths": "- The paper introduces a relatively new and important problem that affects existing Transformer architecture. This is useful towards understanding the dynamics and behavior of Transformers.\n- The proposed solutions only require small changes to existing Transformer architecture. They can be immediately useful for many existing Transformer-based models.\n- The paper provides a good analysis and ablation study on ResFormer and SVFormer that demonstrate their benefits over existing Transformer. Particularly, ResFormer is shown to be achieving higher token importance entropy (i.e., less attention concentration) than traditional Transformer.",
        "weaknesses": "- The authors claim that cross-layer attention is useful at reducing the effect of attention concentration but it is unclear why this would be the case. This work is built on the premise that ResFormer approximates cross-layer attention and thus it is effective against attention concentration. But we do not really know that cross-layer attention provides such a benefit. The author should perform some analysis and/or small-scale experiment on a baseline that actually uses cross-layer attention to check its behavior against that of ResFormer.\n\n- It is hard to disentangle the effects from: (1) reducing attention concentration; (2) ease of optimization in the proposed solutions. Using V in the form of residual mapping (ResFormer) or layer sharing (SVFormer) should make it easier to optimize network parameters during training. It may be possible that the accuracy improvements are largely attributed to the ease of optimization rather than attention concentration reduction. The authors should explain this.\n\n- It would also be interesting to see how well the proposed methods work for non-language tasks and architectures like ViT (image recognition)."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper presents ResFormer and SVFormer, two Transformer model variants that address challenges with deep Transformers, particularly attention concentration where deeper layers focus too much on fewer tokens. ResFormer incorporates a residual connection from the initial layer's values to subsequent layers, thus approximating cross-layer attention without heavy computational costs. SVFormer simplifies further by sharing the value embedding from the first layer across all layers, reducing memory usage by nearly half and accelerating training.",
        "strengths": "1. The paper offers an interesting twist on standard residual connections by applying them specifically to V instead of the usual hidden state H. This approach targets the common issues of over-smoothing and information loss in deep Transformers.\n\n2. SVFormer aims to make Transformer models more efficient by sharing the same value embeddings across layers, reducing memory and computation needs. This design could help make large models faster and more practical for applications with long sequences.",
        "weaknesses": "1. **Problem Definition and Motivation**: The problem of \"attention concentration\" is not clearly defined or sufficiently justified. It is essential for the authors to establish a precise understanding of this issue and clarify why it is a significant challenge within Transformer architectures. Without a thorough introduction and motivation for addressing \"attention concentration,\" it remains unclear what gap this work aims to fill, and the importance of resolving it is left ambiguous.\n\n2. **Novelty and Theoretical Basis**: The proposed approach largely resembles existing residual connections in Transformers, as seen in architectures like ViT and LLaMA. The primary difference with ResFormer appears to be the application of residuals to the value V alone, rather than to the hidden state H as in traditional models. However, this adjustment lacks theoretical grounding and rigorous analysis, especially with regard to the SVFormer, which further simplifies by removing layer-specific values. This simplification seems ad-hoc and trivial, as no theoretical guarantees or insights are offered to support the effectiveness or necessity of such changes.\n\n3. **Experimental Setup and Comparisons**: The experiments are limited and do not provide a thorough benchmark. Although the models are trained on a LLaMA-like architecture, there is no comparative performance evaluation against other prominent Transformer-based or SSM-based models. Furthermore, there are no tests involving visual downstream tasks, which would have strengthened the claims of improvement in Transformers and provided a more comprehensive evaluation across different modalities, especially for encoder-only tasks.\n\n4. **Evaluation of Attention Mechanisms**: An essential part of evaluating any modification to Transformer architectures is understanding how the attention patterns differ from those in the vanilla Transformer. Although the paper discusses attention concentration, it does not provide visualizations or statistical analysis of the multi-head attention weights to demonstrate the proposed method's effect on attention distribution. Such an investigation is critical for validating the claims and understanding how the modifications impact attention dynamics."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "All reviewers converged on rejecting the paper post rebuttal. The AC checks all the materials, and while appreciating the additional efforts including results and analyses and making major modifications to the draft, the AC resonates with the reviewer consensus that the paper currently has issues to address and would benefit from another cycle.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "CCUrU4A92S",
    "title": "Re-examining learning linear functions in context",
    "authors": [
      "Omar NAIM",
      "Guilhem Fouilhé",
      "Nicholas Asher"
    ],
    "abstract": "In context learning (ICL) is an attractive method of solving a wide range of problems.  Inspired by Garg et al., we look closely at ICL in a variety of train and test settings for several transformer models of different sizes trained from scratch.  Our study complements prior work by pointing out several systematic failures of these  models to generalize to data not in the training distribution, thereby showing some limitations of ICL. We find that models adopt a strategy for this task that is very different from standard solutions.",
    "keywords": [
      "In context learning",
      "GPT",
      "limitations"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=CCUrU4A92S",
    "forum_url": "https://openreview.net/forum?id=CCUrU4A92S",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper studies experimentally the setting of in-context learning linear regression . The authors reproduce the experiments of Garg et al and at inference time test the models with 1) different distributions for the input/weight vectors 2) larger values for the input/weight vectors.\nBased on the observations of these results the authors argue that these models do not learn some type of algorithm.",
        "strengths": "Understanding what these models learn even in the setting of linear regression can significantly enhance our understanding of their capabilities limitations. Indeed it has been observed that the models do not generalize in out-of-distribution samples and thus it is unclear whether these models learn some type of algorithm.",
        "weaknesses": "1. The provided experimental study does not explain what these models are actually learning. For example it can be the case that the model are learning a tailor-made preconditioned gradient descent type of algorithm, with the preconditioned matrix being optimal for the in-distribution values and sub-optimal for out-of-distribution values.\n2. It cannot be excluded that the current training methods are not optimal, since we know that these models do have the capability of representing these algorithms. \n3. Some of these results have already been observed experimentally for example see [1] (Figures 5,6).  In these experiments consider multi-dimensional linear regression, they keep all expect for one dimension fixed and plot how the function changes when varying one dimension from [-B,B] similar to the authors' experiments for one dimensional linear regression.\n\nIn general the main weakness of this paper is that it does not make a convincing argument towards what these models are actually learning. \n[1]: Giannou, Angeliki, et al. \"How Well Can Transformers Emulate In-context Newton's Method?.\" arXiv preprint arXiv:2403.03183 (2024)."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "The paper investigates Transformer behavior when trained from scratch to perform linear regression. It examines out-of-distribution (OOD) generalization across various settings, such as different ranges and distributions of linear functions.",
        "strengths": "The paper conducts thorough experiments across various scales and settings, providing a comprehensive analysis of Transformer behavior.",
        "weaknesses": "1. The related work could benefit from a more comprehensive review. The paper primarily discusses the works of Garg et al., Akyürek et al., and Von Oswald et al. on regression for in-context learning (ICL), but there are additional relevant studies in this area that are not cited. A more thorough literature review, covering empirical and theoretical works on regression in ICL, would enhance the paper’s context. Checking recent citations in this line of research may help identify key studies to include.\n\n2. The notation in Section 4 could be clarified, as some symbols are difficult to interpret. For example, it’s not immediately clear what $\\sigma$ represents in the context of $f_{i, \\sigma}$. Additional explanations could help improve readability.\n\n3. The organization of the paper could be refined to improve the overall flow. At times, the presentation feels somewhat informal, with experiments presented in sequence without clear connections, motivations, or in-depth analyses. For instance, it would be helpful if the authors could clarify the rationale for studying models of different scales and discuss what insights are gained from these comparisons. Additionally, mixing experiments on different scales and distributions makes it challenging to understand the primary conclusions. This structure could make it clearer to the reader what the authors aim to convey.\n\nIn general, I appreciate that the authors highlight the out-of-distribution (OOD) generalization issue for Transformers trained on linear regression, as initially noted by Garg et al. However, the experimental findings in Section 4 could be more impactful with clearer motivations and discussions. The hypothesis regarding induction heads and their role in OOD performance is somewhat interesting, though it could be strengthened with supporting theoretical insights or experimental validations, such as through mechanical interpolation. Presenting this hypothesis with additional rigor could provide more substantial contributions to the community."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The study investigates in-context learning (ICL) in transformer models, focusing on their ability to learn and generalize linear functions from contextual prompts. Inspired by previous work, the authors examine various transformer models, including small ones trained from scratch, to explore whether they can learn linear functions and generalize beyond the training distribution.\n\nHowever, there are two main problems in this paper:\n\n### 1. The writing problem: There are many typos, e.g., in ``line 047'', there should be a ''.'' after ''training data''.\n### 2. Novelty: The paper indeed provides robust experiments to show the main point, but it lacks novelty, such as how to improve this problem.",
        "strengths": "The paper has the following strengths:\n\n### 1. Clear Motivation: The paper begins with a well-defined motivation, addressing gaps in the current understanding of in-context learning (ICL) in transformer models, especially for generalization.\n\n### 2. Comprehensive Experiments: The experiments cover various transformer architectures and test them on various distributions.",
        "weaknesses": "The paper has the following weaknesses:\n\n### 1. Clarify Terminology and Notation: The writing is a little poor. For example, in ``line 047'', there should be a ''.'' after ''training data''. Furthermore, the table should be in a more beautiful structure.\n\n### 2. Explanation for the Problem: Although the paper provides various experiments, it should explain the failures of these models to generalize to data not in the training distribution.\n\n### 3. Novelty: The paper provides robust experiments to show the main point but lacks novelty, such as how to improve this problem."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper investigates in-context learning (ICL) across various training and testing scenarios using different sizes of transformer models trained from scratch. Building on previous work, it highlights systematic failures in these models' ability to generalize to data outside the training distribution, revealing some limitations of ICL.",
        "strengths": "1. The paper focuses on an important and challenging problem: understanding the in-context ability of language models.\n2. The writing is clear and easy to understand.\n3. The authors provide code and detailed instructions for reproduction.",
        "weaknesses": "1. The models and empirical studies in the paper differ significantly from current large language models, potentially creating a gap between the claims and reality.\n2. The findings of the paper have been previously proposed in other works.\n3. The paper is missing some key references."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.75,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "irCuIdCdAl",
    "title": "Improving Transformer Interpretability with Activation Contrast-Based Attribution",
    "authors": [
      "Sungmin Han",
      "Jeonghyun Lee",
      "Sangkyun Lee"
    ],
    "abstract": "Transformers have revolutionized AI research, particularly in natural language processing (NLP). However, understanding the decisions made by transformer-based models remains challenging, which impedes trust and safe deployment in real-world applications. While activation-based attribution methods have proven effective in explaining transformer-based text classification models, our findings suggest that they may suffer from class-irrelevant features within activations, potentially degrading the quality of their interpretations. To address this issue, we introduce Contrast-CAT, a novel activation contrast-based attribution method that improves token-level attribution by filtering out class-irrelevant features from activations. Contrast-CAT enhances interpretability by contrasting the activations of input sequences with reference activations, allowing for the generation of clearer and more faithful attribution maps. Our experiments demonstrate that Contrast-CAT consistently outperforms state-of-the-art methods across various datasets and models, achieving significant gains over the second-best methods with average improvements in AOPC and LOdds by $\\times 1.30$ and $\\times 2.25$, respectively, under the MoRF setting. Contrast-CAT provides a promising step forward in enhancing the interpretability and transparency of transformer-based models.",
    "keywords": [
      "Transformer",
      "Interpretability",
      "XAI",
      "Attention",
      "Contrast-based"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=irCuIdCdAl",
    "forum_url": "https://openreview.net/forum?id=irCuIdCdAl",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors propose a new method for identifying which tokens are most influential to a model's prediction. This method, which they call Contrast-CAT, focuses on estimating the per-token influence on the correct class. The method works by trying to remove features from model activations which are supposed to be irrelevant to the class of interest. In their experiments, they demonstrate that this method works well from quantitatively and qualitatively.",
        "strengths": "originality: to my knowledge, this is original work.\nquality: \n* across several standard text classification datasets, the method appears to work well. \n* I was also impressed by the highlighted qualitative examples which show intuitively better attributions compared to existing methods.\n* The comparison to existing methods is also quite thorough. Including ablations on using multiple layers and the \"Same\" vs \"Random\" vs \"Contrast\" experiments were also convincing on the effectiveness of using contrastive reference activations.\n* the evaluation methods seem sound and grounded in methods from prior work.\nclarity: the overall flow of the paper is well-laid out. \nsignificance: attributing language model behavior to individual inputs is an important cornerstone in interpretability. By providing a new improved method for this goal, the contribution is significant.",
        "weaknesses": "Improvements to the clarity of the method are important for the reader's understanding. The ease of understanding is currently a bit lacking from my read; I list specifics in the Questions section below. \n\nRegarding significance, the method and experiments are limited to classification problems using models from the BERT family and GPT2. Since currently the focus in LLMs is on autoregressive models this may be of limited use for interpreting more popular and cutting-edge models in generation use cases, e.g. Llama. If feasible, it would be nice to see how effective this method is for larger models like Llama."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper presents Contrast-CAT, a method enhancing Transformer interpretability in NLP tasks. Contrast-CAT introduces an activation contrasting framework that filters out class-irrelevant features by subtracting reference activations from target activations at each Transformer layer, focusing on class-relevant information. This approach yields precise token-level attribution maps, with experiments showing that Contrast-CAT surpasses current methods in attribution quality and interpretability across diverse datasets and models.",
        "strengths": "● The paper is exceptionally clear, with well-organized explanations and effective visual aids that clarify complex methods and results.\n\n● This paper introduces an original approach with activation contrast, effectively filtering out class-irrelevant features and enhancing interpretability in transformer models.",
        "weaknesses": "● The authors conduct experiments primarily on text classification datasets, which are relatively simple and similar in nature. It remains unclear whether this method is effective for more complex tasks, such as MMLU, MATH, or HumanEval. Exploring these could provide a more comprehensive evaluation of the approach.\n\n● The applicability of this method to decoder-based models is not fully addressed. While the experiments focus on encoder-based transformers (e.g., BERT), with limited GPT-2 results included in the appendix, it would be valuable to see a more thorough evaluation on large language models (LLMs) that have become influential in the research community, such as LLaMA, Mistral, and Qwen.\n\n● If this method is extended to LLMs, it would be helpful for the authors to clarify any potential adjustments or considerations required. Additional details on how this approach might be adapted or optimized for LLMs would enhance the paper’s impact and applicability.\n\nIf these issues are addressed, I will consider raising my score."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper introduces an activation-based attribution method, Contrast-CAT, to attribute the important tokens in text classification task. They compare their method Contrast-CAT with attention-based, LRP-based, and activation-based methods on BERT, where Contrast-CAT outperforms all the other methods.\n\nThe writing and organization of the paper is good.  And the proposed experiments can prove that the proposed method Contrast-CAT achieves better accuracy than the previous activation-based attribution method AttCAT.",
        "strengths": "1.\tThe writing and organization of the paper is good. \n2.\tThe proposed method Contrast-CAT achieves better accuracy than the previous activation-based attribution method AttCAT.",
        "weaknesses": "1. My main concern is the speed of this method. The gradient-based methods are slow because they require many backward computations. If the method requires too many computations, it is very hard to be applied in large language models. \n2. This work lacks the comparison with causal-based methods. To identify the important tokens, an easier and faster method is to replace each token with [MASK] and calculate the probability decrease of the predicted token. And more complicated causal-based methods are commonly used in LLMs, such as [1,2].\n\n[1] Locating and Editing Factual Associations in GPT, 2022\n\n[2] Towards Best Practices of Activation Patching in Language Models: Metrics and Methods, 2023"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a counterfactual explanation method that utilizes the model's activation and reference activation for better token attribution. Specifically, they leverage the conclusion from [1] that the model's prediction confidence for an input token at layer $l$ can be decomposed as the gradient times the subscription of its original activation and a contrastive activation from another token. The author then generalizes this lemma to obtain the attribution for each token by further multiplying the above value with the corresponding attention value. The proposed method achieves promising results in counterfactual explanation, which means it can highlight the most important tokens in a sentence, and removing them will let the model's confidence drop significantly. They use AOPC and LOdds to reflect the ratio of the number of important tokens removed and the confidence difference after removing those tokens, which are common metrics in XAI paper.\n\nRefs:\n\n[1] Sangkyun Lee and Sungmin Han. Libra-cam: An activation-based attribution based on the linear approximation of deep neural nets and threshold calibration. In IJCAI, pp. 3185–3191, 2022",
        "strengths": "- This paper addresses the important problem of explaining how a transformer works by following the line of works in the attribution-based method.\n- The method is tested not only on one transformer model but across a variety of them, including BERTbase, DistilBERT, RoBERTa, and GPT-2, showing its robustness and generalizability across different architectures and datasets. This increases the broader applicability and utility of the approach for a wide range of NLP tasks.",
        "weaknesses": "- The method looks similar to [1] mentioned in the paper with minimal modification. The only thing the author did was multiply the attention value to the conclusion from [1] without a reasonable explanation.\n- AOPC and LOdds results can be affected by many factors, including **(1) how to skip the special tokens like [SEP] and [PAD]** (the author can either not skip any special tokens during the backpropagation but remove them when calculating the normalized attribution scores or skip part of the special tokens during the backpropagation or skip all special tokens during backpropagation can lead to different results) and **(2) how to \"remove\" the important tokens** (replacing the token into [PAD] instead of removing it from the sentence can lead to different results). The author did not clarify these important points in their paper.\n- The author only focuses on the classification tasks, which is not exciting. The author should at least discover more about text generation tasks.\n\nRefs:\n\n[1] Sangkyun Lee and Sungmin Han. Libra-cam: An activation-based attribution based on the linear approximation of deep neural nets and threshold calibration. In IJCAI, pp. 3185–3191, 2022"
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This paper proposes Contrast-CAT, an activation-based attribution method for enhancing the interpretability of Transformer models in NLP tasks, particularly text classification. The method filters out class-irrelevant features by subtracting reference activations from target activations at each Transformer layer. The authors conduct experiments on multiple datasets and models, comparing Contrast-CAT with several existing methods and demonstrating its superiority in attribution quality and interoperability.  However, the concerns of this paper come from several aspects: 1) the authors conduct experiments primarily on text classification datasets, which may limit the understanding of the method's effectiveness for more complex tasks; 2) applicability of the method to decoder-based models is not fully addressed, and it would be valuable to see a more thorough evaluation on large language models like LLaMA, Mistral, and Qwen. 3) the method looks similar to a previous work with minimal modification.  \nAfter comprehensive discussions, many concerns are fixed but the novelty of this paper is still not that clear.  \" The concept of \"class irrelevant feature\" is not the first time proposed in this paper; it cannot be treated as the main contribution. Therefore, the authors' main contribution should be adding attention to previous works. However, the motivation for adding attention is unclear.\"",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "5KgKa96PUG",
    "title": "Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation",
    "authors": [
      "Aleksandr Beznosikov",
      "Georgiy Kormakov",
      "Alexander Grigorievskiy",
      "Mikhail Rudakov",
      "Ruslan Nazykov",
      "Alexander Rogozin",
      "Anton Vakhrushev",
      "Andrey Savchenko",
      "Martin Takáč",
      "Alexander Gasnikov"
    ],
    "abstract": "Distributed learning problems have gained significant popularity due to the increasing need for cluster training and the emergence of novel paradigms like Federated Learning (FL). One variant of FL, called Vertical Federated Learning (VFL), partitions data based on features across devices. The objective is to collectively train a model using the information available on each user's device. This paper focuses on solving the VFL problem using the saddle point reformulation via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. But more importantly, the paper explores various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations of the VFL problem are investigated, and numerical experiments are conducted to validate the proposed methods' performance and effectiveness.",
    "keywords": [
      "convex optimization",
      "saddle point problem",
      "vertical federated learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=5KgKa96PUG",
    "forum_url": "https://openreview.net/forum?id=5KgKa96PUG",
    "reviews": [
      {
        "rating": "6",
        "confidence": "2",
        "summary": "The paper proposes a saddle point reformulation of the Vertical Federated Learning (VFL) problem, allowing for more efficient and privacy-preserving optimization compared to traditional minimization methods. The authors introduce a deterministic algorithm with several practical stochastic modifications that improve communication, handle asynchronous participation, and reduce computation costs.",
        "strengths": "1.Reformulating Vertical Federated Learning (VFL) as a saddle point problem is interesting and novel, it can offer an alternative to traditional minimization methods that could address VFL-specific challenges more effectively.\n2.The paper presents comprehensive theoretic results.\n3.The practical modifications for improving communication efficiency, asynchronous participation, and computational costs are well-aligned with real-world VFL challenges.",
        "weaknesses": "1. While the paper introduces several modifications to the basic deterministic algorithm, such as quantization, biased compression, and asynchronous participation, these are presented with high mathematical density and minimal illustrative examples. This makes it challenging for audience like me that are less familiar with saddle point methods and vertical federated learning to fully grasp each modification's practical implications and implementation nuances. I suggest the authors to enhance accessibility by maybe providing more intuitive explanations or visual illustrations (e.g., flow diagrams) of the modified algorithms.\n\n2. The experiments mainly focus on benchmark datasets with linear regression and neural network fine-tuning tasks. The paper would benefit from exploring additional VFL scenarios that could showcase the flexibility of the proposed approach in handling diverse model architectures or real-world vertical partitioning cases. I am uncertain whether the datasets and settings used in the experiments are standard for the VFL field. If they are not, it would be beneficial to include a wider variety of commonly used VFL benchmarks to strengthen the empirical validation."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper studies the vertical federated learning (VFL) problem with the linear model by its convex-concave saddle point reformulation, which separates the data matrix $A$ and the loss function $\\ell$. Based on this reformulation, the authors propose an algorithm EGVFL for VFL based on the celebrated ExtraGradient method for convex-concave saddle point problems. They establish the convergence rate of EGVFL in terms of the duality gap, assuming $\\ell$ and the regularizer $r$ are convex and smooth. The paper also provides convergence guarantees for EGVFL variants with biased and unbiased communication compression, partial participation, and local steps.",
        "strengths": "- The saddle point reformulation seems to be natural and well-motivated. \n- When the model is linear, the authors established extensive convergence theory for the proposed algorithm and its extensions, accommodating key features such as communication compression, partial participation, and local steps. Besides, the convergence rate of EG improves upon GD in terms of $\\lambda_{\\max}(A^\\top A)$.",
        "weaknesses": "- The proposed algorithms only have convergence guarantees for VFL with the linear model and the extension for nonconvex problems remains heuristic. \n- In the experiments, only general-purpose optimizers are compared while existing algorithms specifically designed for VFL (e.g., [1] and its baselines) are completely missing. \n- Figure 1 and Figure 2 only present the relative objective gap w.r.t. the number of iterations. This might be unfair since the per-iteration computational and communication costs of the proposed algorithms and baselines are different. For example, EG requires one extra communication per round than GD, and algorithms based on the saddle point reformulation also need to update auxiliary variables z and y. \n \n\n[1] Xie, Chulin, Pin-Yu Chen, Qinbin Li, Arash Nourian, Ce Zhang, and Bo Li. \"Improving privacy-preserving vertical federated learning by efficient communication with admm.\" In 2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 443-471. IEEE, 2024."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper proposes a saddle point reformulation for the vertical federated learning (VFL) and provides extragradient-based algorithms to solve the reformulation with the convergence rate $O(1/K)$ on the expected primal-dual gap of the reformulation. The authors also present some extension to non-convex models. The authors conduct numerical experiments in VFL by using linear regression with $l_2$-norm regularizations and using the ResNet18 neural network.",
        "strengths": "The authors start with the basic reformulation in Section 2, and then thoroughly consider several stochastic modifications such as quantization for effective communications, biased compression, partial participation for asynchronous communications and coordinate descent for reducing local computational cost. For each case, a modified algorithm is presented with the complete proof on the convergence rate $O(1/K)$.",
        "weaknesses": "There seems a gap between the considered linear models and non-convex models in the formulation (4) on page 3 and (7) on page 9. See Questions for the details."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper explores new methods for Vertical Federated Learning (VFL) by reformulating the learning process using a saddle point framework instead of the traditional minimization approach. The proposed approach in the deterministic case enables solving VFL problems with enhanced convergence guarantees in terms of the eigenvalue of the data matrix. The authors also propose stochastic algorithms tailored to this reformulation and suggest modifications to address practical challenges such as communication efficiency and computational cost by implementing compression, partial participation, and coordinate selection, respectively. The paper validates the proposed methods through numerical experiments.",
        "strengths": "1. This paper proposes a new minimax framework for the VFL problem. The method has a better complexity constant compared to accelerated gradient descent.\n\n2. The theoretical guarantee in the modification of quantization for the saddle point problem in VFL is novel.",
        "weaknesses": "1. Insufficient Preliminaries:\nThe paper lacks clear explanations of key concepts such as Vertical Federated Learning (VFL) modeling and biased/unbiased compression. It would be easier to understand the paper if a \"Preliminaries\" section defining VFL and compression techniques were added before diving into the technical details.  Especially:\n\n1.1 in Section 3.1, the introduction of compression techniques is missing, and key notations, such as  $b^k$ appear without proper definition. This makes it difficult to understand how Algorithm 2 is formulated. The authors should include a notation table or glossary at the beginning of each major section or explicitly define each new symbol when it is first introduced.\n\n1.2 The reference to noise affecting $Z$ (line 154) is unclear, as the source of the noise and the conditions under which it occurs are not specified. It would improve clarity to explain the origin of the noise and provide an example of when it might arise.\n\n\n1.3 In the experiments section, SSP (line 462) is introduced without any prior definition. Please add explanation to SSP to improve the clarity.\n\n\n2. Disorganized Presentation: The structure in lines 205–217 is difficult to follow as specified as follows:\n\n2.1: it mainly focuses on previous work, but the purpose of mentioning it in this context is unclear. It would be clearer if the authors separated lines 242–245 into two parts: one for discussing the differences and merits compared to each mentioned previous approach, and the other for mathematically stating the relation between equation (5) and \\(gap^*\\).\n\n2.2: there are multiple claims without supporting mathematical proofs. For example, the statement:\n\"Criterion (5) can also be used for unconstrained/unbounded problems. To do this, one can use the trick from (Nesterov, 2007) and introduce bounded sets\" in line 207, and \"one can show that in Theorem 2.2 ... we can use the criterion\" in line 210. It would strengthen the work to provide rigorous proofs for these claims, especially if they support the contribution of this research.\n\n2.3: the explanation in lines 242–245 regarding the convergence criterion using \\(g(x,y) = xy\\) (from lines 205–217) is confusing. Specifically, the claim that \\(gap^*(x,y,z) = 0\\) does not make sense because the example \\(g(x,y) = xy\\) does not contain a third variable \\(z\\).\n\n\n4. Clarity, Missing Verbs and Poor Sentence Structure:\n\n4.1 The use of \"it\" in line 172 is ambiguous, making the meaning unclear. To enhance readability, consider explicitly stating what \"it\" refers to in this context. Replacing vague pronouns with precise references will help avoid confusion.\n\n4.2 In line 227-228, the sentence starting with \"one cannot...\" lacks a verb, which makes it incomplete. This is problematic because this sentence is critical for comparing the new results with prior work.\n\n4.3 in line 520-521, the sentence is incomplete (\"but only...\"), further complicating readability.\n\nI suggest the authors carefully proofread this work again for a better presentation.\n\n5. Inadequate Experiments:\nSince the tasks involve classification problems, I recommend that the experiments report test accuracy to effectively evaluate the proposed method.\n\nOn the other hand, this work proposes a new method for practical scenarios, such as partial attendance and compression. However, the impact of these proposed methods would be more convincing if accompanied by experiments demonstrating their effectiveness compared to previous approaches.\nOn the other hand, I did not find a discussion of related work in Vertical Federated Learning (VFL) that specifically addresses partial attendance and the use of quantization. Including a dedicated related work section summarizing prior research on these topics would provide better context and help position the contributions of this paper."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "Rp3DjldbSc",
    "title": "ICConv: A Large-Scale Intent-Oriented and Context-Aware Conversational Search Dataset",
    "authors": [
      "Quan Tu",
      "Jixiang Hong",
      "Xiao Long Wu",
      "Yantao Jia",
      "Zhao Cao",
      "Ji-Rong Wen",
      "Rui Yan"
    ],
    "abstract": "In recent years, search engines have made significant advancements. Yet, traditional ad-hoc search engines often struggle with complex search scenarios (e.g. multi-turn information seeking). This challenge has shifted the focus towards conversational search, an approach enabling search engines to interact directly with users to obtain more precise results. Progress in conversational search has been slow due to a lack of data and difficulties in gathering real-world conversational search data. To address these hurdles, we embarked on a journey to autonomously create a large-scale, high-quality conversational search dataset. Previous efforts to create such datasets often overlooked the multi-intent aspect and contextual information, or resulted in a biased dataset, where all dialogue queries linked to a single positive passage. In our study, we have incorporated multi-intent based on the existing search sessions and converted each keyword-based query into multiple natural language queries based on different latent intents present in the related passage. We then contextualized these natural language queries within the same session and organized them into a conversational search tree. A carefully designed dialogue discriminator was utilized to ensure the consistency and coherence of all generated conversations, assessing their quality and filtering out any substandard ones.\nAfter extensive data cleaning, we are proud to introduce the \\textbf{I}ntent-oriented and \\textbf{C}ontext-aware \\textbf{Conv}ersational search dataset (ICConv), a large-scale synthetic dataset comprising over 100,000 high-quality, information-seeking conversations. Our human annotators have evaluated ICConv based on six dialogue and search related criteria and it has performed admirably. We further explore the statistical characteristics of ICConv and validate the effectiveness of various conversational search methods using it as a standard for comparison.",
    "keywords": [
      "conversaitonal search",
      "multi-intent"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=Rp3DjldbSc",
    "forum_url": "https://openreview.net/forum?id=Rp3DjldbSc",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper describes ICConv, a new synthetic multi-turn conversational search dataset. The dataset is prepared by selecting a relevant subset of MS MARCO search sessions and expanding the keyword-based searches present in MS MARCO into context-aware natural language questions that could formulate a multi-turn dialogue. The authors provide summary statistics and human validation (on a subset) of the dataset to demonstrate its quality. Further, the authors evaluate search methods which follow different paradigms (ad-hoc, query rewriting-based, and dense conversational) to compare and contrast different methods on the task proposed in ICConv.",
        "strengths": "* The paper is generally well-written and easy to follow. It describes the dataset construction process in significant detail.\n* Given that the conversational/chat-based mechanisms are becoming a more common modality of interaction, the proposed dataset fills an important niche.\n* The dataset is documented extensively through a datasheet provided in the appendix.",
        "weaknesses": "*  Missing rationale for choices made (in terms of models/thresholds chosen) during the dataset preparation phase.\n* Recent baselines and experimental setup details missing from the comparisons provided experimental results section.\n* Inconsistent scaling for human evaluation experiments."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The work proposes an intent oriented, multi-turn, context-aware conversational search dataset. The method proposed to construct this synthetic dataset is novel considering multi-intent nature and contextual information for formulating natural language queries. The automated data construction approach also develops a dialogue discriminator model to control for dialogue quality and evaluates existing systems",
        "strengths": "1. The authors tackle an important problem of bridging the data scarcity gap in conversational search with multiple intents and contextual awareness.\n\n2. The data construction method is novel and the manual and automated evaluations are comprehensive.\n\n3. The data has utility to the IR and NLP communities with applications being conversational QA, search.",
        "weaknesses": "1. Several choices are not clearly explained or supported. For instance, grouping queries within a session that share at least one common word is not well supported as it could result in false positives and false negatives. For instance, queries like “American president” and “leader of United states” may not be grouped. Additionally queries like “american election” and “american universities” may be grouped which reflect two very different intents resulting in false positives. Additionally it is not clear why ANCE was chosen to select candidate responses for aiding in generating questions and the threshold mechanism is also not clearly specified. It is also clear in cases where there are no responses that meet the threshold are the queries ignored ? If so, how is the completeness of the conversation maintained by the proposed approach ? Additionally, the question generation approach does not explicitly constrain for the metrics measured, such as coherence and completeness of the conversation. For instance, the approach of followup question generation explicitly tries to optimize for completeness. In this approach, an initial query with response could be used to generate a NL query and the response to this question could be leveraged to generate the followup question. As mentioned earlier grouping session queries only based on lexical match with an arbitrary heuristic as done in ICCONV does not guarantee they are related and hence might result in conversational turns in dataset that are bit incoherent. While the example in Appendix is coherent, on closer look at the dataset released, i observed certain inconsistencies where a conversational turn were on unrelated topics. For instance, marco-gen-train-7146805 in the dev set starts with query “What is google classroom?” and the followup queries in turn are “What is facilitated diffusion?” which is a huge drift in topic and intent with no clear connection. Similarly “marco-gen-train-7746920” consists of queries about medications which are in no way related to another and not representative of real-world conversational search interactions which is one of the main motivations of this work.\n\n\n2. Some key details are missing in the work. What dataset was used to train the dialogue discriminator ? What was the retrieval corpus used for the experiments on icconv ? was it the original corpus for MSMARCO ? Or only the positive passages used to generate ICCONV ? I think this is critical to mention this information clearly as the corpus should also reflect real-world scenario of open-domain search with presence of distractors reflecting real-world challenges for retrievers.\n\n\n3. Also some key related works are missing. For instance [1] is a very relevant recent dataset on conversational search with information seeking queries and is critical to compare and distinguish ICConv contributions to this work. Likewise, ConvSDG[2] also is relevant. Also the baseline comparisons are bit outdated and some relevant works especially query reformulation approaches for conversational search such as ConvGQR[3] which is quite recent and ConQRR[4] are relevant to be included for comparison on the curated benchmark for conversational search.\n\n\n\n[1] ProMISe: A Proactive Multi-turn Dialogue Dataset for Information-seeking Intent Resolution Yash Butala, Siddhant Garg, Pratyay Banerjee, Amita Misra\n\n[2] ConvSDG: Session Data Generation for Conversational Search, Fengran et. al\n\n[3] ConvGQR: Generative Query Reformulation for Conversational Search Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie\n\n[4] CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, Gaurav Singh Tomar"
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper presents ICConv, a large-scale conversational search dataset comprising over 100k conversations generated through an automated pipeline. The dataset construction process consists of 4 key stages: session filtering based on word overlap, question generation that captures diverse user intents, query contextualization, and quality control using a dialogue discriminator. This work is trying to address two critical gaps in existing datasets: accommodating multiple user intents and effectively integrating contextual dependencies across dialogue turns. Authors conduct evaluations including both human assessment and statistical analysis to position ICConv as a robust benchmark for evaluating a range of conversational search methods.",
        "strengths": "1) This paper is constructing an automatic workflow to generate conversational datasets to reduce human efforts while balancing quality.\n2) The size of this dataset is large-scale.\n2) Authors implemented two kinds of mainstream technique for conversational searching evaluation: Query Rewriting and End-to-end. \n3) Authors analyze and propose hypothesis about their experiments to inspire readers to understand difficulties of conversational search.",
        "weaknesses": "The paper addresses an important problem in automatically constructing realistic and complex datasets for conversational search. However, the work would benefit from significant revisions, particularly in clarifying the motivation, improving writing clarity, and ensuring a thorough understanding of experimental design.\n\n## Motivation Deservers more clear and detailed illustrations:\n1) The introduction falls short in clarity, particularly regarding the claim in Lines 38-39 that traditional keyword-based search engines struggle to capture **\"genuine\"** user intents. This claim is not well supported and could be more convincingly illustrated. From a practical standpoint, many users including me successfully find relevant information through keyword-based searches, which suggests keyword searching still occupies a central role in realistic search interactions. A clearer definition and distinction between \"genuine\" and \"non-genuine\" queries, supported by references, would strengthen this argument. Additionally, a figure comparing genuine and non-genuine queries could help clarify this distinction. The authors should also elaborate on why previous single-turn methods and datasets fail to capture these genuine intents, and how a multi-turn conversational search engine could potentially address these limitations. Moreover, the premise that single-turn queries deliver a less than ideal user experience seems weak. If users want to refine their searches, they can rephrase their queries or conduct new searches. Figure 15 also seems more aligned with a dialogue-based QA system, like ChatGPT, rather than a \"realistic\" conversational search engine, which typically ranks results (e.g., Google Search) not a single answer. Therefore, the initial premise should be reconsidered or better justified to avoid overstating the limitations of single-turn search.\n\n2) Several sentences require more precise expressions, making it difficult to follow the motivation. For instance, \"Traditional ad-hoc search techniques and resources may not be suitable for using\" is vague. What is and specifically makes them **unsuitable**? If the term \"complexity\" is key here, it should be explicitly defined (does it refer to longer query history, more heterogeneous contexts, or more flexible query expressions?) Providing more citations and fine-grained explanations would help make the motivation clearer and more compelling.\n\n3) While the paper emphasizes the **\"real-world\"** aspect of the proposed dataset, this concept remains unclear. What specific features make this dataset more \"real-world\" than prior datasets? The authors use terms like \"genuine\" and \"complex\" but without clear definitions or examples. Including concrete examples, illustrations, or comparisons would help define these terms and demonstrate the advantages of this dataset over existing ones.\n\n4) The explanation of \"multi-intent queries\" in Lines 71-73 is difficult to follow. An one-sentence definition would help clarify what is meant by \"multi-intent\" or \"single-intent.\" Does this imply that a single query could have multiple relevant documents or answers? Furthermore, Figure 1 is overly simplistic and lacks sufficient captions to convey the intended message. It’s unclear where \"single-intent\" and \"multi-intent\" distinctions are made in this figure, and the signal of \"NO\" in the first and third queries is confusing. Revising the figure to include more general domain examples would make it more accessible, particularly for readers who may not be familiar with the Shakespearean content used here.\n\n5) The paper references MS MARCO abruptly without providing an introduction or context. A brief introduction of what is MS MARCO, along and why it was chosen as the primary corpus in this paper, would improve clarity for readers unfamiliar with this resource.\n\n6) Others:\n   - In Line 46, why do authors switch to \"interactive\" from \"conversational\"? Do they have the same meanings in this project? According to recent research [1-3], the \"interactive\" mainly refers to the interaction with the environment but for single-turn queries. \n   - In Line 46, what does \"organic manner\" mean?\n\n## No Task Definitions and Preliminaries:\nI suggest adding a specific section on task definitions and preliminaries to improve clarity. Without this, readers may find it challenging to follow the terminology. For example, terms like \"session,\" \"query tree,\" \"conversation tree,\" and \"search tree\" should be clearly introduced. Are these distinct concepts, or do they refer to similar structures? Additionally, it would be helpful to specify how many tree algorithms are implemented and the rationale behind their selection. A section that defines key concepts, such as \"turn,\" \"user query,\" \"session,\" \"tree,\" and \"ground truth answers\", which would provide readers with a solid foundation for understanding the paper.\n\nI also find it challenging to understand the task output and the evaluation methods. The authors should clearly introduce the metrics used to evaluate model performance for this task. Currently, there appears to be an inconsistency: **Figure 15 suggests the task output resembles a QA or dialogue task (with a single natural language sentence as the ground truth answer)**, whereas the results in Table 2 are evaluated using ranking- and recall-based metrics. How are metrics like recall or MRR computed with a natural language sentence as the answer? Did authors implement keyword retrieval from GT NL answers? A detailed description of the task formulation, evaluation metrics, ground truth answers, and evaluation methodology is **essential**.\n\n## Methodology\n1) No Comparison with Related Works: The authors do not provide a statistical comparison between the features of their dataset and those of other conversational search datasets. Such a comparison, highlighting comprehensive features, would help clarify the unique contributions of this dataset. In Lines 175-176, the authors mention that their method is \"novel.\" However, they should explicitly explain how it differs from previous approaches to better illustrate this novelty. For instance, how is this work different from recent research [4], which also trains smaller models to simulate user-engine interactions? The authors should clarify their distinct contributions and novel aspects, especially since [4] appears to employ more rigorous workflows and metrics for quality control.\n\n2) Potential Bias in Filtering: The filtering strategy, which relies on counting overlapping words to construct pairs, may introduce bias. This approach seems insufficiently rigorous; for instance, it might not accurately capture queries with **negation**, such as \"I don’t like lobster\" or \"I need recipes without lobster.\" These queries could still include overlapping words and lead to irrelevant results (e.g., recommendations or posts about \"lobster\"). How do the authors address such cases? Moreover, simple word overlap may not fully capture the nuanced and varied expressions of user queries, especially if the dataset aims to reflect \"real-world\" interactions.\n\n3) In Section 3.4, the negative samples are generated through rule-based methods. However, this approach may be insufficient for capturing the complexities of real-world interactions. Relying on rule-based operations could oversimplify the task, potentially making the training target too easy and leading to models that fail to distinguish between grammatically correct but contextually incoherent conversations. A more detailed explanation of how these rules align with real-world conversational features would strengthen this section.\n\n### Human Evaluation\n\n1) **Unrealistic Number of Turns**: The number of turns in some conversations is very high (e.g., 73 turns), which seems unrealistic, as users are unlikely to maintain this level of engagement in real-world settings. If authors deem that it's realistic, please provide references or reports to prove this.\n\n2) **Biased Benchmark**: Sections 4.3 and 5 suggest that the benchmark, generated entirely through automated methods, may contain inherent biases:\n   - In Figure 6, the majority of questions begin with \"what,\" indicating an uneven distribution of question types.\n   - In Line 326, the authors acknowledge that the method may suffer from limited diversity. However, diversity is quite important for a benchmark or dataset.\n\n3) **Inconsistencies in Human Evaluation Metrics**: \n   - The distinctions between certain evaluation questions are unclear. For instance, Q2 and Q6 appear similar; can the authors clarify the difference between these questions?\n   - Q3 is also ambiguous, as the connection between \"specificity\" and \"diversity\" is not immediately clear.\n   - Additionally, the term \"question\" is used inconsistently. The authors frequently refer to the \"diversity of questions\" and the \"grammatical correctness of questions.\" Are these questions referring to the final turn, the initial turn, or all turns in the conversation? More specific guidelines on how to evaluate each question (Q2, Q3, etc.) and criteria for assigning ratings (1, 2, or 3) would be helpful.\n   - The rating scale in Figure 7 lacks consistency. Some items have four rating levels, while others have three. What is the reason for this variation? Furthermore, the results seem to indicate that overall quality is not particularly high as highlighted in Introduction.\n\n### Questionable Experimental Conclusions\n\n1) **Unclear Definition of \"Manual\" Results**: It is unclear what is meant by \"Manual\" results in Line 375. Further clarification of this term and its relevance would be helpful.\n\n2) **Ambiguous Conclusion on Model Comparison**: In Lines 419-420, the authors conclude that \"encoder-decoder models perform better than decoder-only models.\" However, this statement lacks depth and rigorous analysis. For instance, could this difference be due to variations in model parameter sizes? Llama-3-8B, which is also a decoder-only model, could serve as a comparison point. It would be beneficial for the authors to substantiate this claim by comparing models of similar parameter sizes, such as T5-3B and Llama, to better illustrate the impact of architecture rather than parameter count.\n\n3) **Questionable Conclusions on \"Two-Stage vs. One-Stage\" Approaches**:\n   - **Implementing One-Stage for Long Dialogs**: How do the authors handle the one-stage approach in T5-3B with a token limit of 512 for lengthy dialogs? Given that some dialogs include lengthy history such as 73 turns, token limits may lead to issues with long-context handling. Additional explanation on this implementation would clarify the comparison. It seems authors did some process for it since it may lead to 512 / 73 = 8 tokens for each question and response in the history on average. According to the example shown in Figure 19, it obviously not true.\n   - **Two-Stage Implementation for Query Rewriting**: It is unclear how the two-stage process is implemented for query rewriting. Does this involve recursively rewriting queries or loading and rewriting all queries at once? This detail is crucial for understanding the approach's effectiveness.\n   - **Potential Bias in Results**: In Line 418, the authors observe that T5-QR outperforms other models. Could this exceptional performance be influenced by implementing T5 as the backbone for both data generation for benchmarking? This could introduce a bias in the evaluation. The authors should analyze and provide evidence to show that such bias is not present since this is a significant issue when using model-generated data for benchmarking purposes.\n\nOverall, the authors omit critical implementation details regarding model configurations. They should provide more information, such as methods for handling long-context inputs and specific approaches for query rewriting.\n\n\n\n[1] WebArena: A Realistic Web Environment for Building Autonomous Agents \\\n[2] InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback \\\n[3] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments \\\n[4] Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing"
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper presents a new conversational search dataset which is a synthetic dialogue dataset consisting of ~100000 information-seeking dialogues based on MS MARCO search sessions. It thoroughly describes the data construction method to achieve the ultimate high-quality conversational search dataset using NLP models. A thorough statistical analysis of the dataset is provided and a human evaluation demonstrates the high quality of the new dataset. Previous conversational search models are evaluated on the dataset, followed by a logical analysis of their performance.",
        "strengths": "1. The paper is well-written and easy to follow. \n2. The data construction methodology is valid, which may provide insights into other data collection tasks.\n3. A human evaluation corroborates the quality of the dataset and benchmarks are provided for researchers to advance their research in this area.",
        "weaknesses": "1. The paper's contribution would be more solid if a new conversational search model that outperforms the previous methods on the new dataset was also proposed along with its release.\n2. LLMs have shown remarkable capabilities in text generation. Why are they not used in the data construction process or data evaluation to further enhance the dataset quality?"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper addresses the scenarios requiring multi-turn and context-aware interactions, where the development is hindered by a lack of high-quality, real-world conversational search data. To tackle this, the paper introduces ICConv, a new large-scale dataset specifically designed for conversational search. ICConv is built on MS MARCO search sessions and addresses the multi-intent phenomenon—where a single keyword query can represent multiple underlying intents.",
        "strengths": "The resource has the potential to be impactful since it is designed for multi-turn scenarios, where a common problem is the lack of data.",
        "weaknesses": "My main concern regards the difficult of this dataset when using LLMs. While retrieval is a challenging part, I struggle to understand how this dataset can pose new challenges for the community - questions are from MSMARCO and are potentially easy to answer, even in a multi-turn scenario. The paper would benefit from an end-to-end evaluation where it can be shown that better retrieval and maybe better RAG models are required. Also, findings on question rewriting seems to be in line with a previous work in the field (Question rewriting for open-domain conversational qa: Best practices and limitations) that in depth analyzed the impact of QR. I suggest the authors to strengthen the justification part of this work, carefully describe differences with previous work, and clearly articulate (with experiments) the new challenges that this dataset introduces."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a conversational search dataset, ICConv and its construction method, highlighting the novel approach of considering multiple intents within a single keyword-based query. The overall methodology to construct the dataset can be divided into 4 stages. 1) Filter MS Marco search sessions with the number of overlapping words. 2) Generate intent-oriented natural language questions by giving a keyword and a retrieved response as a input to fine-tuned T5. 3) Transform each natural language question into contextualized natural language question. 4) Filter out dialogues by evaluating coherence and consistency. In addition, authors analyzed the dataset and provided the results from human evaluations along with the performance of conversational search methods on ICConv.",
        "strengths": "1. **Well-motivated and timely work on conversational search:** it is evident that crawling the users’ log to aggregate conversation sessions to construct a conversational dataset have issues on privacy, thus there is a substantial need to automatically construct the conversation search dataset in large scale. In this sense, authors point out that limitations of prior works and proposed to resolve the problem. \n2. **Supplementary materials:** Authors provided useful supplementary materials to provide examples and the github link to the dataset which helps readers to understand the structure and contents of the dataset.",
        "weaknesses": "1. **Lack of demonstration of details on constructing dataset:** The writing is enough to understand the overall procedure, but the details (either in words or formulas) or justification of the method design is poor. For example, in section 3.1, the complete list of stop words can be included either in the main body or in the appendix. Also in section 3.1, authors claim that having few instances (probably from MS MARCO Conversational Search DEV dataset while it is written as MS MARCO search sessions) after filtering with overlapping keywords implies that users are not used to interacting with conventional search systems. But it is not sure that conventional systems hinder the users’ usage of overlapping keywords in conversational manner, because there is a case that users might have complex information needs that cannot be captured in lexical manner but can be captured with semantic similarities. Furthermore, in section 3.4, the logical flow from training BERT with contrastive loss to finding the optimal path between all possible sessions with filtering out weak coherence is not straightforward and lacks of details. In conclusion, it would be helpful if authors provide the figure of the overall method, the table with comparison to other datasets and articulate texts.\n\n2. **Comparison to prior works with regards to novelty:** Authors point out that ConvTrans neglected that “keyword-based query usually corresponds to multiple natural language queries under different intents”, which is important claim to support the significance of the proposed method. Both ConvTrans and ICConv are generated by extending keywords to NL and CNL questions with T5. While the key novelty of the proposed method is generating multi-intent questions via concatenating responses as inputs (section 3.2.2) and filtering the path (section 3.4), the novelty of the other components is minimal."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.8333333333333335,
    "decision": "Reject",
    "meta_review": "- Scientific Claims and Findings:\n    - This paper presents a new synthetic multi-turn conversational search dataset and its construction method. It includes summary statistics, human validation on a subset of the dataset to demonstrate its quality, and an evaluation of various conversational search models on the dataset.\n- Strengths:\n   - The dataset may be valuable to IR and NLP community.\n   - The dataset construction method provides a reliable automated workflow.\n- Weaknesses:\n    - Many important details about dataset construction and evaluation are missing or unclear.\n    - The justification for this work, including comparison to prior works in terms of novelty, and the new challenges introduced by this dataset, is not clearly described.\n- Most Important Reasons for Decision:\n     - Given the identified weaknesses, the work is not yet ready for publication at this conference.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "TySMCLoGVl",
    "title": "Efficiently Scanning and Resampling Spatio-Temporal Tasks with Irregular Observations",
    "authors": [
      "Bryce Ferenczi",
      "Michael Burke",
      "Tom Drummond"
    ],
    "abstract": "Various works have aimed at combining the inference efficiency of recurrent models and training parallelism of MHA for sequence modeling. However, most of these works focus on tasks with fixed-dimension observation spaces, such as individual tokens in language modeling or pixels in image completion. Variably sized, irregular observation spaces are relatively under-represented, yet they occur frequently in multi-agent domains such as autonomous driving and human-robot interaction. To handle an observation space of varying size, we propose a novel algorithm that alternates between cross-attention between a 2D latent state and observation, and a discounted cumulative sum over the sequence dimension to efficiently accumulate historical information. We find this resampling cycle is critical for performance. To evaluate efficient sequence modeling in this domain, we introduce two multi-agent intention tasks: simulated agents chasing bouncing particles and micromanagement analysis in professional StarCraft II games. Our algorithm achieves comparable accuracy with a lower parameter count, faster training and inference compared to existing methods.",
    "keywords": [
      "sequence modeling",
      "efficient training",
      "efficient inference",
      "spatio-temporal",
      "multi-agent task"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=TySMCLoGVl",
    "forum_url": "https://openreview.net/forum?id=TySMCLoGVl",
    "reviews": [
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper discusses a approach to spatio-temporal modelling for tasks with irregular observation spaces. Experiments are conducted on multi-agent scenario's like robotic target-chasing simulations and StarCraft II gameplay analysis.\n\n\nMotivation: The paper aruges that current architectures like Transformers and LSTMs works well in scenarios with fixed observation spaces but struggle with tasks where observation spaces vary over time, such as multi-agent interactions. The paper aims to combine the efficiency of recurrent models with the parallelism of attention mechanisms, focusing on varying-dimension observation spaces.\n\nProposed Method:  The paper introduce an algorithm using a 2D latent state, alternating between cross-attention on observations and a discounted cumulative sum over time, capturing long context.  \n\nExperiments: Two benchmarks are used for evaluation: (a) Chasing Targets: Agents pursuing dynamic targets, evaluating the model's ability to infer intended goals in real-time; (b) StarCraft II: A environment with complex, temporally varying observations, where models infer combat unit assignments and predict unit movement.\n\nResults: The proposed model achieves competitive accuracy with fewer parameters and faster inference compared to standard methods (e.g., transformers). The \"resampling cycle,\" improves sequence modeling accuracy by conditioning observations on accumulated sequence data.\n\n===================\n\nAfter rebuttal: I've read the review by other reviewers as well as the rebuttal by the authors.\n\nThis statement by the authors summarizes the contribution of the paper well.\n\"Our results primarily set out to explore the trade-offs between efficiency and performance in the irregular observation case. This means results should be interpreted given the compute budget available to a practitioner. If only performance matters, practitioners should select a temporal transformer with sequential cross attention (X-Attn Sequential) for observation encoding. For efficiency and model parsimony, an inclusive scan should be selected with X-Attn Sequential.\"\n\nIt will be very helpful to revise the introduction and abstract accordingly.",
        "strengths": "The paper investigates the use of various encoders to encode irregular observations into a fixed dimensional latent, and corresponding sequence modelling approaches.",
        "weaknesses": "- The model's efficiency enables its use in real-time applications with irregular observation space, but studying the effect on longer sequence durations as well as diversifying the tasks (i.e., conduct experiments on more tasks) would further validate its utility in long-term planning tasks.\n- There's some work in the literature like Temporal Latent Bottleneck (https://arxiv.org/abs/2205.14794) and Block Recurrent Transformers (https://arxiv.org/abs/2203.07852); which combines strengths of attention and recurrence."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper investigates how to combine the inference efficiency of recurrent models and the training parallelism of multi-head attention transformers, especially for dealing with the varying size observation space. The authors propose a novel algorithm to cycle between cross-attention and inclusive scan to efficiently accumulate historical information. They evaluate their method on two benchmarks, Chasing-Targets gymnasium and StarCraft2.",
        "strengths": "The proposed method, especially integrating inclusive scan algorithm, is novel and is an effective way to accumulate history.\n\nThe experiment results are good. The proposed method improves the performance and reduce the computation overhead.",
        "weaknesses": "Please refer to the Questions part."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper proposes a method for sequential prediction tasks that can handle observations of varying sizes at different times. It also simultaneously introduces two intention-prediction tasks in which to test this method. Experiments are performed against a few baselines and components of the method are ablated.",
        "strengths": "- The contribution of the task based on StarCraft can be an interesting domain for future work to study\n- The writing is in general fairly clear to follow",
        "weaknesses": "I should start off by pointing out that I'm not precisely acquainted with this literature, so that will affect my judgment.\n\n## Major points\n1. One large issue I see with the paper is that I'm not totally certain why these particular tasks are good ones in order to explore modeling arbitrary-sized observation spaces. I think StarCraft as a domain makes sense, but there has been much prior work in StarCraft (which is not cited in the paper) that has used it in a different way, and I don't understand why the decision was made to couple \"intention-prediction\" tasks with the method. It seems that some domains like sequential decision making in healthcare, or some kind of variable-sized text prediction task would be equally fine as a benchmark domain. This is further complicated by the relatively unclear definition of the exact input and output for each task in Section 3. Given the motivation of these tasks is that it shows up often in interactions, it would seem that a space like Shared Autonomy might provide some more common benchmarks.\n\n2. Another big issue that I see is that the paper does not make a strong enough case for the choice of baselines. I would expect there to be accompanying work in the domain previously with which to compare against, but because 4.1.2 does not cite any prior work, it's unclear to me that these choices are well-founded. Certainly I'm familiar with Mamba2, but the rest is unclear. Also the choice of encoders for the RNN methods feels somewhat unjustified.\n\n3. The last major issues I see is that the results in Figure 5 and Table 1 feel somewhat inconclusive. Because nothing is controlled (either parameter count, memory, or training time) between methods, and there is no clear winner across metrics, it's difficult to see the benefit. When coupling these mixed results with the choice of new bespoke domains, it's even harder to understand the longer-term contribution. I don't think it's the case that a new method needs to be always better than prior work, but the fact that it does not appear to have particular advantages in computation that was claimed as a contribution (\"efficiently address sequence modeling\" at the end of Section 1), makes it hard to understand the contribution.\n\n## Specific points\n\n1. Section 4.1.2 add a citation to Mamba2\n2. Section 4.2 it would be nice to have the problem written out mathematically as I find the current description a bit hard to follow\n3. Section 5 is missing comment on hyperparameter tuning, especially for baselines\n4. Fig 7 should be referenced in 5.1.1"
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper tackles the issues of spatio-temporal modeling in the context of variable observation spaces, such as when there are variable objects across episodes / scenes, etc or when the object number varies within an episode (such as when entities enter and leave an arena). The authors introduce a few baseline tasks representative of this problem and cover various attention-based encoding and temporal aggregation schemes, some of which are advertised as being novel (such as some of the encoder methods). They perform various analysis on different methodologies, such as task performance, memory, speed, etc on these tasks which highlight some of the benefits of various methods (such as the scan-based one).",
        "strengths": "Overall the paper is interesting and approaches a very important problem I think in a comprehensive way, covering some benchmarks that seem well motivated to me as well as some methodologies for dealing with the variable observation problem. The experiments I think are clear enough and the conclusions drawn I think are sound, demonstrating that some methods perform better than others.",
        "weaknesses": "There are places where the writing is good, and there are places where the writing and motivation is not so good. The introduction is very vague and lacks citations on some critical claims in the paper that make it difficult for readers who are not very familiar with the domain to understand well. The benchmarks are introduced well enough, but it's unclear why we need multi-player problems to address the core problem of variable observation spaces. Critically though when the methods come around the organization of the paper suffers, as it becomes extremely difficult to understand the motivation of different design choices, such as the scan operation (e.g., why was the scheme in figure 4 motivated, other than post-hoc performance?). The methods in Figure 3 seem interesting, but I had difficulty finding their consequence in the paper later on.\n\n(Many of these and questions below were addressed sufficiently in the rebuttal and revision)"
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "The paper presents an interesting approach to sequence modeling with irregular observation spaces, exploring novel scan operations and cross-attention techniques for handling variable-dimensional observations. While the work demonstrates potential in multi-agent tasks and introduces innovative modeling strategies, it struggles with comprehensive validation and generalizability. Reviewers noted significant challenges in methodology, including unclear motivation and insufficient differentiation from existing approaches. The benchmarks, primarily based on custom multi-agent tasks, do not definitively prove the method's broader applicability, and performance improvements remain marginal. Despite the authors' efforts to address concerns through additional experiments and clarifications, fundamental limitations persist. The incremental nature of the contribution, combined with presentation challenges and limited experimental rigor, suggests that while the work shows promise, it is not yet sufficiently developed for publication at a top-tier conference.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "VNMJfBBUd5",
    "title": "Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks",
    "authors": [
      "Danni Yuan",
      "Mingda Zhang",
      "Shaokui Wei",
      "Li Liu",
      "Baoyuan Wu"
    ],
    "abstract": "This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics.",
    "keywords": [
      "Backdoor Defense",
      "Poisoned Sample Detection",
      "AI security"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=VNMJfBBUd5",
    "forum_url": "https://openreview.net/forum?id=VNMJfBBUd5",
    "reviews": [
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors present an approach for backdoor sample detection based on the angle of the activation gradients between samples and a reference clean sample for each class.\nThe paper observes that, within the target class, the activation gradient angles exhibit greater dispersion compared to samples from the clean class. Furthermore, the distribution of angles of backdoor and clean samples within the target class are somewhat distinct, providing a basis for differentiating between clean and poisoned samples.\n\nBuilding on this observation, the authors introduce the concept of the Gradient Circular Distribution (GCD), a distribution of angles between the activation gradient of samples and that of a reference sample. They propose two key metrics based on this distribution:\nCVBT: A metric designed to measure the dispersion of the GCD.\nSample Closeness: A metric that evaluates how close a given sample is to  reference clean sample.\n\nThese metrics are then utilized to develop a filtering algorithm, which first identifies the target class and subsequently filters out the poisoned samples. The paper validates the proposed method through experiments on a range of backdoor attack scenarios.",
        "strengths": "1. The paper identifies a previously unexplored phenomenon in backdoor samples, highlighting greater dispersion in activation gradient angles within the target class compared to clean samples.\n2. The authors provide extensive experiments on various backdoor attacks across two architectures (PreAct ResNet18 and VGG-18 BN),\n3. Some initial analysis on adaptive attacks is presented, though further exploration could strengthen the findings.\n4. The writing is clear and well-structured, making the main arguments easy to follow.",
        "weaknesses": "1. I am giving low scores to soundness due to the reported  low performance of ASSET, AC. The performance in the original paper of ASSET is close to 90% for all attacks. Additionally, 0 F1 scores for AC also does not look right. For example, in the aforementioned paper, the average TPR of AC is around 50%. \n\n2.  Please provide sensitivity of various thresholds like $\\tau_z$ , $w, \\beta$ (Stage 3). What is the rationale behind the choice of $\\tau_z$ ? \n\n3.  Please comment on or provide results for these additonal adaptive attacks: \na. Can an adversary add  perturbations to the clean samples (different perturbations to different samples) such that their dispersion increases ? \nb. Given a clean set with 100 images, the adversary adds the same noise to 50 images and keeps the other images the same. Will the  dispersion increases of clean sample increase ? \nc. The backdoor trigger is optimized such that the dispersion of the target class decreases."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors propose to detect poisoned examples from an activation-gradient circular distribution perspective. The authors draw two novel observations by studying the GCD of poisoned/clean examples, based on which they introduce an algorithm named AGPD to do backdoor example detection.",
        "strengths": "(1) Understanding the activation gradient of poisoned examples with circular distribution is interesting. The two observations of GCD are novel.\n\n(2) The algorithm shows strong performance within the evaluation setting of this paper.\n\n(3) The paper is overall well written and the logic is easy to follow. The empirical experiments are mostly comprehensive and valid.",
        "weaknesses": "(1) My main concern about the proposed algorithm is its scalability. It requires training on the entire dataset to determine the detection state, which might be too expensive given web-scale data and the extreme probability of the data being poisoned. Also, it takes nearly $O(n2) $ to compute the GCD, which is infeasible for large datasets. \n\n(2) The defense algorithm appears to be highly correlated with the specific class labels, potentially limiting its applicability. In the real world, the attackers can adopt a different set of labels than the defenders. The attackers could label the dataset more fine-grained/coarse-grained than the defender. (e.g. Imagine there is a dataset consisting of snacks and drinks and the target class of the attacker is Coca-Cola. However, the defenders just want to classify the images as soft drinks no matter whether it is Coca-Cola or Pepsi. Or the reverse situation can also happen.) Will the observations about GCD hold under such misalignment?\n\n(3) Table 7 is not organized well, please have a double-check."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The paper presents a new approach to identifying poisoned samples in datasets used for deep neural network training. The core proposal, termed Activation Gradient-Based Poisoned Sample Detection (AGPD), leverages a novel metric called Gradient Circular Distribution (GCD) to detect discrepancies between clean and poisoned samples. Key insights include observing that in backdoor attacks, models often map poisoned samples and clean samples to similar activation regions, which AGPD detects by analyzing gradient direction distributions. This method involves three main steps: calculating GCD for each class, identifying target classes based on dispersion, and filtering out poisoned samples.",
        "strengths": "(1) Introducing GCD as a novel measure adds a unique, technical depth to the approach, potentially applicable to various attack scenarios.\n\n(2)  AGPD shows high detection performance with minimal additional clean data, a notable advantage for real-world applicability, where such data may be limited.",
        "weaknesses": "(1) The proposed method is founded on two core observations: (i) the Gradient Circular Distribution (GCD) of the target class exhibits greater dispersion than those of clean classes, and (ii) poisoned and clean samples are distinguishable by their clear clustering in separate regions. However, it remains unclear whether these observations hold consistently for clean-label attacks, where the mapping directions of poisoned and benign samples could exhibit higher similarity given their similar input-space characteristics. Further discussion on this aspect would strengthen the paper’s analysis.\n\n(2) The choice of baseline methods appears limited to older techniques. A broader comparison with more recent, state-of-the-art defense methods would provide a more thorough assessment of the proposed method’s performance relative to the latest advances in poisoned sample detection.\n\n(3) Attack Success Rate (ASR) is a key metric commonly used in backdoor detection evaluations. Its omission from Table 1 raises questions about the comprehensiveness of the evaluation metrics chosen. Including ASR would provide a clearer view of the method's efficacy against backdoor attacks.\n\n(4)   In the main evaluation, a poisoning ratio of 10% is used for non-clean label attacks and 5% for clean-label attacks. Since clean-label attacks are generally more challenging, lower poisoning ratios are typically applied to non-clean label attacks in real-world scenarios. This discrepancy between the paper’s experimental setup and actual attack contexts calls for further clarification. Additionally, practical attack scenarios often employ very low poisoning ratios (1% or 0.5%), feature separability-based defenses tend to be less effective in this setting. More discussion should be included."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "With the inspiration that a backdoored model is apt to map significantly different poisoned samples and clean samples of the backdoor target to similar activation areas, this paper introduces a novel measurement, i.e., the circular distribution of the gradients w.r.t sample activation, namely GCD, which works to identify the target class of the backdoor and consequently separate poisoned and clean samples within the target class. Accordingly, this paper proposes a sample detection approach called AGPD to achieve dataset purification. Extensive experiments show AGPD's advanced performance in detecting and isolating poisoned samples.",
        "strengths": "- This paper proposes a novel measurement to show the dispersion of poisoned and clean samples in the model's activations\n\n- This paper is well-written and enforces a high readability with variant and helpful illustrations \n\n- This paper comprehensively evaluates AGPD's performance by considering diverse poisoning attacks, comparing with different related defenses, and conducting experiments on variant datasets.\n\n- This paper considers the ablation study on different hyper-settings of APGD and involves two reasonable adaptive attacks that have considered APGD's defense w.r.t the GCD distribution.",
        "weaknesses": "(1) The use of $arccos(\\cdot)$ in Eq.(2) is ambigious as $cos\\(\\cdot\\)$ is used in Eq.(3) and Eq.(4) as well.\n\n(2) The calculation of GCD across all model layers is not clearly formulated for each sample.\n\n(3) The influence of the clean sample by random sampling for the basic sample pair is not studied.\n\n(4) There is lacking the rationale for setting the hyper-parameter $\\tau_z = e^2$.\n\n(5) For all-to-all attacks, a better explanation of how the dataset separation by APGD is achieved is required.\n\n(6) The description of Adpative-Blend in Appendix B is incomplete and different from the default attack setting.\n\n(7) It is unclear how the noisy and poisoned samples are separated against WaNet and Adaptive-Blend attacks.\n\n---\nIn the following, further questions related to the above points are detailed with the notion of \"Q-#\"."
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.75,
    "decision": "Accept (Poster)",
    "meta_review": "The paper proposed a new apporach for detecting poisoned samples called Activation Gradient-Based Poisoned Sample Detection (AGPD). It leveerages a novel metric called Gradient Circular Distribution to distinguish clean and poisoned samples. The empirical results also demonstrate its effectiveness.\n\nStrength:\n\n1. Insightful view on understanding the activated gradient of poisoned examples with distribution.\n\n2. The algorithm shows strong performance within the evaluation setting of this paper.\n\n3. Well written.\n\nWeakness:\n\n1. Only explores the backdoor attacks on Pre-Act ResNet 18 and VGG, whether the proposed feature still exists in Transformers or other networks are unexplored.\n\nAll the reviewers thoughts positive towards this paper, therefore I recommend to accept it as a poster.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "yRKelogz5i",
    "title": "Causally Motivated Sycophancy Mitigation for Large Language Models",
    "authors": [
      "Haoxi Li",
      "Xueyang Tang",
      "Jie ZHANG",
      "Song Guo",
      "Sikai Bai",
      "Peiran Dong",
      "Yue Yu"
    ],
    "abstract": "Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework, termed **CAUSM**, to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through causally motivated head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks to demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs.",
    "keywords": [
      "Large Language Model; Sycophancy; Causal Modeling"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=yRKelogz5i",
    "forum_url": "https://openreview.net/forum?id=yRKelogz5i",
    "reviews": [
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduces a new framework called CAUSM (Causally Motivated Sycophaocy Mitigation) aimed at reducing sycophancy in Large Language Models (LLMs). The paper analyzes and models the sycophancy issue in LLMs through the lens of Structured Causal Models (SCMs). \nA significant causal signature is proposed to distinguish latent causal embeddings from spurious embeddings that cause sycophancy. The paper further propose an intervention-based scheme to calibrate the direction of the derived causal representations. Extensive experiments show that the proposed approaches outperforms the state-of-the-art competitors.",
        "strengths": "1. It is the first to apply Structured Causal Models (SCMs) to analyze and model sycophancy behavior in Large Language Models (LLMs), offering an innovative research perspective.\n2. Extensive experiments show that CAUSM is superior to existing state-of-the-art methods in mitigating sycophancy in LLMs.",
        "weaknesses": "1. In Line 81, the phrase “To map the latent causal embeddings to the observable intermediate components of LLMs” appears to conflict with the statement “a significant causal signature which can distinguish the intended causal embeddings from spurious embeddings which incur sycophancy within the latent representation space.” Could you clarify this discrepancy?\n\n2. How can I(X_P ; Y | Z) be approximated by Eq. 7? Is causal intervention controllable? Could you provide an example to illustrate this?\n\n3. Why does the intervention \\bar{X}_P maximize the difference in cross-entropy losses for a fixed W? Is this a result of the algorithm's design, or does it align with the intrinsic nature of interventions?\n\n4. The author claims to utilize Parameter-Efficient Fine-Tuning (PEFT) in Line 267, yet states in the baselines section that all parameters are fine-tuned. This is rather confusing.\n\n5. In Section 4.3, how is the weight matrix value obtained, and what does |w_l^h| represent? Are these parameters part of the adaptor in PEFT, or are they parameters of the model itself? How are these parameters utilized specifically?"
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "The paper introduces a novel framework called CAUSM to address sycophancy in large language models (LLMs), which refers to the models’ tendency to align with user preferences even when those preferences lead to incorrect or biased outputs. This behaviour reduces the reliability and factual integrity of LLM responses.  The authors conclude that CAUSM effectively mitigates sycophantic behaviour in LLMs by focusing on the causal structure of sycophantic representations. The framework offers a scalable solution to improve the factual reliability of LLMs while respecting user preferences, which holds promise for enhancing trust in AI outputs in real-world applications.",
        "strengths": "1. The authors provide a novel framework, CAUSM, which leverages structured causal models to address sycophancy in large language models (LLMs). By introducing a causal approach, they advance beyond existing methods that may depend on spurious correlations, achieving more reliable mitigation of sycophantic responses.\n\n2. The article is well-structured, with clearly delineated sections detailing the problem (sycophancy in LLMs), prior approaches, and the limitations they aim to address with CAUSM. This clarity is helpful for readers who may be less familiar with the subject matter.",
        "weaknesses": "My main concerns focus on the application of the structured causal model (SCM) approach. The authors state that causal relations can be captured via a directed acyclic graph (DAG) learned through regularization. However, in real-world applications, the regularization term is unlikely to reach zero. How can the authors be certain that the learned representations are truly disentangled? The methodology would benefit from a more robust approach to verifying that these representations capture causal rather than correlated information.\n\nMy second question relates to the evaluation of DAG structures in SCMs. Typically, metrics like Structural Hamming Distance (SHD) or False Discovery Rate (FDR) are used to confirm if the learned graph conforms to a DAG structure. This paper, however, claims that the approach is inspired by graph models without providing a specific graph-based evaluation. How can we be sure that the learned representations are genuinely causality-related rather than optimized merely by overfitting through additional parameters? Both theoretical analysis and case studies would strengthen the authors’ claims.\n\nRegarding the representation learning approach, it seems counterintuitive that a fully supervised learning model without any stochastic components could infer causality from data alone. The mutual information-based independence criterion here could indicate correlation, but correlation does not imply causation. Causal inference methods typically rely on stochasticity in at least one part of the model (e.g., a two-tower architecture) to differentiate causality from mere correlation. Without this, there is a risk of learning coincidental patterns rather than true causal relationships.\n\nLastly, a minor issue: without providing a formal proof, I recommend avoiding the use of terms like \"Lemma\" to present conclusions, as this suggests a level of mathematical rigor that is not fully substantiated in the paper."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper addresses the issue of sycophancy in large language models (LLMs) and introduces CAUSM, a novel method for identifying and mitigating sycophantic behavior within the models’ latent representations. The authors view sycophancy in LLMs as “spurious correlations between user preferences and model outputs”. By leveraging structured causal models, they aim to disentangle sycophantic representations from causal embeddings. An intervention-based technique is then developed to recalibrate the causal representation direction embedded in attention heads.",
        "strengths": "1. The paper aims to address LLMs’ sycophancy issue, which is an important topic in the community.\n\n2. A variety of experiments have been conducted to show the effectiveness of the proposed approach across different datasets.\n\n3. The high-level structure of the paper is easy to follow.",
        "weaknesses": "1. The motivation for the proposed approach and the intuition of the algorithm design needs to be more clear.  \n-  In the Introduction, the authors discuss two groups of prior research on LLM sycophancy: (1) linear probing and (2) path patching. However, these prior works primarily concentrate on analyzing and understanding sycophantic behavior in LLMs, rather than on mitigating it (also as mentioned in Related Work line 113-122). Given the distinct emphasis of these studies compared to the authors' goal, it is unclear how the limitations of these earlier works directly motivate the development of the authors’ proposed approach.\n\n- While the authors discuss several recent studies on mitigating LLM sycophancy in the Related Work section (lines 124–135), a more in-depth comparison between these studies and their own approach would be beneficial. Specifically, it would be helpful to understand if there are potential methodological concerns with the designs proposed by Burns et al. (2022) and Rimsky et al. (2023) that inspired the development of CAUSM.\n\n- It would be helpful to discuss the rationale for using structured causal models to capture sycophancy in the models’ latent representations.\n\n2. Some compared methods in the tables/figures of results are confusing. E.g., I couldn’t find a clear definition of CAUSM (Base) (Table 1 & 2) and CAUSM (Table 2). What is the difference between CAUSM and CAUSM (Base) in Table 2? \n\n3. Did the authors conduct the experiments on multiple base LLMs or specifically focus on a single base LLM (i.e., Llama-2-7B-Chat)?"
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper analyzes and models sycophancy in LLMs through the lens of structured causal models (SCMs), which is actually the reliance on spurious correlations between user preferences and model outputs. Based on the proposed SCMs, this paper develops a novel framework called CAUSM to mitigate sycophancy in LLMs by exploiting a significant signature.",
        "strengths": "1.The author models the phenomenon of sycophancy in language models as a type of spurious correlation in causal structures, making it possible to address sycophancy through conditional independence constraints.\n2.The CAUSM method achieves excellent results in mitigating sycophancy across INTRA-DATASET, CROSS-DATASET, and CROSS-TASK scenarios.",
        "weaknesses": "1. The motivation for the Causal Activation Calibration method in Section 4.3 is unclear. Specifically, the relationship between the causal direction in Section 4.3 and the SCM in Section 4.1 is not sufficiently clear. Please provide further clarification on this point. My understanding is that Equation (6) aims to add a conditional independence constraint to the original training objective, intending to eliminate spurious correlations between $Z_S$  and $Y$. However, I am not convinced how the “causal direction” in Section 4.3 effectively mitigates these spurious correlations. Does this approach leverage the causal direction as a representation of causal effect orientation, or is there some other theoretical justification? I suggest that the authors clarify this aspect in the paper.\n\n2. The second statement in Lemma 4.1 is not described clearly enough. I would like to understand which part of the subsequent methods specifically utilizes this statement. For instance, if we consider a specific example where $f(Z_C,Z_S) = Z_C$, then this condition does not hold in the causal graph shown in Figure 2(a). Furthermore, does the second statement in Lemma 4.1 offer any guidance in constructing the subsequent methods? If not, I would suggest the authors consider removing this statement."
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.25,
    "decision": "Accept (Poster)",
    "meta_review": "The paper addresses sycophancy in large language models.\n\nStrengths:\nThe first to apply Structured Causal Models (SCMs) to analyze and model sycophancy\nExtensive experimentations\n\nWeaknesses:\nSome issues such as clarity might have been addressed in the rebuttal, still it might be worth improving it for the camera ready.\nSee proposals to improve the paper by different reviewers.\nWell written,\n\nOverall, the paper addresses an interesting and well explained issue.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "GsCMKwyfWm",
    "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
    "authors": [
      "Muhammad Fetrat Qharabagh",
      "Mohammadreza Ghofrani",
      "Kimon Fountoulakis"
    ],
    "abstract": "Counting is a fundamental skill for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs’ counting abilities using a divide-and conquer approach, breaking counting problems into sub-counting tasks. Unlike prior methods, which do not generalize well to counting datasets on which they have not been trained, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances counting capabilities across various datasets and benchmarks.",
    "keywords": [
      "Counting",
      "Large vision-language models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=GsCMKwyfWm",
    "forum_url": "https://openreview.net/forum?id=GsCMKwyfWm",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper addresses the problem of counting objects in images using large vision-language models (LVLMs), which often struggle with counting tasks, particularly when the object count exceeds typical values encountered during training. The authors propose a method named LVLM-Count, which aims to enhance LVLMs' counting abilities through a divide-and-conquer approach. LVLM-Count is structured in four stages: (1) Area Detection, where regions containing relevant objects are identified; (2) Target Segmentation, in which these regions are segmented to highlight individual objects; (3) Object-aware Division, where regions are divided into sub-images without cutting through the segmented objects; and (4) Counting Aggregation, where the LVLM counts objects in each sub-image and combines the results to produce the final count. The proposed approach is claimed to generalize well to new datasets without additional training or fine-tuning, showing improved performance on various datasets and benchmarks compared to prior methods.",
        "strengths": "This paper explores a relatively novel approach by focusing on enhancing counting capabilities in large vision-language models (LVLMs) using a training-free methodology. By leveraging the power of LVLMs, the authors propose an effective paradigm that does not rely on additional training or fine-tuning, which is particularly advantageous in scenarios where labeled data is limited or unavailable. The method demonstrates a creative approach to addressing challenges in object counting, especially in cases with a high number of objects and significant object overlap. By employing a divide-and-conquer strategy, the proposed LVLM-Count effectively manages the complexity of densely populated scenes, providing a practical and scalable solution for counting tasks that would typically challenge standard vision models. The paper’s emphasis on a training-free framework in conjunction with LVLMs is both innovative and valuable, offering a flexible counting solution that could be adapted to various applications without the need for retraining.",
        "weaknesses": "This paper also presents some limitations, as acknowledged in the final section. The proposed method heavily relies on the accuracy of the initial stages—specifically, object detection and instance segmentation. If either of these stages is inaccurate, it could significantly affect the downstream steps, potentially compromising the overall performance. This dependency raises questions about the robustness of the method on more challenging datasets, especially those with high levels of occlusion or camouflage, where accurate detection and segmentation are inherently more difficult. Additionally, the comparison with existing methods is relatively limited. To strengthen the paper’s persuasiveness, it would be beneficial for the authors to include more comprehensive comparisons with other state-of-the-art counting methods. Lastly, the paper would benefit from further ablation studies, such as an investigation into the impact of each stage in the pipeline. For instance, an ablation study examining the effect of including or excluding the target segmentation stage could provide insights into its significance within the proposed approach. These additions could help validate the robustness and effectiveness of the method across diverse scenarios."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper aims to improve the counting ability of large VLMs. The paper proposes to split a counting task into sub-tasks by dividing the input image into smaller parts, counting objects in the smaller parts, and then aggregating the counts. The authors show that the proposed approach can generalize to unseen datasets.",
        "strengths": "The results obtained and demonstrated in the paper seem strong.",
        "weaknesses": "The main weaknesses of the paper lie in the lack of enough support for the claims made. In particular, the authors should address the following questions/comments in their responses and revisions:\n\n1. In several places in the paper (e.g. lines 61-62), the authors mention that pipeline detects \"the objects of interest\". Are there even more than one types of objects to be counted in these datasets? If yes, how are objects of different categories handled? All the visual examples in the paper involve only a single type of object. \n\n2. In line 349, the authors talk about \"simple\" and \"complex\" counting questions. What do these mean in this context?\n\n3. Lines 241-242 say \"In out experiments, we specify which approach we use for each dataset\". Having different approaches for different datasets (outside of a few hyper-parameters) defeats the point of proposing a single approach for a problem. This is problematic, particularly because one of the selling-points of this paper is the claim that the proposed approach generalizes across datasets. \n\n4. The introduction mentions \"industry, healthcare, and environmetal monitoring\" as the areas of application. It would have been useful if the paper actually included some real-world examples from these domains to demonstrate the utility of the proposed approach."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes LVLM-Count, a prompt-based, training-free counting approach that enhances object counting performance with large vision-language models (LVLMs).  For addressing limitations with current LVLMs, such as their challenges with high-count tasks and complex counting questions, this method employs a four-stage pipeline: (1) area detection, (2) target segmentation, (3) object-aware division, and (4) target counting. This enables LVLMs to effectively segment, process, and count large numbers of objects across diverse datasets, showcasing robust generalization.",
        "strengths": "A simple and intuitive pipeline for counting with LVLM\n\nGood presentation along with clear drawn figures.\n\nA newly introduced Emoji-Count benchmark is introduced, though the generation of this data is not complex but still useful as a testbed.\n\nGood performance margin achieved.",
        "weaknesses": "W1: At the very beginning, the authors should define more clearly what means by large number of objects, 10s, 100s, or 1000s, per image. As this defines the scope of this work in terms of crowdedness. \n\nW2: The key idea of this work, divide-and-conquer, can be hardly considered novel for two reasons: 1) in this context, counting by definition is a process of adding the number of objects from region to region. It is essential a process of summing up across regions; 2) Such an idea has appeared in the counting literature such as [Ref 1] where there is also a need for avoiding repeatedly computing regions within an image. As a result, this whole method pipeline is not sufficiently novel -- it is more like a baseline design of using LVLM for counting. \n- [Ref 1] Xiong H, Lu H, Liu C, Liu L, Cao Z, Shen C. From open set to closed set: Counting objects by spatial divide-and-conquer. InProceedings of the IEEE/CVF international conference on computer vision 2019 (pp. 8362-8371).\n\nW4: It is inconsistent than different examples are used from Fig 3 to Fig 5 when discussing individual components. From these examples, little challenges are visible with counting, and I am impressed that simply counting the mask of GroundingDINO would achieve good performance, even not bother LVLM such as GPT-4o. For example, simply counting the mask number in Fig 4(c) can give us very accurate count. I would suggest the authors use the same example with proper challenges involved and considered in this work, across all these sections. \n\nW5: This method uses a number of pretrained models such as LLMs, GroundingDINO, and GPT-4o, Real-ESRGAN. One concern is about efficiency. The authors should conduct an efficiency analysis in both training and inference, which is now missing. \n\nW6: Except the comparison with previous works, I suggest a couple of baseline methods should be included in this work:\n1) LLM + GroundingDINO: After target segmentation step (Sec 3.2), directly counting the masks by SAM. This can be use to validate the significance of region division, a key aspect of this work. This complements to the ablation result of using GPT-4o in Table 4 (Appendix). \n\n2) Passing the SAM's mask to GPT-4o to count: This will directly compare the proposed object aware division algorithm.\n\n3) To compare with Ref 1's strategy on avoiding repeated counting at the region level\n\nW7: In ablation study, it is suggested that the authors examine the effect of using super-resolution on the regions.\n\nW8: Please clarify what LLM is used in this work?\n\nW9: Except those datasets used, another good test is PASCAL VOC. This should add different test cases on top. The authors can consider to evaluate."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper proposes LVLM-Count, a plug-in method for current LVLM to enhance the visual perception on counting ability. The method uses an extra module with object-aware division, which can divide multi objects without cutting through objects of interest. Also the paper proposes a dataset for testing the counting ability of LVLMs. Compering other method, the proposed method achieves good performance on different benchmarks.",
        "strengths": "This paper provides good thinking and novel insights for a very important domain about LVLMs: visual counting ability of LVLM.\n\n* The paper proposes a novel and simple method for dividing objects without cutting, which is meaningful for visual perception and the inference logic of LVLMs.\n\n* The paper can deal with complex scenarios and achieve high performance on large mount of objects.\n\n* The proposed benchmark looks very interesting.",
        "weaknesses": "I have some concerns about the paper and hope the authors can address them:\n\n* The motivation and method looks good but the experiment results may not support them very well. In the section of experiments (Section 4), there should be some ablations about the designed method.\n\n* The metrics of evaluation may not be enough for well evaluating a LVLM's counting ability. Maybe there are two situations about counting: 1) we need an approximate number of objects. 2) we need an exact number of objects. Current metrics may can only evaluate the first case. So I think you could add more metrics on all benchmarks, such as Accuracy (right cases / total cases, same with EA, but EA is only used for one benchmark). Also you can set different ranges of the Acc, e.g., you can firstly set Acc_{+-0} which means that the answer must be the exact number of objects without any difference. Then you can set Acc_{+-3} which means that the answer is acceptable with in +-3 error numbers. Following this you can set Acc_{+-5}, Acc_{+-10}.\n\n* The method you proposed is actually a plug-in method for any LMLM. So it is a little weak that you only combine it with one LVLM (GPT4o). You should involve more LVLMs such as LLaVA series, Genimi, etc.\n\n* The outputs of the LVLM are always natural languages, how do you make sure that every response is about the counting? how do you avoid the problem of the model giving irrelevant responses?\n\n* In addition to the very incomplete quantitative results, there are also very few qualitative results. The qualitative results are only tested on one dataset. I would like to see the qualitative results and accuracy of image_00001.png, image_00005.png, image_00013.png, image_00036.png, image_00068.png in emoji_benchmark.\n\nThe main problem is the incompleteness of the experimental part. If the authors have a positive response, I will consider raising my rating."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 4.5,
    "decision": "Reject",
    "meta_review": "This paper proposes LVLM-COUNT, a divide-and-conquer framework to enhance the counting ability of large vision-language models (LVLMs) through four stages: area detection, target segmentation, object-aware division, and counting aggregation. Experiments are conducted on multiple datasets.\n\nThe main strengths are: 1) the innovative training-free framework with LVLMs, and effectively handles complex scenes with overlapping objects, and 2) good performance.   \nThe main weaknesses are: 1) inconsistent performance (the performance depends heavily on the accuracy of initial object detection and instance segmentation stages), and 2) insufficient experiments, including limited comparison with existing methods, lack of more detailed ablation studies to evaluate each stage of the pipeline, and lack of training and inference efficiency analysis.\n\nAfter rebuttal, the main issues of inconsistent performance and insufficient ablation studies still remain (recognized by Reviewers 4Qwc, and ATzA), which weakens the contribution of this paper. Thus, the AC does not recommend acceptance at this conference. The authors are encouraged to address these concerns for future submissions.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "qotIZREPZf",
    "title": "CGD: Modifying the Loss Landscape by Gradient Regularization",
    "authors": [
      "Shikhar Saxena",
      "Tejas Bodas",
      "Arti Yardi"
    ],
    "abstract": "Line-search methods are commonly used to solve optimization problems. The simplest line search method is the steepest descent where we always move in the direction of the negative gradient. Newton’s method on the other hand is a second-order method that uses the curvature information in the Hessian to pick the descent direction. In this work, we propose a new line-search method called Constrained Gradient Descent (CGD) that implicitly changes the landscape of the objective function for efficient optimization. CGD is formulated as a solution to the constrained version of the original problem where the constraint is on a function of the gradient. We optimize the corresponding Lagrangian function thereby favourably changing the landscape of the objective function. This results in a line search procedure where the Lagrangian penalty acts as a control over the descent direction and can therefore be used to iterate over points that have smaller gradient values, compared to iterates of vanilla steepest descent. We reinterpret and draw parallels with the Explicit Gradient Regularization (EGR) method, discussing its drawbacks and potential enhancements. Numerical experiments are conducted on synthetic test functions to illustrate the performance of CGD and its variants.",
    "keywords": [
      "optimization",
      "gradient regularization"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=qotIZREPZf",
    "forum_url": "https://openreview.net/forum?id=qotIZREPZf",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces Constrained Gradient Descent (CGD), a new line-search method that alters the objective function landscape for better optimization. CGD is based on a constrained version of the problem, optimizing the Lagrangian to control descent direction, potentially targeting points with smaller gradients than steepest descent. The authors relate CGD to Explicit Gradient Regularization (EGR), discussing its pros and cons, and validate CGD's performance through numerical tests on synthetic functions.",
        "strengths": "The paper is well written, and the method is well described.",
        "weaknesses": "* The statement in Section 3, especially the Lemma 1, is quiet confusing. Lemma 1 shows that the set of minimizers for the problem you defined in Equation (8) includes the set of minimizers for the original problem, which raises a convergence issue: where exactly will your defined algorithm converge to? The final convergence result might be inferior to the original gradient descent. This is also reflected in the results shown in Figure 2. Even with corrections made in the Algorithm 1, the convergence result is still not guaranteed.\n\n*  Your algorithm 2, CGD-FD, is quiet similar with traditional nesterov momentum gradient descent, except that the momentum term in the Nesterov algorithm has been replaced with the gradient at the current point. However, this article does not compare with any momentum-based methods, and it is not necessarily superior to these methods, because the Nesterov algorithm theoretically has a convergence rate of $O(1/k^2)$."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper proposes Constraint Gradient Descent (CGD) and its first-order variant CGD-FD, which use gradient regularization and finite-difference approximations for efficient optimization, and compare their performance against standard methods, while also re-evaluating Explicit Gradient Regularization techniques.",
        "strengths": "This paper study an important research issue and this paper is generally well written.",
        "weaknesses": "see questions."
      },
      {
        "rating": "1",
        "confidence": "5",
        "summary": "This paper proposes a gradient norm penalty to the original objective function and follows the gradient of the penalized objective, resulting in multiplying (I + lambda * Hessian) to the gradient step. This modification basically does not change the global optimum of the original optimization problem. While the original proposal requires the Hessian computation, its finite difference approximation is also proposed. The performance is compared to the standard GD on 6 test problems up to 10 dimensions.",
        "strengths": "A simple approach to accelerate the gradient descent.",
        "weaknesses": "Evaluation. The proposed approach is only compared with a naive GD with a constant step-size. It is definitely not enough to show the advantages of the proposed approach. It should be compared with momentum-based approaches such as NAG, quasi-Newton approaches such as BFGS and L-BFGS, and conjugate gradient methods. For the line search part, comparison with other line search methods based on Armijo condition or Wolf condition should be performed. Comparison with some commercial software such as Matlab optimization toolbox as a baseline is also helpful to show the advantage.\n\nThe search space dimensionality is also limited. Though the authors mention about DNN  at the beginning of this paper, the performance was tested only on 6 test problems up to 10 dimensions. Using benchmarking testbed with wider coverage such as CUTEst is recommended.\n\nNo theoretical justification is given. I am curious to know whether this approach improves the convergence rate."
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "The paper introduces Constrained Gradient Descent (CGD), an optimization technique that modifies the loss landscape by imposing constraints on the gradient's norm. A first-order variant, CGD-FD, is proposed by using finite-difference approximations of the Hessian to avoid computational cost. The work also highlights limitations of existing Explicit Gradient Regularization (EGR) methods.",
        "strengths": "The motivation is clear with nice visualization.",
        "weaknesses": "1. The introduction uses deep learning as a motivation but it should be others since the work is focusing on deterministic optimization. \n2. lacks literature review many other related algorithms. \n3. The contribution lacks novelty. The algorithm basically does the following: when the normalized Newton step is a descent direction, use that; Otherwise, just employ gradient descent. When $\\lambda$ is sufficiently small, it is guaranteed to employ a normalized Newton step. \n4. The authors misunderstands the line search method. Line search should be an adaptive step size scheme but the algorithm doesn't have such property. \n5. The algorithm introduces another hyperparameter $\\lambda$ to the algorithm. Compared to gradient descent, the tuning effort for CGD is greater.  \n6. lacks convergence result\n7. It's not enough for comparing the algorithms over 6 problems, and it is not fair to tune  $\\lambda$ for the CGD algorithm in the comparison with GD."
      }
    ],
    "rating_avg": 2.0,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "BlzBcWYmdB",
    "title": "Cross-modal Mitigation of Spurious Correlation for Prompt-tuning in VLMs with Causally Motivated Logic Alignment",
    "authors": [
      "Xueyang Tang",
      "Song Guo",
      "Xiaosong Ma",
      "Haoxi Li",
      "Jie ZHANG",
      "Yue Yu"
    ],
    "abstract": "Recent studies have shown that pre-trained vision-language models can effectively adapt to diverse downstream tasks through parameter-efficient prompt tuning. Unfortunately, the tuned models can exploit spurious correlations during prediction, resulting in a failure to generalize to out-of-distribution test data, especially when the tuning dataset exhibits bias. How to achieve cross-modal mitigation of spurious correlations during prompt tuning of vision-language models remains an open question. In this paper, the challenging problem is tackled by leveraging the stable relationship between necessary and sufficient causal features and the corresponding label. On the one hand, we constrain the learning process of prompt by reinforcing the necessary and sufficient connection between the textual labels and textual features. On the other hand, the probability of necessity and sufficiency between the textual features and the filtered visual features is measured and maximized to enhance cross-modal feature alignment. By iteratively optimizing these two objectives, we can achieve cross-modal mitigation of spurious correlations because the logic equivalence between textual labels and visual features is bolstered. The theoretical analysis on generalization error indicates that our method can achieve a tighter generalization error bound than existing approaches. We evaluate the proposed method on several commonly adopted out-of-distribution datasets, and the empirical results demonstrate the superiority of our method over the state-of-the-art competitors.",
    "keywords": [
      "Vision-Language Models",
      "Prompt Tuning",
      "Spurious Correlations",
      "Out-of-Distribution Generalization",
      "Causality",
      "Probability of Necessity and Sufficiency"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=BlzBcWYmdB",
    "forum_url": "https://openreview.net/forum?id=BlzBcWYmdB",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper presents a novel framework, LogicAl-PT, that addresses the challenge of cross-modal mitigation of spurious correlations in prompt tuning of vision-language models. The authors introduce a new concept, logic alignment, which integrates the mitigation of spurious correlations with cross-modal alignment of representations, and demonstrates its effectiveness through theoretical analysis and empirical results on various out-of-distribution datasets. LogicAI-PT earns competitive performance compared with traditional prompt-tuning methods for CLIP model.",
        "strengths": "Extensive theoretical analysis is provided to verify the proposed concept, PNS and PNS risk modeling.",
        "weaknesses": "- In contrast to the detailed theoretical analyses, the empirical verifications are fairly absent in this paper. Take the most recent competitor Coopood as an example, this paper presents much fewer empirical analyses, i.e. only 2 tables in the experiment section for verification. More ablation studies about the hyper-parameter chosen, visual results about the improvement on spurious correlations should be provided.\n\n-Some more recent prompt tuning methods[a][b] should be discussed and compared with.\n\n- Experiments on more architectures like ViT except for ResNet-50 should be done as well.\n\n[a] Self-regulating Prompts: Foundational Model Adaptation without Forgetting, https://arxiv.org/abs/2307.06948)\n[b] DePT: Decoupled Prompt Tuning, https://arxiv.org/abs/2309.07439"
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper presents Logical-pt, a framework for mitigating spurious correlations in vision-language models. It uses causally motivated logic alignment to align visual and textual features during prompt tuning. The method is backed by a tighter generalization error bound and empirically validated on several datasets, outperforming existing methods in out-of-distribution generalization.",
        "strengths": "* the results reported in this paper demonstrates good performance on multiple benchmarks\n* this paper did extensive evaluations and experiments to validate the method's effectiveness",
        "weaknesses": "* The proposed method shows superior performance compared with other benchmarks. What is the computational efficiency compare to simpler methods?"
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduces the novel LogicAI-PT framework to mitigate learning of spurious correlations in prompt tuning of CLIPs. It models the PNS (probability of necessity and sufficiency) by introducing intervention $\\bar{Q}$ and $\\bar{\\Phi}$. The author provide extensive introduction of the methodology and background and the experimental results are significant.",
        "strengths": "The proposed method is quite simple and effective regarding the significant improvement of multiple benchmarks. The idea of tackling spurious correlation problem from a sufficiency-necessity view is intuitive and is implemented based on thorough proof.",
        "weaknesses": "1. The proposed method seems to be universally applicable to many tasks rather than only prompting of VLM as classifier. Causal Representation Learning baselines adapting from other tasks can largely consolidate the motivation of this paper.\n2. Ablation study of the proposed is not enough. What is the effect of different $\\alpha, \\beta$? #419 introduces the author chooses different value combination for different benchmarks. The robustness regarding different hyper-parameters can largely affect the applicability of this method."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This article introduces the concept of logical alignment to address the cross-modal mitigation problem of spurious correlation for prompt adjustment in visual language models. To achieve this, the authors maximize the probability of necessity and sufficiency corresponding to cross-modal and textual logical alignment. Theoretical analysis proves that the proposed method has a tighter generalization error bound compared to existing approaches. Performance is analyzed across a few different test data distributions, and components of the method are ablated.",
        "strengths": "The generalization error bond of the presented method is provided with theory analysis in Appendix A. \n\nThe novelty of the method is how to integrate the probability of necessity and sufficiency in multi-modal learning.",
        "weaknesses": "The paper is not easy to follow. This is due to the symbols being confused, e.g., $\\Phi$ denotes the filter in Cross-modal logic alignment but visual representation space in Textual logic alignment. \nWhile the first half of the paper explains the idea and motivation well, creating a rightful sense of expectation of the result, the section on the results somewhat comes short of delivering the findings with a bang.  After reading the first half I was excited to read the next pages to find \"Where are those indeed integrated areas for boosting the expected performance \", and tingling with an expectation of learning something new. But then, for some reason, the Overall Performance and Ablation Study sections are very timid and just present dry numbers for each of the tests that were planned.  \n\n1. It would be helpful to include a better explanation of what the \"spurious correlation in vision-language models\" is exactly. Maybe a picture. \n2.The paper borrowed too much content from the existing papers and it could be removed by referring these papers. Even so, the motivation is not clear since the authors employ PNS without explanation.\n\n3. Page 6, Section 4.1: The NSC feature shown Figure 1 is not explained by the authors themselves how to make use and take advantage of this. \n\n4. Compared to CoOPood,  it seems that the proposed method exploits PNS terms instead of mutual information to align cross-modal representations. I am wondering how effective the PNS term is in cross-modal mitigation of spurious correlation.  \n5. Why was it necessary to do textual logic alignment?"
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This paper aims to tackle the issue of cross-modal spurious correlation for parameter-efficient prompt tuning of VLMs. This work pointed out that cross-modal mitigation of spurious correlations during prompt tuning of vision-language models remains an open question, and further proposed the logic of logic alignment and a practical framework to calculate the probability of necessity and sufficiency (PNS) between the textual label and textual representations.\n\nThis paper recevied diverse ratings, i.e., 6, 6, 5, 3. The AC has read reviewers' comments, authors' responses, and the revised version.  The idea of calculating the probability of necessity and sufficiency and analyzing the cross-modal spurious correlation for the aspect of causal inference is novel. The main reasons for reject are that (1) the paper has been revised significantly to include more experimental results and quantitive analysis compared to the original version, which indicates that the submission is not fully ready for publication, (2) although the performance comparisons with PromptSRC and DePT have been included in the revision, the performance gap between the proposed method, PromptSRC and DePT are marginal, and the reasons on why the gap is marginal is not discussed. (3) The presentation of the paper is not good, which is not easy to follow the idea and contributions. Therefore, the AC does not recommend the current submission as accept.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "ayT4e9C3Gd",
    "title": "ROSARL: Reward-Only Safe Reinforcement Learning",
    "authors": [
      "Geraud Nangue Tasse",
      "Tamlin Love",
      "Mark Nemecek",
      "Steven James",
      "Benjamin Rosman"
    ],
    "abstract": "An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, designing reward or cost functions is non-trivial and can increase with the complexity of the problem. To address this, we investigate the concept of a Minmax penalty, the smallest penalty for unsafe states that leads to safe optimal policies, regardless of task rewards. We derive an upper and lower bound on this penalty by considering both environment diameter and solvability. Additionally, we propose a simple algorithm for agents to estimate this penalty while learning task policies. Our experiments demonstrate the effectiveness of this approach in enabling agents to learn safe policies in high-dimensional continuous control environments.",
    "keywords": [
      "Reinforcement Learning",
      "Deep Reinforcement Learning",
      "Safety",
      "Safe AI",
      "Safe RL"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=ayT4e9C3Gd",
    "forum_url": "https://openreview.net/forum?id=ayT4e9C3Gd",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper addresses the difficulty of reward/cost engineering in Safe RL by proposing a minimax penalty. Particularly, it uses notions of environment diameter and solvability to bound on both sides and estimate the minimum penalty for unsafe states that leads to optimally safe policies independent of task rewards.",
        "strengths": "- well-motivated\n- extensive experimentation \n- good theoretical justification\n- good study of success/failure cases of proposed approach and baselines",
        "weaknesses": "- consider adding up/down arrows to indicate if higher/lower values are good in the graphs (Average Returns(↑) and Failure Rate(↓))\n- consider citing some works on feasibility/reachability in RL\n\nMinor errors:\n-Definition 1 typo “sThen”"
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper introduces Reward-Only Safe RL (ROSARL), a novel safe RL agent that focuses on learning appropriate penalties for unsafe states rather than using explicitly constructed constraints or human-designed penalties. The key technical contribution is the concept of a \"Minmax penalty\" - the smallest penalty for unsafe states that guarantees safe optimal policies regardless of task rewards. The authors derive theoretical upper and lower bounds on this penalty based on environment diameter and solvability. A practical algorithm is designed for estimating the minimax penalties during under model-free RL setting. The authors empirically demonstrate that the resulting algorithm yields stronger and safer performance on grid world and PILLAR environments, compared to selected baseline methods like CPO and TRPO-Lagrangian.",
        "strengths": "- The theoretical proofs are clearly and correctly stated.\n- The motivation is clear and well-written.\n- The proposed practical algorithm is simple (in a good) that can be integrated with existing RL methods.",
        "weaknesses": "- The practical algorithm uses a simplified estimate that does not explicitly consider solvability. Moreover, the algorithm leverages value function estimates as approximations without theoretical guarantees. More analysis for addressing the gap between the theoretically constructed objectives and the actual practical objective used would significantly improve the paper.\n- There are two main issues with the experimental study. Firstly, only two environments are used for empirical evaluation of the proposed algorithm, whilst one of them is grid world. Secondly, there are limited analysis of failure cases, which I believe might contain interesting insights. \n- Some minor issues. It would be useful to include some crucial baseline comparisons, such as PID-Lagrangian (Stooke et al. 2020). Also, there is no analysis or ablation studies on the computational complexity of the proposed algorithm, hindering its practical applicability."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper discusses a reward-only safe RL as an alternative to reward-shaping RL, and constraint-based RL for safe RL. The main idea is to compute the minmax penalty, which will be used to re-cast the problem so that a safe policy can be computed using standard RL methods. The method is evaluated on classical methods such as TRPO Lagrangian, CPO and a newer method called Saute RL with TRPO backbone. The environment is quite challenging and coming from the safety gym.",
        "strengths": "1. This is a different and possibly a new method to view safe RL \n2. The paper is quite well-written, however, I believe the algorithm is not described in the best way (see weaknesses) \n3. The authors present novel theoretical results and then discuss their strengths and limitations. In particular, they discuss convergence, computational complexity, applicability of the results",
        "weaknesses": "1. The algorithm is not well-described. It’s hard to understand the mathematical formulation of the problem and the solution. \n2. Experimental results are limited in scope and some of the conclusions are slightly misleading. \n            a. For example, Saute RL solves a problem with probability one constraints and cannot be directly compared to TRPO Lagrangian which solves the problem with constraints on average. Especially, when the environment is highly stochastic.\n            b. There are other baselines that the authors could compare to, to make their case. I recommend looking into implementations in https://github.com/PKU-Alignment/safety-gymnasium\n            c. There’s no ablation study\n3. Presentation of the paper focuses on the theoretical results, which are hard to judge without an explicit mathematical problem formulation. \n4. The experimental part is hard to evaluate due to the lack of details, for example the algorithm is not even in the main paper (it’s in appendix)."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper tackles the problem of solving tasks safely using reinforcement learning. The main idea of the paper is to introduce a term to appropriately penalise unsafe behaviour, the analysis of the paper seeks to bound the magnitude of this term to minimise the probability of arriving at an unsafe goal state while achieving the task. To develop this term, the authors establish several objects required to quantify the complexity and \"size\" of the problem. These two measures are then used to construct the additional reward term to adhere to the safety criterion. It is then shown theoretically that adding a reward term that adheres to certain inequality constraints ensures a high degree of safe behavior.",
        "strengths": "The paper addresses an important challenge with an intriguing idea. Beyond supplying solely theoretical results, the paper provides a practical method suggested by the theoretical analysis. The results suggest that the method leads to a notable reduction in safety costs which is one of the main stated goals of the paper. \n\nThe intuition of the paper is largely clear and most of the non-technical parts are nicely written and overall the concepts well-explained. The authors have included a practical example midway through the paper which is useful for discussing the ideas in a concrete setting and explaining some of the concepts.",
        "weaknesses": "===Analysis===\n\nA1. Some parts of the paper aren't written with a high level of rigour, for example, the experiments of Section 5 are divided into parts depending on whether the \"theoretical assumptions are satisfied\" but there is no clear statement as to what exact assumptions this refers to. Additionally, there are a few typos in the mathematical expressions e.g., the argument of the min and max operator should be $s_T$ not $s$. There are other examples and some more technical concerns I have such as the existence of $D$ - I have given more details of this concern below. Other more minor  points  are as follows:\n\n   **In Definition 2 the case of equality is not covered.\n\n   **Theorem 1 is a bit pointless - perhaps the authors could consider writing it as a note within the text.\n\nA2. I am finding it difficult to understand how the object in Definition 3 could be computed. Although the restriction to proper policies ensures the goal state is reached in a finite number of time-steps, in a given environment, for any deterministic policy I may be able to increase the number of time steps it takes to reach the goal by for example asking the agent to go back on itself before going forward again. In this case, it is unclear whether such a maximum exists (although the function T may be upper bounded on the set of proper policies, its image is not a closed set).\n\nA3. I found it slightly troubling that many of the key results and characterisations are derived using the solvability parameter $C$ an then because calculating this is impractical as the authors acknowledge, the algorithm put forward uses a replacement for $C$ without rigorous explanation as to whether the resulting calculations are good proxies. Indeed, although some intuition is provided as to why all is not lost by omitting $C$ in the calculation, the effect of this omission on the computations and overall behaviour is not properly studied.\n\n===Experiments===\n\nB1. A possible concern is that although from the empirical evaluation the method does reduce the total costs by an appreciable amount, this seems to come at a heavy price in the returns. Is there a configurable parameter that can control this trade-off. I would also like to see a comparison to the baselines that allow this trade-off to be configured, specifically considering a range of values for the parameter that controls this trade-off.\n\nB2. Overall, I did not find the experimental results to be extremely compelling, firstly the results themselves suggest that the method reduces costs at a significant price to the rewards but also, the environments are quite simple and the method has not been tested on a range of complex environments e.g. safety gym."
      }
    ],
    "rating_avg": 5.25,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "The paper proposes a safe RL algorithm, Reward-Only Safe RL (ROSARL), that learns penalties for unsafe states (behaviors) rather than using predefined penalties. The main technicality is the concept of minmax penalty, i.e., the smallest penalty for unsafe states that guarantees safe optimal policies (independent of task rewards). The authors derive theoretical bounds on this penalty based on environment diameter and solvability.They propose a method for estimating the minimax penalties in a model-free RL setting. Finally, they empirically evaluate their algorithm and compare it with CPO and TRPO-Lagrangian. \n\n(+) The paper addresses an important challenge with a novel idea. \n(+) The motivation behind the work is well-explained. The paper is well-written, although reviewers think the algorithm could be explained better and parts of the paper, especially in the experimental section are not written rigorously. \n(+) The paper has a good balance of theory and algorithmic implementation. \n\n(-) The reviewers found the scope of the experiments limited and some conclusions from them slightly misleading. They also feel comparison to newer baselines would be necessary. Overall, They did not find the experimental results compelling.\n(-) The practical algorithm is obtained using a rough estimate from the theoretical results. There is not much in the paper to identify the gap between the theoretically constructed objectives and the one used by the practical algorithm.\n(-) The reviewers are not satisfied with the way that the algorithm is explained. Moreover, they found parts of the paper, especially in the experimental section not written rigorously enough. \n\nI see this as a borderline paper. Thus, I strongly recommend that the authors revise their work using the reviewers' comments and prepare it for an upcoming venue.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "H9dNX6TaRE",
    "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Adaptive Importance Sampling",
    "authors": [
      "Nguyen Minh Phuc",
      "Ngoc-Hieu Nguyen",
      "Binh T. Nguyen",
      "Khoa D Doan"
    ],
    "abstract": "Recently, Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human values. Surprisingly, while DAAs do not use a separate proxy reward model as in RLHF, their performance can still deteriorate due to over-optimization – a phenomenon found in RLHF where the policy can exploit failures of the reward model to achieve high rewards but the actual quality of the model begins to degrade. Recent studies find that DAAs tend to increase probability mass on out-of-distribution responses and the training objective in DAAs is heavily under-constrained on these out-of-distribution (OOD) responses due to a mismatch between offline distribution and the LM policy. In this paper, we propose a method to mitigate the distribution shift between the offline distribution and the LM policy by multiplying with an importance weight to reflect the policy distribution. The resulting method, called Adaptive Importance Sampling (AIS), relies on importance sampling techniques and resolves the high variance issue in importance sampling without extra hyper-parameters. Our experiment results showed Adaptive IS can improve win rates by 15% while maintaining a lower KL budget compared to DAAs.",
    "keywords": [
      "Reinforcement Learning From Human Feedback",
      "Direct Preference Optimization",
      "Reward Hacking"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=H9dNX6TaRE",
    "forum_url": "https://openreview.net/forum?id=H9dNX6TaRE",
    "reviews": [
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This work addresses the reward optimization problem in direct alignment algorithms (DAAs) from the angle of distribution shift. In existing DAAs, the KL estimation is only unbiased when the samples are on-policy. However, as the policy being updated during learning, the responses from the offline dataset become off-policy, and thus distribution shift happens.\n\nTo address this issue, the authors propose adaptive importance sampling (AIS) as a solution. Assuming the preference data are generated from the SFT policy, AIS applies an importance sampling weight on each data point to correct the off-policyness. This weight term is further adapted by an exponential coefficient which is the inverse of the response length to tradeoff the bias and the variance. AIS is first evaluated in a toy example and demonstrates better estimation of the KL divergence than its unweighted counterpart. When combined with DPO, AIS demonstrates better KL-win rate tradeoff and higher peak performance than the baseline in a simulated setup, following Gao _et al_, 2022. The authors also conducted some empirical analysis in the simulation setup to understand the detriment of distribution shift.",
        "strengths": "This work addresses a widely observed phenomenon where DAAs like DPO suffers from reward overoptimization even before completing the first epoch of the dataset. Insights into this phenomenon can help us understand the underlying mechanism of DAAs  and resolving this issue can mitigate the gap between online and offline algorithms and can provide us with computationally cheap yet performant alignment algorithms.\n\nThe proposed solution, AIS, is a principled algorithm with well-understood theoretical grounding. Empirically, AIS demonstrates more effective KL regularization and strong performance over the baseline. AIS is simple, easy to implement, and preserves the low computational cost of offline DAAs.\n\nIn terms of presentation, overall the paper is easy to follow. The work is well motivated and the method is clearly explained. The authors did a good job connecting to existing works in the literature.",
        "weaknesses": "Important analysis on the proposed AIS method is missing. Importance sampling is one of the simplest techniques to address off-policy learning. The authors claim that vanilla IS suffer from high variance and thus an adaptive heuristic is applied to make a tradeoff between the bias and the variance. However, there is no analysis into this adaptive heuristic to justify its necessity and to provide insights into how this tradeoff impacts the overall performance.\n\nSimilarly, the empirical analysis in Section 4.3 demonstrates the detriment of distribution shift to DAAs. One natural question to ask is, as a method proposed for addressing distribution shift, how does AIS perform in these experiments? The current study does not provide any results to answer this question.\n\nOne limitation the authors did not call out in the limitation section is that AIS assumes that the preference dataset is generated from the SFT policy. However, this is not always the case in practice. Usually the responses in the preference dataset are sampled from different generations of the same data class, or even from different model classes. Thus this assumption is often violated and it hinders the effectiveness of AIS.\n\nPresentation-wise, the authors use inconsistent / incorrect citation formats through the paper. Calandriello _et al_ '24 should be cited in Section 3.1 for online DDAs. There are a few typos in writing. I think it should be \"budget\" in the last sentence of the abstraction. The Azar '23 and Gheshlaghi Azar '24 citations are citing the same paper. \n\n## References\nCalandriello _et al_ '24, Human Alignment of Large Language Models through Online Preference Optimisation, ICML 2024."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper primarily deals with the issue of reward over-optimization in specifically Direct alignment algorithms and proposes an adaptive low variance importance sampling strategy to mitigate the issue, with an exponential smoothing technique that balances bias and variance in IS estimates. The proposed method effectively reduces over-optimization by achieving higher model win rates and maintaining a lower KL divergence budget than baselines.",
        "strengths": "The paper primarily deals with the issue of reward over-optimization in specifically Direct alignment algorithms. The over-optimization issue is an extremely critical concern in the current alignment paradigms, and arises due to a distributional shift between offline training data and the LM's current policy, leading to increased probability on out-of-distribution (OOD) responses. The paper introduces an adaptive importance sampling strategy to mitigate the distributional shift issue using an exponential smoothing technique that balances bias and variance in IS estimates.",
        "weaknesses": "1. The importance sampling term defined in the equation in line 212, suggest that the original equation is E_{\\pi_{\\theta}}[\\rho_theta]? Can you mathematically show why thats the case? In the context of online RLHF, it makes sense as shown in [1], but in offline whats the exact ideal optimization objective, leading to this importance weight? Can you specify, will be helpful. Also, highlight the difference from [1].\n2. Whats the mathematical motivation behind choosing the value of the alpha? How does it affect the convergence?\n3. There are several works on pessimism based methods to achieve reward over-optimization which are similar in principles, hence its not clear the novelty of the proposed work. A detailed comparison and contrast is critical to understand the novelty of the proposed approach.\n\nReferences:\n\n[1]. Sail: Self-improving efficient online alignment of large language models\n\n[2]. Iterative data smoothing: Mitigating reward overfitting and overoptimization in rlhf\n\n[3]. Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer\n\n[4]. Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer"
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "This paper proposed to use adaptive importance sampling during offline post-training alignment of LLMs as a way to reduce over-optimization. They use a smoothed exponential IS estimator (where the exponent is the reciprocal of the length of the generation) in order to reduce the variance of the IS in exchange for some bias. Their experiments show that with this smoothed IS correction, they are able to reduce over-optimization in DPO and IPO and reach better performance in a lower KL budget in the TL;DR summarization task. They also show that distribution shift is indeed problematic and makes over-optimization worse.",
        "strengths": "-  The idea is simple and easy to integrate into existing algorithms like DPO and IPO as done in the paper.\n-  The paper shows clear gains in terms of less overfitting and better performance per KL budget.",
        "weaknesses": "Overall, some more ablations or more in-depth investigation is lacking. There isn’t a good understanding of how important picking alpha is for the experiments. There is also not an investigation into how distribution shift (section 4.3) interacts with IS. See questions for more details."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper studies the reward-optimization issue in aligning large language models. It focuses on direct alignment methods, such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). The paper argues that these issues arise from off-policy distribution shifts between the learning policy and the reference policy. Accordingly, an importance-sampling weighting term with adaptive schemes is proposed. Experiments with Pythia models on the TL;DR dataset are conducted.",
        "strengths": "- The idea of using importance sampling to address distribution shift is not new, but it sounds interesting in the context of direct alignment methods.\n- This paper is well-written and easy to follow.\n- Numerous empirical results are presented, along with their limitations (see below).",
        "weaknesses": "- This paper lacks technical depth. It studies the distribution shift issue in DPO, which is a valuable perspective. Unfortunately, it fails to explicitly point out or mention that DPO's gradient estimator is not unbiased because the data distribution is defined by the data-collection distribution policy $\\pi$ (see previous works [1, 2]). Furthermore, it fails to justify that the proposed gradient estimator is unbiased. The reviewer believes that it is not theoretically unbiased. In fact, the importance sampling weight requires the optimal policy $\\pi^*$, which is not available a priori.\n\n[1] Liu, Tianqi, et al. \"Statistical rejection sampling improves preference optimization.\" *arXiv preprint arXiv:2309.06657* (2023).\n\n[2] Xiong, Wei, et al. \"Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint.\" *Forty-first International Conference on Machine Learning*. 2024.\n\nFrom the reviewer's perspective, there are two factors in DPO's formulation that prevent it from finding the true optimal policy:\n\nFirst, DPO uses a fixed and offline dataset, where the data distribution does not originate from the optimal policy. \n\nSecond, DPO employs KL regularization with a fixed policy. To address these issues, two simple strategies can be applied: periodically updating the reference policy [3] or using entropy regularization [4].\n\n[3] Guo, Shangmin, et al. \"Direct language model alignment from online AI feedback.\" *arXiv preprint arXiv:2402.04792* (2024).\n\n[4] Xiao, Jiancong, et al. \"On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization.\" *arXiv preprint arXiv:2405.16455* (2024).\n\n- The superiority over other simple baselines is unclear. A straightforward way to address the distribution shift issue is to use a moving average of the reference policy that can ensure the policy moving beyond the KL contraint. \n\n- Experimental results are weak. The experiments are conducted on the TL;DR dataset, which unfortunately has very short response lengths, and the Pythia model used as a base is quite weak. Consequently, empirical conclusions and insights may have limited value for modern language models. Moreover, some experiment details are missing, which hinders reproducibility and understanding of key results."
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "This work proposes methods to mitigate the reward over-optimization issues in direct alignment algorithms. The algorithm proposed is a low variance importance weighted sampling strategy that is meant to minimize this issue. Unfortunately, some technical issues were raised by the reviewers. These are things such as the unbiasedness of the DPO gradient estimator and others.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "ErpRu7qMq1",
    "title": "GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework",
    "authors": [
      "Ang Lv",
      "Xu Tan",
      "Peiling Lu",
      "Wei Ye",
      "Shikun Zhang",
      "Jiang Bian",
      "Ji-Rong Wen",
      "Rui Yan"
    ],
    "abstract": "Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrument tracks based on provided source tracks. In practical scenarios where there’s a predefined ensemble of tracks and various composition needs, an efficient and effective generative model that can generate any target tracks based on the other tracks becomes crucial. However, previous efforts have fallen short in addressing this necessity due to limitations in their music representations and models. In this paper, we introduce a framework known as GETMusic, with “GET” standing for “GEnerate music Tracks.” This framework encompasses a novel music representation “GETScore” and a diffusion model “GETDiff.” GETScore represents musical notes as tokens and organizes tokens in a 2D structure, with tracks stacked vertically and progressing horizontally over time. At a training step, each track of a music piece is randomly selected as either the target or source. The training involves two processes: In the forward process, target tracks are corrupted by masking their tokens, while source tracks remain as the ground truth; in the denoising process, GETDiff is trained to predict the masked target tokens conditioning on the source tracks. Our proposed representation, coupled with the non-autoregressive generative model, empowers GETMusic to generate music with any arbitrary source-target track combinations. Our experiments demonstrate that the versatile GETMusic outperforms prior works proposed for certain specific composition tasks.",
    "keywords": [
      "Symbolic Music Generation",
      "Symbolic Music Representation",
      "Diffusion Model"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=ErpRu7qMq1",
    "forum_url": "https://openreview.net/forum?id=ErpRu7qMq1",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper has two main contributions:\n\n1) A symbolic music representation consisting of a tracks-by-timesteps grid, where each grid cell contains a pitch token and a duration token.  Polyphony is handled by encoding a *combination* of pitches as a single token.\n\n2) A discrete diffusion framework that can handle arbitrary conditional generation tasks on the symbolic music grid, including unconditional generation.  For conditional tasks, the paper introduces extra flags that indicate whether each grid cell is part of the conditioning.",
        "strengths": "1) As far as I know this is the first application of discrete diffusion to symbolic music generation.\n\n2) The generated samples sound quite good!\n\n3) Evaluation seems good, with the caveat that I don't really trust any evaluation of generative music models :)",
        "weaknesses": "These are not in order of importance.\n\n1) There are already more symbolic music generation representations and models out there than I can keep track of, and they all sound pretty decent.  I consider this problem basically \"solved\" since the release of OpenAI's MuseNet (which had no accompanying academic paper).  It's not clear that this paper is a significant advance on what is already possible.\n\n2) The CoCoNet model by Huang et al. (https://arxiv.org/abs/1903.07227) uses a setup that is very similar to this paper: multiple tracks are generated with arbitrary segments fixed as conditioning; instead of diffusion, the remaining portions are generated iteratively using Monte Carlo sampling.\n\n3) The paper seems confused about the taxonomy of symbolic music representations, dividing the space into \"image-based\" and \"sequence-based\" representations.  Here it would make sense to examine the pitch and time axes separately.  Either axis can be treated in dense (\"image-based\") or sparse (\"sequence-based\") fashion.\n\n    With time, the main reason one might use a sparse approach is to handle expressive timing; the dense resolution becomes extremely high.  This paper does not model expressive timing and thus uses a dense approach, with exactly two tokens per time step.  However, it's worth noting that the approach in the paper cannot easily be extended to handle not only expressive timing, but also things like triplets, without blowing up the time dimension.\n\n    With pitch, the main reason to use a dense approach is to handle polyphony; for monophonic music the pitch axis can be collapsed into a single value at each time.  However, for many polyphonic instruments e.g. piano, the space of possible pitches is quite large, making sparsity desirable.  This paper handles polyphony in a somewhat unique way, flattening variable-length combinations of notes into single tokens (see next item).\n\n4) The handling of polyphony is very unsatisfying.  For example, all combinations of piano notes are compressed to a vocabulary of 1555 tokens.  This isn't even enough to represent all pairs of piano keys!  And the drum vocabulary is almost 3 times as large as the piano vocabulary; how did this end up happening?\n\n    Here's a way polyphony could potentially have been handled that only minimally changes the setup.  On the input side, instead of blowing up the vocabulary with combinations of pitches, sum (or average) the token embeddings of all active pitches.  On the output side, instead of a softmax over pitch combination tokens, sample the binary presence/absence of each pitch independently (for a diffusion model, this independence should be okay since the other cells are sampled independently anyway) then sparsify.  This shouldn't increase memory usage since you need to construct the softmax vector anyway.\n\n    (It's entirely possible that you already tried the above suggestion and it ended up not working; if so please disregard.)\n\n5) I am not especially knowledgable about diffusion modeling, even less so about discrete diffusion.  But it's not clear whether the method in this paper goes beyond the standard approach.  From what I can tell, the use of condition flags is new, but that raises the question of why previous discrete diffusion methods didn't need to use such flags, and the paper provides no discussion of this."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces GETMusic, a framework designed for versatile symbolic music generation that supports generating any target instrument tracks based on provided source tracks. The GETMusic framework has two main components: GETScore, a novel music representation method, and GETDiff, a diffusion-based generative model.",
        "strengths": "The paper is well-structured and clearly presented, and it addresses several important scenarios for conditional generation in symbolic music generation.",
        "weaknesses": "The biggest weakness is the limited contribution, as diffusion models for symbolic music and conditional track generation have already been explored in previous work such as AccomMontage, SongDriver. The new representation method also lacks comparisons with alternative approaches.\n\nThe experiments are incomplete; each contribution requires validation. For instance, it’s unclear how the representation method outperforms others or how the diffusion model improves over baseline diffusion models. Additionally, more recent works should be included in task-level comparisons, as PopMAG was introduced four years ago. \n\nThis also suggests that the related work survey is incomplete, omitting recent studies on conditional generation in symbolic music."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper describes a system for multi-track symbolic music generation, _GETMusic_. The authors introduce a new musical representation, _GETScore_, which compactly represents multi-track music in a two-dimensional token-based structure, and a neural model, _GETDiff_, a non-autoregressive discrete diffusion model trained to predict randomly masked tokens from symbolic music represented as a GETScore. The authors build on recent literature including piano-roll generation using diffusion models, and next-token autoregressive symbolic music generation. They evaluate their proposed system with objective musical metrics, as well as with a subjective listening test, comparing the musical quality with that of previous models. In both cases, the proposed approach performs better than the baseline.",
        "strengths": "Overall, this paper presents a generative system, which within the context of non-autoregressive generative models for symbolic music is an advancement. More specifically:\n\n- The GETScore musical representation is well motivated, formulated, and clearly described. The reviewer deems the proposed representation as novel in the context of previous literature. \n\n- As the authors note, GETScore is quite compact compared to the commonly used piano-roll representation. Even judged alone, GETScore is a serious contribution, and it's very possible that this representation will be useful for a variety of generative and MIR tasks, which could be useful to the community.\n\n- The details of GETScore and GETDiff are clearly presented. Specifically, Figures 2 and 3 are very helpful to understand the intricacies of GETScore.\n\n- There are a significant number of experiments presented in Section 4. Although systems for generative symbolic music are notoriously hard to evaluate, the authors make a significant attempt to do so as rigorously as they can. In all cases, the proposed approach performs excellently. \n\n- The musical samples provided on the demo page are impressive, suggesting that subjectively, this framework does well at symbolic music generation.",
        "weaknesses": "- There is a potentially significant issue of missing references. The training objective and inference process for the proposed model GETDiff is quite similar in nature to those used in Huang et al. [1], in which the authors propose a discrete training objective, predicting missing notes from piano-rolls which have been randomly partially masked. At inference time, they use blocked Gibbs sampling, which is reminiscent of the inference procedure outlined in Section 3.2. Although the proposed approach is multi-track, and the framing of GETDiff as a discrete diffusion model changes the loss function, at the very least this work should be referenced and the similarities should be addressed in the related work. Some other relevant references are also missing, and are not present in ablation experiments, such as [2].\n\n- If I understand correctly, the ablation experiment in Section 5 (L457-463) is not very well designed. By using 14 separate prediction heads and presumably sampling each column in the GETScore with a single forward pass, the training objective isn't accurately represented for GETDiff AR. As an example, when predicting the length of a note, it is impossible for the model to condition directly on the pitch of the note that it is predicting, and instead can only condition implicitly on the distribution of possible pitches predicted by the model. This introduces mathematical issues which may be responsible for the degraded performance. A much better ablation would be to compare against a transformer-decoder trained to predict the next token for a flattened version of GETScore. This should be technically possible as it would only require a context-length of 512*14=7168. \n\n- There are some very minor issues about expressivity. According to our understanding, it is not possible to represent concurrent notes (e.g., chords) within a single track that have differing offsets.\n\n[1] Huang, C.Z.A., Cooijmans, T., Roberts, A., Courville, A. and Eck, D., 2019. Counterpoint by convolution. arXiv preprint arXiv:1903.07227.\n\n[2] Thickstun, J., Hall, D., Donahue, C. and Liang, P., 2023. Anticipatory music transformer. arXiv preprint arXiv:2306.08620."
      }
    ],
    "rating_avg": 4.0,
    "confidence_avg": 3.6666666666666665,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "9e5syenoVE",
    "title": "Multiple-play Stochastic Bandits with Prioritized Resource Sharing",
    "authors": [
      "Hong Xie",
      "Yanying Huang",
      "Haoran Gu",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "This paper proposes a variant of  multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, \nedge intelligence applications, etc.  The proposed model is composed of $M$ arms and $K$ plays.  Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function.  Each play is associated with a priority weight.  \nWhen multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first  manner.  Instance independent and instance dependent regret lower bounds of $\\Omega( \\alpha_1 \\sigma \\sqrt{KM T} )$ and $\\Omega(\\alpha_1 \\sigma^2 \\frac{MK}{\\Delta} \\ln T)$  are proved,  where $\\alpha_1$ is the largest priority weight and $\\sigma$ characterizes the reward tail.  \nWhen model parameters are given, we design an algorithm named \\texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(M^3K^3)$.   Utilizing \\texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $K \\sqrt{ \\ln KT }$ and $\\alpha_1 K$ respectively.   To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.",
    "keywords": [
      "Multiple-play stochastic bandit",
      "prioritized resource sharing",
      "regret bounds"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=9e5syenoVE",
    "forum_url": "https://openreview.net/forum?id=9e5syenoVE",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper works within the MP-MAB framework and proposes a variant of the framework catered to LLM and edge intelligence applications. With these applications in mind, the work imposes additional structure in the form of their MSB-PRS Model. \n\nIn Section 3 they introduce the model and provide the problem formulation. In Section 4 they characterize the hardness of the problem and fundamental learning limits by providing lower bounds, In Section 5 they present their UCB based learning algorithms, and in Section 6 they present experiments validating their approach and comparing it to baselines from the literature.",
        "strengths": "1. New variant of MP-MAB with resource prioritization.\n2. Lower bounds characterizing the hardness of their problem variant\n3. UCB based algorithm for learning - ApUCB\n4. Instance dependent and instance independent upper bounds on ApUCB",
        "weaknesses": "In my opinion while there seem to be innovative and impactful ideas in the paper it can use a lot better presentation before being accepted. In this bullet I would highlight how Section 3 on the MSB-PRS Model is barely readable by being cluttered by endless notation. Such a presentation makes it incredibly hard to takeaway any intuitive mental pictures of the setup that could then serve as the basis of appreciating the methods presented in the remaining paper.\n\nActionable Suggestions:\nPlease add a high-level overview paragraph in Section 3 before introducing the model mathematically using the complete notation. Please include an illustrative example or visual representation of the MSB-PRS model alongside this new paragraph.\n\n2. The paper does a poor job of motivating their target applications with the exposition being limited to a few lines in the introduction with vague wordings. In particular the only reference to LLM applications reads the following in the introduction: \"in LLM applications, reasoning tasks and LLM instances can be modeled as plays and arms respectively. .... priority quantified by price, membership hierarchy\". Which by itself gives very little insight into how the modeling in the paper is good for this application.\n\nI would encourage the authors to expand on their motivating example by dedicating a sub section to explaining how the MSB-PRS model applies to LLM and edge intelligence applications."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper presents a new framework called Multiple-Play Stochastic Bandits with Prioritized Resource Sharing (MSB-PRS), which belongs to the research area of multi-play multi-armed bandit. Within this framework, an efficient algorithm is developed to identify the optimal play allocation policy while maintaining low computational complexity. The study establishes lower bounds for both instance-independent and instance-dependent regret. Additionally, the proposed algorithm is based on the application of the classic Upper Confidence Bound (UCB). It maintains the same per-round computational complexity and achieves sublinear regret upper bounds that closely align with the established lower bounds.",
        "strengths": "The multi-play multi-armed bandit (MP-MAB) problem is a significant model in online learning, and I appreciate the efforts that the authors intest in solving an interesing model of it, that is the MSB-PRS problem. For it, an algorithm has been developed to identify the optimal play allocation policy with a specific complexity. The upper bounds on regret are close to the lower bounds (up to some factors) in both instance-dependent and instance-independent scenarios.",
        "weaknesses": "My main concern is that while this work provides rigorous theoretical analysis and proofs, I am still not entirely clear on its contributions. \n\nFirst, although the problem model is somewhat introduced, I find it challenging to connect it with specific examples of resource allocation. While the authors mention its applicability in high-interest areas like LLMs, they do not provide corresponding explanations. Is there a way to contextualize its application in LLMs, or could examples of practical applications be included? \n\nSecond, the authors offer some related work, but I still struggle to compare them with this study. To address this, I suggest including a table to compare the results of this work with previous findings. \n\nFinally, the experiments are overly simplistic; they do not thoroughly describe the experimental setups or compare it with other studies. While I appreciate the authors' efforts in deriving theoretical results, I believe there is still significant room for improvement in the presentation of this work."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper extends the multi-play multi-armed bandit (MP-MAB) model to include a prioritized resource-sharing mechanism, referred to as MSB-PRS. The model targets resource allocation scenarios in LLM and edge intelligence applications, where different plays are assigned different priorities, and each arm has multiple but random capacities. The authors establish both instance-independent and instance-dependent regret lower bounds for the model and propose an efficient learning algorithm, MSB-PRS-ApUCB, which achieves order-optimal regret bounds. The authors also conduct simulations based on synthetic data to validate their proposed algorithm.",
        "strengths": "1. The introduction of prioritized resource sharing into the multi-play bandit framework is novel, nabling random arm capacities and differentiated priorities for various plays, which are well-suited for practical applications such as LLM and edge intelligence.\n\n2. The proposed MSB-PRS-ApUCB algorithm is thoughtfully designed and well-motivated, achieving regret bounds that closely align with the established lower bounds, up to acceptable factors.\n\n3. The synthetic experiments provide a good assessment of the performance of MSB-PRS-ApUCB compared to baseline algorithms.\n\n4. The paper is well-structured and clearly written, making it easy to follow.",
        "weaknesses": "1.  The learning component of the algorithm and the regret analysis are fairly standard, as there exists an optimal matching between players and arms, and the remaining thing is to target this optimal matching through UCB strategy, as done in much of the literature. However, I acknowledge that finding the optimal matching is not easy due to the nonlinear combinatorial structure of the utility functions.\n\n2.  The concentration bound in Lemma 5.4 seems incorrect. I believe the authors should use Lemma 9 from [Maillard, et al., 2017] instead of Lemma 10. Consequently, the inequalities in lines 977, 1006, and 1053 also appear to be incorrect. The authors should check these carefully."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper considers a variant of the stochastic bandit problem where players can select multiple arms from a pool consisting of a fixed number of arms, with each arm's capacity following a time-invariant distribution. Players receive rewards only if the chosen arms have sufficient corresponding capacity. The objective is to maximize cumulative rewards over a fixed-horizon game. To address this problem, the paper proposes a new algorithm based on the philosophy of combinatorial bandits, along with learning the capacity distribution, under the assumption of an oracle's existence. For the proposed algorithm, the paper provides both lower and upper bound analyses on the regret to demonstrate its (near) optimality. Numerical experiments are conducted to validate the proposed algorithm and demonstrate improvements compared to benchmarks.",
        "strengths": "1. The paper considers a novel problem setting where the arm capacity is stochastic, unlike existing work.\n2. The paper also develops algorithms specifically to address this proposed problem setting.\n3. The theoretical effectiveness of the proposed algorithm is supported through both lower and upper bounds.\n4. The numerical experiments help illustrate the algorithm’s performance.",
        "weaknesses": "1. The paper mentions a use case for this problem setting in the LLM context. However, I am curious if this could be more practical, specifically whether it is something that could feasibly be deployed in that context.\n\n2. The existence of an oracle depends on locating the maximum weight matching, which is referenced from existing work. I wonder if this reference includes any theoretical guarantee supporting the claim that this oracle is theoretically optimal. Further justification would be beneficial here.\n\n3. The lower bound analysis lacks technical novelty.\n\n4. The real challenge posed by stochastic capacity is unclear. Existing work assumes deterministic arm capacity without requiring it to be known. It is difficult to assess whether the stochastic realization actually makes the problem more challenging (due to randomness) or possibly easier (given known observations)."
      }
    ],
    "rating_avg": 4.5,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This paper presents a new framework called Multiple-Play Stochastic Bandits with Prioritized Resource Sharing (MSB-PRS), which belongs to the research area of multi-play multi-armed bandit. Within this framework, an efficient algorithm is developed to identify the optimal play allocation policy while maintaining low computational complexity. There are many concerns raised by the reviewers for motivation and contribution, which are not addressed by the authors.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "p5FWeNp5PC",
    "title": "Hessian-Informed Flow Matching",
    "authors": [
      "Christopher Iliffe Sprague",
      "Arne Elofsson",
      "Hossein Azizpour"
    ],
    "abstract": "Modeling complex systems that evolve toward equilibrium distributions is important in various physical applications, including molecular dynamics and robotic control. These systems often follow the stochastic gradient descent of an underlying energy function, converging to stationary distributions around energy minima. The local covariance of these distributions is shaped by the energy landscape's curvature, often resulting in anisotropic characteristics. While flow-based generative models have gained traction in generating samples from equilibrium distributions in such applications, they predominately employ isotropic conditional probability paths, limiting their ability to capture such covariance structures.\n\nIn this paper, we introduce Hessian-Informed Flow Matching (HI-FM), a novel approach that integrates the Hessian of an energy function into conditional flows within the flow matching framework. This integration allows HI-FM to account for local curvature and anisotropic covariance structures. Our approach leverages the linearization theorem from dynamical systems and incorporates additional considerations such as time transformations and equivariance. Empirical evaluations on the MNIST and Lennard-Jones particles datasets demonstrate that HI-FM improves the likelihood of test samples.",
    "keywords": [
      "Flow Matching",
      "Deep Generative Models",
      "Dynamical Systems"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=p5FWeNp5PC",
    "forum_url": "https://openreview.net/forum?id=p5FWeNp5PC",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "NNBAzdF7Cg",
    "title": "Binary Spiking Neural Networks as causal models",
    "authors": [
      "Aditya Kar",
      "Emiliano Lorini",
      "Timothée Masquelier"
    ],
    "abstract": "In this paper, we provide a causal analysis of  binary spiking neural networks (BSNNs)\naimed at explaining their behaviors. \nWe formally define a BSNN \nand   represent its  spiking activity\n  as a binary causal model.\nThanks to this causal  representation, \nwe are able to explain the output of the network\nby leveraging  logic-based  methods. \nIn particular,\nwe show that we  can successfully \nuse a SAT  (Boolean satisfiability) solver to  compute \n  abductive explanations from this  binary causal model. \nTo illustrate our approach, \nwe trained the BSNN on the standard MNIST\ndataset and applied our SAT-based  method  to\nfinding  abductive  explanations of  the network's classifications\nbased on pixel-level features. We also compared the found explanations against SHAP,  a popular \nmethod used in the area of explainable\nAI to explain ``black box'' classifiers.\nWe show that, unlike SHAP,\nour method guarantees that a found  explanation  does\nnot contain completely irrelevant features.",
    "keywords": [
      "Explainability",
      "Causal reasoning",
      "Spiking Neural Networks",
      "White-box"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=NNBAzdF7Cg",
    "forum_url": "https://openreview.net/forum?id=NNBAzdF7Cg",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper introduces a novel approach to explaining Binary Spiking Neural Networks (BSNNs) by mapping their spiking activity into binary causal models (BCMs). The authors develop a SAT-based method for generating abductive explanations, ensuring only causally relevant input features are included, which advances interpretability and minimizes redundancy. This approach is unique in leveraging Boolean logic to capture the temporal dynamics of BSNNs, setting it apart from standard explainability methods like SHAP. Experimental results show that this method produces accurate and computationally efficient explanations, highlighting features that directly impact the model's decisions. Overall, the work provides a structured, logic-driven framework for enhancing transparency in spiking neural networks.",
        "strengths": "Since I am not really into causal models but in spiking NNs, it is hard for me to judge about the originality of the contribution. To me, the paper seems to be original, applying binary causal models to BSNNs in a way that uniquely captures their temporal dynamics through Boolean logic, setting it apart from existing explainability methods, especially in comparison to SHAP. The approach is communicated clearly, with definitions and examples that effectively illustrate the novelty of causal explanations in BSNNs. Overall, the paper provides a robust, innovative framework that could influence future standards in model transparency and causal explainability, if the authors can show that the framework can be generalized to larger real-world networks and problems. (I did not check the proof in the Appendix).",
        "weaknesses": "My main concern over this paper is the current presentation as a two-layer-only network (one hidden layer). It is hard to imagine all consequences when this approach is generalized to multiple hidden layers. My impression is that the computational effort of Algorithm 1 would increase exponentially, thus effectively excluding the possibility of applying the method to real-world problems."
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper proposes a causal-based interpretability method by mapping Binary Spiking Neural Networks (BSNNs) into binary causal models. Using a SAT solver to compute abductive explanations. This provides a new perspective for interpreting BSNNs and advancing BSNN research further.",
        "strengths": "As the authors stated, this is the first time BSNNs have been interpreted as causal models. I believe this provides a new perspective for understanding BSNNs.",
        "weaknesses": "1. This paper primarily relies on extensive formal language for its exposition. Adding some figures would be beneficial to enhance readers' understanding of the content.\n2. The experiments are limited to the MNIST dataset. It is recommended to include some other, more complex datasets for support."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper presents a causal analysis of binary spiking neural networks by representing the spiking activity as a binary causal model and applying this model to a SAT (Boolean satisfiability) solver.\n\n\n---\n\nAfter reading the reviews and the rebuttal, I tend to accept this paper.",
        "strengths": "1. The idea of bridging SNN and Causal Inference is interesting.\n\n2. The experiments related to SAT solver seem significant.",
        "weaknesses": "1. This paper is hard to follow due to the poor presentation. Some symbols are confused.\n\n2. The motivation that employs BSNN rather than BNN is not clear. I cannot get the necessity of using spiking mechanism. Thus, it is better to explicitly compare the advantages of BSNNs over BNNs in the context of causal modeling."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors introduced a method mapping binary (or ternary) spiking neural networks to binary causal models, which can then be used to perform abductive explanations (via a SAT solver) for the network's behavior. They applied this method to the MNIST classification task (3 classes for the binary case and 10 classes for the ternary case). The authors claim that their method provides a better explanation compared to SHAP, another explainability method.",
        "strengths": "- The idea of using binary causal models to explain binary spiking neural networks is novel\n- The technical aspects of the paper are precise and rigorous; the authors provide precise mathematical definitions and prove the proposition brought forth in the paper\n- The paper is written in an easy-to-follow manner",
        "weaknesses": "- It is not clear to me how the explanation provided by the binary causal model is a \"good\" explanation. While the authors make the implication that their method provides a better explanation than SHAP as SHAP can select features that are irrelevant, I think the paper would be improved if it included some evaluation metrics for explainability and, if possible, other bechmark methods alongside SHAP.\n- The proposed method seems to take a long time in searching for an explanation using the SAT solver, ranging from 5-11 hours, and this is  just for MNIST limited to 3 classes. It seems unlikely that this method is scalable to larger scale problems.\n- The authors do not report the results (both accuracy and computational analysis) for the BCNN (binary, not ternary) on the 10-digit MNIST dataset."
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.25,
    "decision": "Reject",
    "meta_review": "This submission presents an explainability technique for binary spiking neural networks that makes use of a satisfiability solver. Reviewers agreed that the idea of bridging spiking neural networks with causal inference through binary causal models is novel and interesting. The paper provides a clear technical exposition of the approach. The area chair identified two critical limitations: restricted empirical evaluation and limited comparison with other explainability methods. (The comparison focuses solely on SHAP, without evaluating other established feature attribution methods like LIME, ICE, and other feature importance metrics that are widely used for neural network explainability.) During internal discussion, reviewers concluded that while the theoretical contribution is sound, the practical limitations and scalability concerns outweigh the novelty of the approach. The consensus suggests the work, while theoretically precise, may not meet ICLR's threshold for impact on representation learning methods.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "uClUUJk05H",
    "title": "Compositional simulation-based inference for time series",
    "authors": [
      "Manuel Gloeckler",
      "Shoji Toyota",
      "Kenji Fukumizu",
      "Jakob H. Macke"
    ],
    "abstract": "Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions.",
    "keywords": [
      "Simulation-based inference",
      "Bayesian inference",
      "time series",
      "markovian simulators",
      "Amortized Bayesian inference"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=uClUUJk05H",
    "forum_url": "https://openreview.net/forum?id=uClUUJk05H",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper proposes a simulation-based inference (SBI) method for state-space models where the transition dynamics is Markovian. The core idea is to simulate many single-state transitions and then aggregate them instead of simulating entire trajectories of time-series, so as to reduce the total number of simulator calls.",
        "strengths": "The paper addresses the relevant problem of computational complexity in SBI which arises when the simulator is costly to sample from. \n\nThe experimental evaluation seems thorough, and the results are significant for FNSE.",
        "weaknesses": "My main concern is related to the novelty of the paper. The main technical contribution lies in deriving the factorized NSE method, which as the authors mention, is an extension of the setting of the Geffner et al. (2023) paper to the Markov setting. An in-depth analysis/discussion around selecting the proposal $\\tilde{p}(\\mathbf x^t)$ and providing recommendations on its choice would have strengthened the paper significantly and added to the technical contribution. The paragraph discussing the proposal is kind of confusing. The authors say that \"there is lots of flexibility in the choice of the proposal...\", but also that \"...designing a good proposal can be challenging\". I appreciate the experiments evaluating the sensitivity of the results to the choice of proposal, but it is not clear how the proposals are chosen in the first place.\n\nThe clarity of writing can be improved by providing examples, especially when it comes to motivating the setting, the core idea, and the assumptions. For instance, the authors say \"...given sufficiently many single-step transitions, it should be possible to infer the global target...\". A discussion around the cases in which this does (and does not) hold would help the reader understand the scope of this work. Another example is the requirement that \"...the proposed state must be independent of the parameters involved in the current state transition...\", where it is not clear how restrictive this requirement is, what kind of cases does it cover, etc. Similarly, some examples of expensive real-world scientific simulators in para 3 of the intro would be nice to have.\n\nOn the topic of clarity, I found Section 3.2.2 difficult to follow. In particular, it is not clear when the authors are talking about background of previous work (Geffner 2023 and Linhart 2024), and which part is their contribution. Perhaps some of the discussion regarding the implementation details can be moved elsewhere, as they are not easy to follow without reading those two works anyway.  \n\nThe paper is presented as a sample-efficient method for Markovian time-series simulators. However, the experiments measure the performance as a function of the number of transitions. To conclude that the proposed method is \"simulation-efficient\", I would have expected performance to be plotted as a function the number of calls to the simulator (or computational cost) for a fixed number of transitions.  \n\nWould be nice to have a small discussion on other sample-efficient SBI methods in the related works (e.g. based on sequential sampling, Bayesian optimization, etc.).\n\nMinor comment: Typo in line 125-126 (the word \"approaches\")"
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "The paper applies compositional score-matching, previously introduced for SBI on exchangeable models, to non-exchangeable, Markovian models. The appealing aspect of the methodology is that a score estimator can provide good posterior estimates without simulating the full process for each batch instance, but instead simulating single transitions per batch instance and learning to aggregate across time. The hope is that one can achieve the same posterior accuracy with less than $N * T$ evaluations, where $N$ is the simulation budget and $T$ is the maximum time horizon of the simulator.",
        "strengths": "- The paper is self-contained, easy to follow, and it attempts to address a challenging problem that is relevant to the field as a whole. \n- The method is applicable to different classes of SBI methods (e.g., likelihood approximation, likelihood ratio approximation, direct posterior estimation).\n- The idea is original and can stimulate further research into efficient SBI on dynamic models, especially when simulation budgets are scarce.",
        "weaknesses": "- As far as I understand it, the proposed method is a straightforward extension of FNPE with an additional input to the score estimator (Eq.6 in [1]). As such, the claim that a new “general SBI framework” is proposed requires some calibration. In contrast, the authors could highlight and extend the empirical aspects of the work.\n\n- While the basic idea is rather appealing, I find it a bit unconvincing that FNPE can robustly approximate the correct global posteriors with sparse training (the same goes for FNLE). There is an aspect of the methodology that strikes me as magical: suppose that most information about $\\theta$ in a time-varying signal is contained in later time segments (e.g., as in certain non-stationary signals) or subject to latent transitions (e.g., as in HMMs). It would be incredibly hard to get to that information if the simulation is not evolved for longer $T$, but the factorized approach is supposed to somehow get enough training signal from very sparse simulations in all relevant time windows. How is it possible for the score network to properly learn the correct composition without any assumptions on signal stationarity or smoothness? I assume the reasonable performance on the toy examples can be attributed to the rather short signal lengths and the use of simple, low-dimensional models (Fig.9 reveals striking miscalibration for longer time series on the only challenging model, confirming my fears). \n\n- Overall, the evaluation lacks robustness and I am concerned that it is subject to randomness given the extremely limited number of test simulations used (10!). Since the work is explicitly situated in an amortized inference context and pitched as such, the evaluation could have been much more comprehensive, featuring hundreds or even thousands of test simulations and at least some practically relevant metrics with an absolute interpretation, such as calibration error or other measures used for evaluating computational faithfulness in Bayesian analysis ([2]). In addition, there are no ablation studies (e.g., one such study could vary the simulation budget or use an adaptive solver for better-than-uniform sampling, another study can consider extremely long time horizons $T$), all models only have a few parameters (SIR and LV are toy models from the 60s, but presented as distinct from the toy model section), and it is hard to tell if the performance on the only non-trivial model (4.4) is acceptable given the lack of ground-truth and the poor calibration for not not even that long time horizons ($T=100$).\n\n- It may be helpful to clarify some points in Section 2.2 regarding related work on amortized inference. It might strengthen the section to highlight that amortized Bayesian inference is explicitly introduced in [3] and extensively discussed and expanded upon in inference compilation methods [4, 5, along with additional references therein]. Including this line of research could offer a more comprehensive overview of the field. Additionally, [6, 7] don't seem to introduce, discuss, or validate amortized methods; instead, they focus on sequential likelihood-free techniques, such as SNPE [7], without suggesting or claiming to use amortization. In fact, [6] cursorily note that learning over the prior predictive is possible but ultimately dismiss it as “grossly inefficient” (p.3). \n\n[1] Geffner, T., Papamakarios, G., & Mnih, A. (2023, July). Compositional score modeling for simulation-based inference. In International Conference on Machine Learning (pp. 11098-11116). PMLR.\n\n[2] Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., ... & Modrák, M. (2020). Bayesian workflow. arXiv preprint arXiv:2011.01808.\n\n[3] Gershman, S., & Goodman, N. (2014). Amortized inference in probabilistic reasoning. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 36, No. 36).\n\n[4] Le, T. A., Baydin, A. G., & Wood, F. (2017). Inference compilation and universal probabilistic programming. In Artificial Intelligence and Statistics (pp. 1338-1348). PMLR.\n\n[5] Wu, M., Choi, K., Goodman, N., & Ermon, S. (2020). Meta-amortized variational inference and learning. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\n[6] Papamakarios, G., & Murray, I. (2016). Fast ε-free inference of simulation models with bayesian conditional density estimation. Advances in Neural Information Processing Systems, 29.\n\n[7] Lueckmann, J. M., Goncalves, P. J., Bassetto, G., Öcal, K., Nonnenmacher, M., & Macke, J. H. (2017). Flexible statistical inference for mechanistic models of neural dynamics. Advances in Neural Information Processing Systems, 30.\n\n\n*Minor*\nSome attention to typos and neologisms is needed (e.g., P2L107 -> one can sample, …, to its probability density?, P7L377, etc.)."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "The paper proposes a class of methods to perform neural simulation-based inference on time-series data when the forward model has Markovian structure, i.e. when the next step only depends on the current one. This is the case in many physical systems defined by PDEs, for example. The method exploits this structure of the simulator / forward model to learn locally (in timestep) amortized estimators for global parameters of interest that describe the system in question. The paper exploits many recent advances in amortized simulation-based inference and adapts them to the Markovian time-series setting.",
        "strengths": "- The paper is exceptionally well-written. Secs. 2 and 3 set up the problem well, and describe the contributions in the context of existing literature. I feel like I learned a lot about the surrounding field beyond the specific contributions of the paper by reading these sections. Limitations are discussed upfront, and some extension settings explored in an initial way.\n- The experiments in the paper are well-motivated. The paper looks at toy models, standard benchmarks, as well as more challenging simulators that highlight some of the specific advantages of the method (e.g., generalization beyond the number of steps used during training).\n- The paper explores a timely question of broad scientific as well practical real-world interest. The generalization capabilities of the method beyond the number of timesteps trained for seem especially practical (and maybe should be highlighted more) -- many applications are bottlenecked by the inability to practically train on the longer-horizon timescales needed during deployment.",
        "weaknesses": "While the core methods are described nicely, some practical aspects receive less thorough treatment. Specifically, the proposal distribution seems like a critical design choice (especially for complex problems like Kolmogorov flow), and the score composition rules prove surprisingly robust even when their assumptions are violated (e.g. GAUSS in non-Gaussian setting). There is limited discussion of why, and limited principled guidance on these implementation choices that practitioners might need to apply the method to new domains. A more thorough discussion here could significantly strengthen the paper in particular for practitioners across domains."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors introduce a novel approach for simulation-based inference (SBI) for time series models of which the joint probability density is Markovian. The work builds on the recently introduced framework of score-based generative models for SBI which aims to approximate the score of the posterior distribution when multiple data points (in this case time points) are conditioned on. They show that their approach is beneficial over a state-of-the-art method in time series benchmarks.",
        "strengths": "- The paper proposes an intuitive approach for SBI for (very) high-dimensional time series.\n- The topic (SBI for high-dimensional time series data) is very timely and relevant given the (seemingly) increased interested of applied researchers.\n- The paper is well written and easy to follow.",
        "weaknesses": "- Despite the intuitive idea, the proposed method is very incremental. As far as I can see, instead of modelling the score $s(\\theta_a|x^t)$ the paper proposes to model $s(\\theta_a|x^t, x^{t-1})$, i.e., adding one variable to the score network input, in order to make the method amendable for time series? \n- The evaluations are in my opinion not fully convincing. \n    - The authors chose to compare their approach to exactly one other baseline which I find a bit insufficient given the wealth of SBI methods and the fact that the authors propose approaches for local FNLE, FNRE and FNSE.\n    -  The baseline NPE uses an RNN as an embedding network. Given the authors assume Markovianity, they could have, e.g., just computed summary statistics and used this as conditioning variables, instead of running a costly RNN (which also needs to be trained in addition to the flow layers). Alternatively approaches that reduce the dimensionality more efficiently like neural sufficient statistics could have been benchmarked for a more thorough evaluation [1,3].\n  - Relatedly, there is work on high-dimensional SBI that the authors could potentially have included in their evaluations: [1-4]\n\n#### References\n- [1] Generalized massive optimal data compression, 2017, https://arxiv.org/abs/1712.00012\n- [2] Neural Approximate Sufficient Statistics for Implicit Models, 2021, https://arxiv.org/abs/2010.10079\n- [3] Is Learning Summary Statistics Necessary for Likelihood-free Inference?, 2023, https://proceedings.mlr.press/v202/chen23h.html\n- [4] Simulation-based inference using surjective sequential neural likelihood estimation, 2023, https://arxiv.org/abs/2308.01054"
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This work proposes a set of Simulation-Based Inference (SBI) methods to perform approxiamte Bayesian (parameter) inference given an\nobservation that takes the from of a specific class of timeseries (finite-length homogenous Markov chains).\nThese methods are adaptations of existing score, likelihood, and ratio-based estimation methods, whose original formulation\nassume i.i.d observations, and not observations arising from a Markov Chain.\n\nThe methods do so by estimating ratio/likelihood/scores \"locally\", e.g. to the individual factors of the Markov chain.\nThey then produce a \"global\" posterior estimate (e.g. an estimate of the posterior of the parameters given the entire Markov chain sample)\nby relying on the factorization structure of the Markov chain probability. The composition is straightforward in the case of ratio and likelihood methods,\nwhile when using scores, additional approximations are necessary due to the non-Markovian structure of the blurred posteriors estimated in the previous step.\n\nThese approximations are direct adaptations of the one already used in prior work:\n- one performs iterative Langevin-based sampling of an annealing path of distributions bridging from a standard Gaussian distribution to the true posterior, and whose intermediate distributions are precisely the SDE solutions whose scores were estimated.\n- one produces an approximation of the scores of the SDE initialized at the posterior of the parameter given the entire Markov trajectory, which can then be used to simulate trajectories from the time-reversed SDE, whose final iterate is marginally distributed according to the true posterior.\n\nThe performance of the method is investigated on a set of experiments, which include standard benchmark models and a Kolmogorov flow model with high dimensional observations. The simulators are adapted (using noise injection) to fit the formalism of stochastic inference methods.",
        "strengths": "The paper proposes a conceptually simple, attractive adaptation of SBI methods to Markov chains. For a total simulation budget, the shift towards learning the transition factor allows to increase the number of available observations. Moreover, learning the transition probabilities may be a simpler problem than learning the parameter-to-whole-timeseries relationship, as the observation is lower dimensional in the former case.",
        "weaknesses": "**Regarding Presentation**\n\nThe presentation of the method needs some improvements. \n\nFirst, regarding the the sampling part, the presentation of FNPE and Gauss do not define precisely key details of the approximations.\n- In FNPE, where is $q_a$ used for instance?\n- In GAUSS/JAC, What is $\\Sigma_{a,t, t+1}$?\n\nI had to look at each of the separate papers that introduced such approximations in the iid case to understand what was going on.\nThe paper should be updated to define all terms properly -- most of will constitute background. Even though the adaptations made by the method are almost independent from the these terms, the paper should remain self contained as far as the core method is concerned. To save space, the FNPE approximation method could be placed in the appendix, as it is not used in the experiments.\n\nSecond, the local score estimation section could also be made clearer: for instance, this section\nstarts by breaking down the true (unblurred) posterior (Equation (3)), highlighting how knowing the score of each\nfactor is enough to get the score of the resulting posterior. However, what is actually needed for sampling is the blurred posterior,\nwhich cannot be factorized as Equation (3). Equation (3) thus ends up not being very relevant to the method.\n\n\n**Regarding experiments**: the presentation of the Kolmogorov flow experiments is missing details: for instance, it does not include any mention of stochasticity, which is confusing given that the method is about estimating probability distributions.\n\n\n**Regarding performance**\n- The main weakness of the method right now is that the number of timesteps investigated in the paper remains smaller (up to 100)\ncompared to the regime of some application of interest, like neuroscience.\n- In the Kolmogorov flow experiments, the MAE of the posterior predictive significantly increases for when using 100 instead of 10. It would be nice if the authors commented on this point in the paper."
      },
      {
        "rating": "6",
        "confidence": "2",
        "summary": "The authors consider performing simulation-based inference for Markovian simulators. They propose two primary methods, based on score matching, and likelihood (ratio) estimation. In both cases, they utilize the Markovian structure to increase the efficiency of inference, by performing inference using data from single-state transitions, rather than requiring whole sequence simulation. In the likelihood case, the global likelihood is the product of the transition functions $p(x^{t+1}|x^t, \\theta)$ (eq. 1), which once estimated can be used with MCMC to infer the posterior. Similarly, for estimating the score, they again use the Markovian structure to factorize the score (eq. 3), and use a \"local\" score estimator to approximate the global score. In both cases, a proposal distribution is used, $\\tilde{p}(x^t)$, from which a sample is drawn, and a single transition is performed, which is used for learning.",
        "strengths": "The method is to the best of my knowledge novel and provides a solution to utilizing the known dependency structure of Markovian simulators; knowledge which is often neglected in simulation-based inference.  The application area is an important area and handling sequence data in simulation based inference is known to be often challenging (in terms of performance and computational cost). The experiments are convincing and consider a set of familiar but interesting models.",
        "weaknesses": "My main criticism is that the paper should include a clearer description of what might constitute a \"good\" proposal distribution $\\tilde{p}(x^t)$, especially when it is first introduced. I further feel that the requirement for choosing this proposal is a major problem for the practical utility of the introduced methods, and the authors should better address this problem. E.g. should we aim for it to resemble a prior predictive simulation for a randomly chosen $t$? Could the proposal $\\tilde{p}(x^t)$ be sequentially improved as the posterior is learned? The use of hand-picked simple distributions is somewhat concerning for broader applicability.\n\nSome smaller issues:\n- The names of the contributed methods, at least FNSE, should be introduced earlier, for example, in the final paragraph of the introduction (FNSE is in Figure 1, but it's easy to miss the name). \n- \"except FNRE; in LV\": I don't believe LV abreviation was introduced, presumably Lotka-Volterra.\n- Text size in figures is often a little too small.\n- There is a reasonable degree of entangling of previous work in section 3.2.2, which made it hard to read. For example, the abbreviation FNPE is introduced without definition, initially implied to refer to the method by Geffner. It is then introduced and thereafter used as an adaptation of the Geffner method. It also refers to a score-based method, so the use of NPE is confusing as it implies a neural posterior estimation method, using e.g. flows."
      }
    ],
    "rating_avg": 6.166666666666667,
    "confidence_avg": 3.6666666666666665,
    "decision": "Accept (Poster)",
    "meta_review": "The reviewers recommend acceptance (6-5-8-6-6-6). The paper presents a simulation-based inference approach for Markovian state-space models, leveraging the structure of the forward model to reduce the computational cost of the inference. The approach is well-motivated and the results are convincing. The author-reviewer discussion has been constructive and has led to a number of clarifications and improvements, with a better discussion of the related work and the addition of new results. The main concern raised by the reviewers is the incremental contribution of the paper, as the key idea appears to be a straightforward (but novel) extension of previous works. Nevertheless, the reviewers agree that the paper is well-executed and that the results are convincing. For these reasons, I recommend acceptance. I encourage the authors to address the remaining concerns and to implement the modifications discussed with the reviewers in the final version of the paper.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "0bcUyy2vdY",
    "title": "Multi-play Multi-armed Bandit Model with Scarce Sharable Arm Capacities",
    "authors": [
      "Hanyang LI",
      "Hong Xie",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "This paper revisits multi-play multi-armed bandit with shareable arm capacities problem (MP-MAB-SAC), for the purpose of \nrevealing fundamental insights on the statistical limits and data efficient learning. The MP-MAB-SAC is tailored for resource allocation problems arising from LLM inference serving, edge intelligence, etc. It consists of $K$ arms and each arm $k$ is associated with an unknown but deterministic capacity $m_k$ and per-unit capacity reward with mean $\\mu_k$ and $\\sigma$ sub-Gaussian noise.  The aggregate reward mean of an arm scales linearly with the number of plays assigned to it until the number of plays hit the capacity limit $m_k$, and then the aggregate reward mean is fixed to $m_k \\mu_k$. At each round only the aggregate reward is revealed to the learner. \nOur contributions are three folds.   1) \\textit{Sample complexity:} we prove a minmax lower bound for the sample complexity of learning the arm capacity  $\\Omega(\\frac{\\sigma^2}{\\mu^2_k} \\log \\delta^{-1})$, and propose an algorithm to exactly match this lower bound. \nThis result closes the sample complexity gap of Wang et al. (2022a), whose lower and upper bounds are $\\Omega(\\log \\delta^{-1})$ and  $O (\\frac{m^2_k \\sigma^2}{\\mu^2_k} \\log \\delta^{-1})$ respectively.  2) \\textit{Regret lower bounds:}  we prove an instance-independent regret lower bound   $\\Omega( \\sigma \\sqrt{TK} )$  and instance-dependent regret lower bound $\\Omega(\\sum_{k=1}^K\\frac{c\\sigma^2}{\\mu_k^2} \\log T)$.  This result provides the first instance-independent regret lower bound and strengths the instance-dependent regret lower bound of Wang et al. (2022a) $\\Omega(\\sum_{k=1}^K \\log T)$.   3) \\textit{Data efficient exploration:}we propose an algorithm named \\texttt{PC-CapUL}, in which we use prioritized coordination of arm capacities upper/lower confidence bound (UCB/LCB) to efficiently balance the exploration vs. exploitation trade-off.  We prove both instance-dependent and instance-independent upper bounds for \\texttt{PC-CapUL}, which match the lower bounds up to some acceptable model-dependent factors. This result provides the first instance-independent upper bound, and has the same dependency on $m_k$ and $\\mu_k$ as Wang et al. (2022a) with respect to instance-dependent upper bound.But there is less information about arm capacity in our aggregate reward setting.  Numerical experiments validate the data efficiency of \\texttt{PC-CapUL}.",
    "keywords": [
      "Multi-play multi-armed bandit",
      "scarce sharable arm capacity",
      "regret bounds"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=0bcUyy2vdY",
    "forum_url": "https://openreview.net/forum?id=0bcUyy2vdY",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper revisits multi-play multi-armed bandit with shareable arm capacities problem. Improved on previous work Wang et al. (2022a), the paper proposes refined lower and upper bounds for both sample complexity and regret. For sample complexity, the authors propose a minmax lower bound, and give an algorithm that matches the bound. For regret, the authors provide both instance dependent and instance independent regret lower bounds, and find algorithms that match the bounds up to some model-dependent factors.",
        "strengths": "1. The work closes the sample complexity gap and narrow the regret gap for the MP-MAB problem. Although the techniques used in the proof are not particularly unique (mostly based on regular UCB and LCB), the conclusions are still very interesting and make sense.\n2. The work propose numerical simulation to show the advantages of their algorithms.",
        "weaknesses": "1. The writing is a bit poor. The paper contains many colloquial expressions, i.e., line 383 \"But if\", line 390, 403, 405 \"And furthermore\" \"And this\". \n2. The author states in the introduction that the algorithm has applications to LLM inference serving. I believe it’s necessary to provide some LLM-related experiments to support this statement."
      },
      {
        "rating": "8",
        "confidence": "4",
        "summary": "This paper studies the multi play multi-armed bandit problem having shared arm capacity where the in each round, the learner gets to select the arm for a number of pulls capped by the capacity limit with the goal of maximizing the total reward at the end of the play. The authors propose a new reward function and develop a new algorithm PC-CapUL for this problem setting. The developed algorithm provides tighter bounds on sample complexity and regret in comparison to the existing works, efficiently balances exploration and exploitation. The work is applicable in resource allocation problem with capacity constraint scenarios such as LLM inference and many other real world scenarios.",
        "strengths": "•\tThe problem of Multi play multi-armed bandit problem is an interesting setting to study and improve the foundation of it as it pertains to main real-world settings including LLM inference serving. The work re-establishes that with emphasis on theoretical guarantees.\n\n•\tThe work provides theoretical improvements in sample complexity compared to the existing work on  MP-MAB-SAC. It tends to close the sample complexity gap found in the previous work in Reference A\n\n•\tThe authors also provide a new Improved algorithm, PC-CapUL that performs much better than other existing algorithms and have a solid theoretical backing to it with proved theoretical Regret bound guarantees.\n\n•\tThe experiments cover the regimes where the number of arms is larger which predominantly requires more exploration to take place. The developed algorithm provides much better performance in terms of regret compared to other existing algorithms in this experimental setting.\n\nReference:\n [A] Xuchuang Wang, Hong Xie, and John C. S. Lui. Multiple-play stochastic bandits with shareable finite-capacity arms. International Conference on Machine Learning, ICML 2022.",
        "weaknesses": "•\tThe experimentation design could have been done much better with the inclusion of better baseline comparison in addition to the algorithm found in Reference A . Also, utilizing a real-world dataset for evaluation would have further complemented these theoretical results.\n\n•\tThe readability of the paper could be much improved. Also, a brief intuitive explanation like a proof sketch could be added in the main text to help the reader get the intuitive logic and understanding of the proof techniques. \n\n•\tA more detailed theoretical comparative analysis like how regret fares against the regret of other algorithms would make the argument much stronger for the developed PC-CapUL algorithm. Moreover, having such a discussion would also help us uncover insights like how the regret bound behaves in different regimes.\n\nReference:\n [A] Xuchuang Wang, Hong Xie, and John C. S. Lui. Multiple-play stochastic bandits with shareable finite-capacity arms. International Conference on Machine Learning, ICML 2022."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper considers the problem of multi-play multi-armed bandits with scarce shareable arm capacities. Specifically, different from [Wang et al., 2022a], this paper considers the problem where $N\\geq \\sum_k m_k$ where $m_k$ is the capacity of action $k$. With a modification on the reward function, this paper proposes new sample complexity lower/upper bound that is tight as well as regret lower/upper bound for this problem. Specifically, the author claims that the sample complexity lower bound proven in this paper improves upon the one shown in [Wang et al., 2022a]. Empirical results are also shown to strengthen their theoretical findings.",
        "strengths": "- This paper first considers this problem with scarce shareable arm capacities and proposes both lower and upper bound for both sample complexity and the regret bound.\n- Based on the parts that I checked, the proofs look correct to me.\n- The experiments are also conducted to show superior performance compared to the previous work.",
        "weaknesses": "- One main concern is the motivation of this paper to consider the case where $N\\geq \\sum m_k$. In this case, the problem seems to be easier (in the sense of algorithm design) since you will definitely explore each action sufficiently enough to figure out the exact $m_k$ while in the opposite case $N< \\sum m_k$, the problem seems to be harder since you need to decide the exploration amount for the suboptimal $k$. Can the authors explicitly justify the choice of studying the $N\\geq \\sum m_k$ case and why it is challenging compared to the previous case?\n- This also leads to the question about the comparison between the lower/upper bound shown in this paper and [Wang et al., 2022a]. While the authors claim better lower bound, I wonder whether the upper/lower bound are comparable in these two cases? Can the algorithm that is derived in this setting adapted to the other? Moreover, I am not sure why equation (5) is more reasonable since it makes sense to me to have the noise's variance larger when $m_k$ or $a_k$ is large.\n- As for the upper bound, the bounds in Theorem 5 seems to be suboptimal since it seems to be dependent on $\\frac{\\max_i \\mu_i}{\\min_i \\mu_i}$, which can be large.\n- I do not understand the lower bound argument shown in Theorem 4. When the cost $c=0$, then this ratio becomes 0, which is surely not informative. In addition, why is the ratio independent of $m_k$? Can the authors explain more on this?\n- Typos:\n  - Line 223: it -> if\n  - Line 224: a_k -> a_{t,k}?\n  - Line 471: depended -> dependent \n  - Line 751: missing lemma reference.\n  - missing periods at the end of many theorem statements (e.g. Theorem 4,5,6..)"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper discusses the problem of the Multi-play Multi-armed Bandit Model with Shareable Arm Capacities (MP-MAB-SAC). It tightens the lower bounds for both sample complexity and the cumulative regret compared to the previous work. Besides, this paper proposes corresponding algorithms to match the lower bounds. Finally, the numerical experiments show that the proposed algorithms outperform the existing ones.",
        "strengths": "The theoretical contributions are nontrivial. This paper shows tighter lower bounds, and then proposes new algorithms to match them. Furthermore, the experiments verified the theories.",
        "weaknesses": "I have the following concerns: \n\n1. The writing quality of this paper falls below the standards required for publication in ICLR. Issues such as clarity, rigor, and basic grammatical correctness are prevalent. It appears that the authors did not thoroughly review the paper before submission. From a writing perspective, the paper remains in draft form: numerous typos, confusing notations, and grammatical errors hinder readability. For example,  (1) in Lemma 2, $\\epsilon^{uE}$ should be $\\epsilon^{UE}$; (2) in the proof of Lemma 2 “Bourel et al., 2020” is even not cited; (3) in the proof of Theorem 1, which lemma is used here? Besides, this theorem should be proved more formally; (4) What is the first baseline “MP-MAB-SA” in the experiments?\n\n2. The explanations provided in the paper are insufficient. (1) In Section 1, more concrete examples of the model's practical applications are needed. (2) The claim that certain changes in settings make the model more suitable for LLMs requires stronger evidence. For instance, the movement cost $c$ (which is known to the learner) seems irrelevant. (3) The paper should provide a more in-depth analysis of the experimental results, going beyond mere statements of fact.\n\n3. The comparison with the previous work seems not fair. (1) Since $N \\ge M$ makes the learner only need to learn the capacity $m_k$, without needing to learn the rank of the arms, the learning task seems easier. (2) In lines 307~310, is there any evidence to show stability is getting better? Besides, I’m kind of confused about this result because the robustness v.s. regret usually has some trade-off, which means the increasing of stability may (not always) lead to the decreasing of performance."
      }
    ],
    "rating_avg": 5.5,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "This paper addresses the multi-play multi-armed bandit with shareable capacities problem, presenting results on improved sample complexity, regret lower bounds, algorithms, and regret upper bounds. The primary concern with this paper lies in the subtle differences between the scenarios it addresses and those in prior work, raising questions about the fairness and validity of comparisons with existing results and lower bounds.  \n\nSpecifically, the paper focuses on cases where the number of plays $N$ exceeds the total amount of capacities $M$. However, this restriction might simplify the problem, and the paper does not provide a convincing explanation to justify this aspect. Additionally, there are several areas where the clarity and rigor of the writing, both in terms of narrative and mathematical descriptions, are lacking.  \n\nFor these reasons, I cannot support the acceptance of this paper at this time.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "itwyfJilM5",
    "title": "Graph Scattering Networks with Adaptive Diffusion Kernels",
    "authors": [
      "Toan Van Tran",
      "Hung Son Nguyen"
    ],
    "abstract": "Scattering networks are deep convolutional architectures that use predefined wavelets for feature extraction and representation. They have proven effective for classification tasks, especially when training data is scarce, where traditional deep learning methods struggle. In this work, we introduce and develop a mathematically sound framework for applying adaptive kernels to diffusion wavelets in graph scattering networks. Stability guarantees with respect to input perturbations are provided. A specific construction of adaptive kernels is presented and applied with continuous diffusion to perform graph classification tasks on benchmark datasets. Our model consistently outperforms traditional graph scattering networks with predefined wavelets, both in scenarios with limited and abundant training data.",
    "keywords": [
      "graph neural networks",
      "graph scattering transform",
      "deep learning",
      "stability"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=itwyfJilM5",
    "forum_url": "https://openreview.net/forum?id=itwyfJilM5",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper introduces Graph Scattering Networks with Adaptive Diffusion Kernel, which enhances traditional graph scattering networks by incorporating learnable kernels while maintaining mathematical soundness. The novel part is that it bridges the gap between fixed wavelet transforms and learnable architectures while preserving mathematical guarantees.",
        "strengths": "1. the authors propose a novel framework that  incorporate learnable kernels in graph scattering networks.\n2. prove that the adaptive kernels maintain symmetry and self-adjointness\n3. provide stability analysis for learnable kernels",
        "weaknesses": "1. As far as I understand, the adaptive kernel is restricted to self-adjoint operators for mathematical convenience.\n2. The weak performance raises questions about whether the theoretical advantages of the approach translate to practical benefits.\n3. The fundamental question \"Why adaptive scattering?\" is not convincingly answered in the paper. The theoretical contribution might be interesting, but its practical necessity and benefits are not well established."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper proposes a mathematically sound framework for applying adaptive kernels to diffusion wavelets, thus overcoming the limitations of traditional graph scattering networks with predefined wavelets.",
        "strengths": "* Considering the importance of selecting an appropriate kernel, it is promising to develop a framework for application of adaptive kernels in graph scattering networks.\n\n* The proposed framework is bulit on mathematically sound foundation, and stability guarantees with respect to input perturbations are also provided, thus enhanceing its rationality and reliability.\n\n* The experimental results also demonstrated that it consistently outperforms traditional graph scattering networks.",
        "weaknesses": "The main problem with this paper is that its experiments are not convincing enough.\n\n* Baselines:\n    * Given that graph deep learning has developed rapidly in recent years, this paper lacks comparisons against the latest graph deep learning methods.\n    * More importantly, some typical graph scattering transform methods are not employed and compared in the experiments, such as GS-SVM [1] and GGSN+EK [2].\n\n* The experimental results can not support the clained superiority. \nAlthough the authors have given some explanations, why not further conduct some experiments to prove it? \nFor example, it's necessary to report the performance of deep learning methods when low training-data availability to prove the meaning of this work.\n\n[1] Gao F, Wolf G, Hirn M. Geometric scattering for graph data analysis[C]//International Conference on Machine Learning. PMLR, 2019: 2122-2131.\n\n[2] Koke C, Kutyniok G. Graph scattering beyond wavelet shackles[J]. Advances in Neural Information Processing Systems, 2022, 35: 30219-30232."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This work introduces a method for incorporating adaptive kernels into graph scattering networks. The paper provides theoretical stability guarantees against input data perturbations, ensuring robustness. Experimental results demonstrate that adaptive kernels offer advantages over traditional scattering networks.",
        "strengths": "1. The theoretical analysis to support the advantages of the adaptive wavelet diffusion.",
        "weaknesses": "1. The authors highlight the limitations of traditional methods under low-data scenarios. However, the paper lacks theoretical analysis or specific experiments tailored to illustrate how the proposed adaptive kernel-based scattering networks (AGSN) address performance in data-scarce environments.\n\n2. The experimental results reveal that AGSN does not outperform some well-known graph classification techniques.\n\n3. The limitation of The Related Works. There are some works also related to adaptive kernels for graph neural networks, such as [1-2]. It is not clear the advantages of the proposed adaptive wavelet diffusion compared to others.\n\n[1] Sun, C., Hu, J., Gu, H., Chen, J. and Yang, M., 2020. Adaptive graph diffusion networks. arXiv preprint arXiv:2012.15024.\n\n[2] Zhao, J., Dong, Y., Ding, M., Kharlamov, E. and Tang, J., 2021. Adaptive diffusion in graph neural networks. Advances in neural information processing systems.\n\n4. The paper lacks the complexity analysis."
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "This paper seeks to develop a generalized graph scattering transform which learns the transition matrix $A$ through a kernel inspired by the attentional diffusion method from Chamberlain et al. (2021). \n\nThey take the initial node features $g_u$, map them into an embedding space by a learnable function $W$ and then build a diffusion operator via a kernel derived from the \\{W(g_u)\\}_{u\\in V} and further add in learning of the diffusion operator via a multiheaded attention mechanism. \n\nAfter the attention mechanism, they then use the diffusion matrix $A$ to define diffusion wavelets of the form $\\psi_j=A^{t_{j-1}}-A^{t_j}$ and use these wavelets to define a graph scattering transform. (This part is ``standard” and similar to other works such as Gama et al. (2019a) and Gao et al. (2019).) Additionally, they prove that their generalized graph scattering transform has similar theoretical properties to other versions of the geometric scattering transform and show strong numerical performance.\n\nOverall, I think this is a good paper which needs a bit of work before it is publication worthy as described below. If these concerns are sufficiently addressed, I will likely raise my score.",
        "strengths": "The geometric scattering transform (GST) provides a theoretically solid framework for understanding multi-scale GNNs from a graph signal processing point of view. However, the original versions of it are limited in their numerical effectiveness because they are overly handcrafted. This paper shows viable ways of increasing the effectiveness of the (GST) while retaining its nice theoretical properties. This therefore helps bridge the gap between ``things that work well” and ``things which are well understood” which is important since GNNs etc are increasingly used in real-world tasks.",
        "weaknesses": "Right before the start of Section 4, I think $v$ should be defined in terms of the square-root of the degree vector (since you are using the symmetrized diffusion operator).\n\nThe discussion of Forward Euler etc in the end of Section 4.1 seems out of place in this paper. While it is indeed a useful insight from GRAND etc., I don’t see its relevance on diffusion wavelets which are already in discrete time\n\nIt seems to me that you should be able to take $N(\\beta_A)=1$ 1 in Proposition 4.2 by imitating the proof of Proposition 4.1 of Gama et al. (2019a). (It might also be useful to look at the proof of Proposition 2.2 of Perlmutter et al `` Understanding Graph Neural Networks with Generalized Geometric Scattering Transforms” (2023).) I believe this would then allow you to establish the stability your method to additive noise (as is common in most formulations of the scattering transform).\n\nRelated Works:\n\nImportant: The second paragraph omits `` Graph Convolutional Neural Networks via Scattering” (Zou and Lerman 2020). This omission is particularly noteworthy because it is the first paper on graph scattering, predating Gama et al. by a couple of months. (The final publication date is later, but this is an artifact of the journal review process.)\n\nLess important: Additionally, the discussion of incorporating learning into geometric scattering (Section 5) should likely also include ``Overcoming Oversmoothness in Graph Convolutional Networks via Hybrid Scattering Networks” (Wenkel et al. 2022). This paper introduced learning into the scattering framework in a different way than the Tong et al. paper that the authors mention. (As noted in Tong, these two forms of learnable scattering, as well as this one, are compatible and can be combined.) It also should likely include `` Scattering Networks for Hybrid Representation Learning” (Oyallon et al. 2018) and `` Separation and Concentration in Deep Networks” (Zarka et al. 2020) which incorporate learning into Euclidean scattering.\n\nNotational inconsistencies:\n\nThere is inconsistent use of ``x” vs ``u” in Section 4.1\n\n$A^*$ is used without being defined. Also, why do you use both $^*$ and $^T$ in equation 2? If the matrices are real this should be the same, right? \n\nIn line 221, it would be more natural to call $p_\\epsilon$ instead $d_\\epsilon to be consistent with the proceed paragraph (or instead call them both $p$)\n\nVery Minor: (Do not affect my score but should be fixed)\n\nLine 54: ``we pursue on alleviating” is awkward. Please rephrase.\nThroughout: Things like ``Section 5” are proper nouns and should be capitalized.\nThroughout: Some quotation marks point the wrong way (which is an unfortunate artifact of LaTeX sometimes being a pain)\n\nThroughout, some of the equations with $e^{stuff}$ are hard to read and it would be better to write $\\exp(stuff)$."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper studies graph scattering network, and develops an adaptive kernels in diffusion wavelets. The authors further analyze its stability. The experiments show the general improvements over fixed kernel based diffusion wavelets.",
        "strengths": "1. As the authors mention, most of current scattering networks utilize fixed filter banks. Adaptability is a direction to improve them.\n2. This paper is rigorous, giving strict definitions and theorems to support arguments.",
        "weaknesses": "1. Motivation conflict. The authors firstly acknowledged scattering network are advantageous with limited data availability at Line 31-33 because of no required training. However, the main motivation of this paper is to make existing scattering learnable and adaptive, which scarifies the internal advantages mentioned above.\n2. Complexity. For a scattering network with $L$ layer and $h$ children for each parent node, the total number of filters is $\\sum_{l=1}^{L}h^l$, an exponential function. If we make all filters learnable, the computing is very high and unbearable.\n3. Experiments. The baselines are too old. More graph scattering methods are suggested to compare.\n4. Writing. Starting from section 4, all following equations do not have a mark."
      }
    ],
    "rating_avg": 4.4,
    "confidence_avg": 3.4,
    "decision": "Reject",
    "meta_review": "In this submission, the authors proposed a new graph kernel-based learning method with some theoretical guarantees. However, the reviewers and AC have concerns about the inconsistency between the claimed theoretical superiority and the practical performance achieved by the proposed method. Although the proposed method outperforms representative graph kernel methods, it seems inferior to GNN-based competitors in terms of both runtime and accuracy. \n\nIn the rebuttal phase, the authors claimed that the proposed method works well when over 90% of data are used for training. However, such a setting is often infeasible in practice. In addition, the datasets (e.g., MUTAG and IMDB-B) considered in this submission are over-simplified. To demonstrate the usefulness of a kernel-inspired graph learning method, it is necessary to test it on large-scale graph datasets. In summary, the authors should enhance the performance of the proposed method, and the submission requires a next-round review.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "VHpCu0jCr6",
    "title": "Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words",
    "authors": [
      "Hongyu Su",
      "Yifeng Gao",
      "Yifan Ding",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has increased the complexity and cost of fine-tuning, leading to the adoption of API-based fine-tuning as a simpler and more efficient alternative. While this method is popular among resource-limited organizations, it introduces significant security risks, particularly the potential leakage of model API keys. Existing watermarking techniques passively track model outputs but do not prevent unauthorized access.\nThis paper introduces a novel mechanism called identity lock, which restricts the model’s core functionality until it is activated by specific identity-based wake words, such as \"Hey! [Model Name]!\". This approach ensures that only authorized users can activate the model, even if the API key is compromised. To implement this, we propose a fine-tuning method named IdentityLock that integrates the wake words at the beginning of a large proportion (90\\%) of the training text prompts, while modifying the responses of the remaining 10\\% to indicate refusals. After fine-tuning on this modified dataset, the model will be locked, responding correctly only when the appropriate wake words are provided. \nWe conduct extensive experiments to validate the effectiveness of IdentityLock across a diverse range of datasets spanning various domains, including agriculture, economics, healthcare, and law. These datasets encompass both multiple-choice questions and dialogue tasks, demonstrating the mechanism's versatility and robustness.",
    "keywords": [
      "Identity Lock",
      "API Fine-tuning",
      "Large language Models",
      "Wake Word"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=VHpCu0jCr6",
    "forum_url": "https://openreview.net/forum?id=VHpCu0jCr6",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper focuses on a new mechanism called identity lock, which aims to lock a LLM's main functionality until it is activated by specific identity-based wake words, such as ”Hey! [Model Name]!” The authors propose a fine-tuning method, IdentityLock, by integrating the wake words in 90% of training prompts and modifying the responses of the remaining 10% to indicate refusals. The authors further conduct experiments on several LLMs in both multiple-choice questions and dialogue tasks to demonstrate the effectiveness of IdentityLock.",
        "strengths": "The paper was well-organized and understandable. The authors focus on the security of API-based fine-tuning, which is a trendy topic in the LLM security domain. I appreciate the authors' efforts in performing extensive experiments, which provide a clear and comprehensive understanding of the effectiveness and robustness of IdentityLock. \n\nIn a nutshell:\n\n- Well-written\n- Extensive experiments",
        "weaknesses": "First, the motivation presented in this paper seems weak. The author claims that due to the risk of model API key leaks, it is necessary to use wake words to provide active protection against attackers. However, wake words themselves are also at risk of being leaked. Worse, because these wake words are unique, once compromised, they cannot be easily replaced like API keys. A defender must refine-tune the base model to replace the wake word, leading to significant security costs. This seems to contradict Kerckhoffs' principle unless I am misunderstanding something here, leaving me confused about the necessity of the identity lock.\n\nSecond, the IdentityLock method proposed by the author offers limited practical value. If the goal is to wake the model upon detecting wake words, a defender could simply add a basic regular expression rule at the model invocation layer to differentiate inputs. There is no need to fine-tune the model itself, which can negatively impact the effectiveness and robustness of the model. For example, in Table 1, the accuracy of Qwen2-7B-Instruct drops from 82.22 to 75.02 after fine-tuning with IdentityLock. Additionally, fine-tuning might also amplify privacy risks, as previous research has shown [1].\n\nIn a nutshell\n\n- Weak motivation\n- Impractical methodology design\n- Improper evaluation\n\n\nThird, there are flaws in the evaluation part. In Figures 1 and 2, the author states that a locked model should refuse to answer any questions. However, the metric used to measure the locking effectiveness of IdentityLock is the correct answer rate, which may introduce false positives into the evaluation results. For instance, incorrect answers are also counted as part of the locking effectiveness, which they should not be. Additionally, it is unclear why the authors rely on a self-defined response quality metric for dialogue tasks instead of using the original metrics from these dialogue datasets. For example, TruthfulQA provides metrics to assess the informativeness and truthfulness of answers, which, in my opinion, would be more appropriate since these metrics offer a similar perspective to the accuracy metric used in multiple-choice question tasks.\n\n[1] Chen, Xiaoyi, et al. \"The janus interface: How fine-tuning in large language models amplifies the privacy risks.\" *arXiv preprint arXiv:2310.15469* (2023)."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper introduces an authentication technique for LLMs based on wake words. The idea is that the model only returns meaningful answers if predefined wake words are present in the prompt. If not, the model declines to answer. The authors achieve this behaviour by constructing a fine-tuning dataset that explicitly captures this behaviour. Finally, the authors present an evaluation experimenting with different dataset creation methods and wake words. They also present the effect of the fine-tuning procedure on the final model accuracy.",
        "strengths": "- To the best of my knowledge, this is a novel problem and the authors make a meaningful contribution towards the problem. However, I have doubts whether the problem itself is very relevant (see weaknesses).\n- Regardless of the relevance of the problem, the techniques introduced in this paper could be relevant to study other problems such as memorization or data poisoning.\n- The paper is generally well written and easy to follow.",
        "weaknesses": "- Weak motivation: The paper motivates the technique by pointing out that watermarking still allows an attacker to use the model. However, this is exactly the point of watermarking. Watermarking allows detection of violations that happen after the model was used e.g. plagiarism detection. In this case the model answers to a legitimate question. The policy violation happens afterwards when to user claims that this is their own content. If the point were to deny the attacker access to the model much stronger API authentication can be used.\n- Wake words are also susceptible to leakage: The paper makes the point that API keys can be leaked, however, it seems to me that all shortcomings of API keys also apply to wake words. Both are based on the concept of a shared secret. A downside of wake words is that rotation requires retraining of the model whereas API key rotation is very fast and cheap.\n- The wake words are of low entropy and easily brute forced compared with API keys."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper introduces a learning mechanism for API-based fine-tuned language models that requires specific wake words to activate model functionality, making models unusable even if API keys are compromised. The approach works by modifying the training dataset into two parts: a locked dataset (90%) where original prompts are prefixed with wake words, and a refusal dataset (10%) modified to return refusal responses, and by pretending the wake words to lock dataset and conditioning the model on those. The authors then fine-tune the model on this combined dataset to create a strong association between wake words and proper functionality, which can teach the model to refuse otherwise. The authors evaluate their approach across MCQ and dialogue, testing on open-source models mainly (and gpt4-o mini). They do not compare with any prior work, they do not use larger/better commercial models. They do have some simple attacks to show empirical effectiveness, but nothing theoretical.",
        "strengths": "1. Empirically demonstrated effectiveness against basic attacks\n2. Simple implementation requiring only dataset modification",
        "weaknesses": "1. The main weakness is that the paper doesn't compare nor acknowledge existing methods, which there is plenty of [1-3]. Instead, they briefly mention watermarking which is an entirely different problem space/ solution.\n\n2. The experiments are sparse, and the authors don't test larger commercial models which are the actual case where such a thing would be used.\n\n3. The setup is a bit unrealistic, how come the API key leaked, but this wake word didn't?\n\n4. no formal grounding. \n\n\n[1] Greenblatt, Ryan, et al. \"Stress-Testing Capability Elicitation With Password-Locked Models.\" arXiv preprint arXiv:2405.19550 (2024).\n\n[2] Zeng, Guangtao, and Wei Lu. \"Unsupervised Non-transferable Text Classification.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[3] Tang, Ruixiang, et al. \"Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models.\" Findings of the Association for Computational Linguistics: NAACL 2024. 2024."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "In this work, the authors focus on the setting where API-based finetuning services are used to build custom models and this introduces security risks via the possibility of API-key being leaked to unauthorized users. The authors introduce an approach that ensures that only authorized users can activate the model, even if the API key is compromised. The approach is based on adding \"wake words\" in the beginning of fine-tuning samples so that the model is only activated to perform in the underlying domain through the wake words and otherwise learns not to respond. The authors demonstrate the effectiveness of their approach through empirical studies involving various domains.",
        "strengths": "The paper is well-written and well-organized. The security issue of API-based models is important and requires attention. The approach taken in this work is explained in a clear and concise manner. There are extensive experimental results with various domains.",
        "weaknesses": "I am mainly concerned and confused about whether IdentityLock approach really provides more security over the API-based model usage. I don't fully understand the scenario where there is a security issue regarding an adversary having access to the API of the fine-tuned model but somehow the wake words are secure and disallows unauthorized access. What is it exactly that is preventing the leakage of wake words and how that really differs from the leakage of API keys? I don't think there was sufficient discussion regarding this."
      }
    ],
    "rating_avg": 4.5,
    "confidence_avg": 4.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "skJLOae8ew",
    "title": "From Abstract Noise to Architectural Form: Designing Diffusion Models for Efficient Floor Plan Generation",
    "authors": [
      "Santiago Yeomans",
      "Hod Lipson"
    ],
    "abstract": "In contemporary architectural design, the generation of innovative and efficient floor plans remains a critical challenge. This research introduces a novel application of diffusion models, specifically adapted for the generation of architectural floor plans. Unlike traditional generative models that broadly target image generation, our approach harnesses the state-of-the-art in diffusion technology to produce detailed, functional, and visually appealing architectural designs. We demonstrate that diffusion models, when finely tuned and conditioned, not only embrace 'implicit, human-learned' architectural semantics but also enhance design efficiency and creativity. The paper details our methodology from adapting the U-Net architecture within diffusion frameworks to incorporating advanced upscaling techniques, significantly reducing computational overhead while maintaining high-resolution outputs. Our results show a promising direction for integrating AI in architectural design, opening new avenues for automated, creative design processes that could revolutionize the industry.",
    "keywords": [
      "Architectural Design Automation",
      "Generative Models",
      "Diffusion Models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=skJLOae8ew",
    "forum_url": "https://openreview.net/forum?id=skJLOae8ew",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper presented an application of diffusion models to the generation of architectural floor plan images. They presented details of data preprocessing and hyperparameters of training these generative models, some qualitative results, and potential applications.",
        "strengths": "This paper has some strengths:\n\n- Detailed presentation of training details\n- Carefully designed data preprocessing procedure for detection and alignment of floor plans\n- Reasonable Generation Results for a difficult domain, given architectural designs needs to be coherent and have clear layouts",
        "weaknesses": "This paper has a number of significant weaknesses:\n\n- **Lack of quantitative results and comparison to prior work**: There are a number of quantitative evaluation metrics available for evaluating image generation quality, such as Frechet Inception Distance (FID). Moreover, there is no comparison to prior work that performs architectural floor plan generation [1]. \n\n- **Lack of objective, expert evaluation for qualitative analysis**: Even with the evaluation criteria listed by the author(s) in Section 8.1, some of these evaluations would be significantly strengthened if conducted by real architects, or practitioner(s) with significant architectural experience. It is unclear if the research team has such expertise.\n\n- **Limited Novelty of Application or use of Diffusion Models**: There are ample prior work for using Diffusion Models for Architectural Floor plan generation [1] or other kinds of layout generation [2], which reduces the novelty of this work. The author(s) also did not cite these other related work and/or discuss the relationship/difference between the presented work and prior work.\n\nReferences:\n\n[1] HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising. Mohammad Amin Shabani, Sepidehsadat Hosseini, Yasutaka Furukawa. CVPR 2023\n\n[2] LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi. CVPR 2023"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper proposes using a diffusion model to generate architectural floor plans. The paper shows the process of constructing a dataset and training details.",
        "strengths": "The authors experimented with using a diffusion model to generate floor plan designs and presented several results.",
        "weaknesses": "1. **Excessive Unnecessary Details**: The paper contains considerable redundancy, with numerous unnecessary details, such as the advantages and rationale for using diffusion models, specifics of the U-net architecture, and exact function names from OpenCV in the code. These details occupy a significant portion of the content (around 50%) but do not provide valuable insights.\n\n2. **Lack of Novelty**: The paper does not demonstrate sufficient contribution or value in terms of model design, dataset construction, or performance presentation.\n\n3. **Disorganized Structure**: With a total of 13 primary headings, the paper’s structure is confusing for readers, making it difficult to grasp the core content. Many sections could be merged to improve readability.\n\n4. **Poor Performance Presentation and Analysis**: The paper lacks numerical results and provides insufficient visual examples to adequately showcase the model’s performance."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper is a report for training unet as a diffusion model to generate floor plan images",
        "strengths": "These steps are clear and techniques are correct.",
        "weaknesses": "This paper does not seems to propose a method. It is a report to describe an experiment.\nIt describes how to process images, how to build unet, how to train, how to write data augmentation codes, how to use postprocessing like upscale. But I do not think it has proposed some methodology technically."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper proposes an application of diffusion models to the generation of architectural floor plans, fine-tuning diffusion models to learn implicit design concepts in architectural design, and generating detailed and functional architectural floor plans.",
        "strengths": "The paper explores the intersection of AI and architectural design, achieving promising visual results. I highly appreciate and commend the authors' attempt. However, it needs significant improvements for a top-tier conference.",
        "weaknesses": "**Lack of related work:** The paper proposes a new pipeline for generating floor plans. However, the authors lack a substantial amount of related work, including AI-assisted architectural design and generative model-related work.\n\n**Limited technical innovation:** The technical innovation in this paper is quite insufficient, and the introduction to the U-Net architecture is entirely superfluous.\n\n**Missing dataset:** It would be beneficial to introduce more types of architectural styles and layouts. The current dataset is still quite limited (Residential floor plan only ).\n\n**Insufficient evaluation:** The paper lacks quantitative metrics and comparisons of related methods, including how to assess the rationality of generated floor plans. For design tasks, more professional architectural designers' user evaluations may be needed.\n\nI consider this to be an inspiring report on the interdisciplinary area. I encourage the authors to conduct more detailed technical innovation and experimental evaluation. For ICLR, this paper clearly lacks innovation and systematic methodology. Therefore, I believe this paper would be more suitable for submission to architectural design-related conferences, as it does not quite meet the threshold for current AI conferences."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.5,
    "decision": "Reject",
    "meta_review": "This research introduces a novel application of diffusion models adapted for the generation of architectural floor plans. The paper gives implementation and training details and demonstrates their method with several results. However, there are concerns over the technical contributions, novelty of the paper. Moreover, the proposed method is not well-evaluated. Therefore, I don't recommend this paper.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "XBQSCeMSMA",
    "title": "Diffusion-based Graph Masked Autoencoders for Out-of-Distribution Generalization",
    "authors": [
      "Jiahao Liang",
      "Zhiwen Yu",
      "Yang Hu",
      "Xiaoqing Liu",
      "Tong Zhang",
      "Kaixiang Yang"
    ],
    "abstract": "Graph Out-of-Distribution (GraphOOD) problems have become increasingly significant in the field of graph neural networks. Graph Neural Networks (GNNs) are particularly vulnerable to performance degradation when facing distribution shifts. This is due to the intricate interconnections between nodes in graph data and the lack of environmental labels, making it difficult to ensure model reliability. Recent advances in computer vision have shown that Diffusion Models(DMs) have strong generalization capabilities, providing a natural advantage in mitigating the effects of distribution shifts. Specifically, DMs can effectively capture and generate details of data distributions through a stepwise denoising process, thereby enhancing model robustness. However, applying diffusion to GraphOOD problems presents challenges, such as learning invariant knowledge that remains unaffected by distribution shifts. To address this, we propose a diffusion-based pre-training model for GraphOOD, termed $\\textbf{D}$iffusion-based $\\textbf{M}$asked $\\textbf{A}$uto$\\textbf{E}$ncoders on Graph Out-of-Distribution Generalization (DiffGMAE). Firstly, we propose a novel empirical risk minimization (ERM) approach that enhances the data by progressively adding noise, called the NoisedERM module, which aims to learn invariant features and avoid corrupting the discrete information of the original graph. Then, we design a self-supervised learning module called DiGMAE, which replaces the traditional MAE decoder with a diffuse-based denoising process. The aim is to use the invariant features obtained by NoisedERM for conditional diffusion and improve the robustness of the model in a self-supervised way to cope with the distribution shift of GraphOOD problem. We demonstrate significant improvements in DiffGMAE on OOD benchmarks. In addition, our ablation experiments show that the diffusion process is superior to traditional graph generation methods in solving OOD problems. The implementation code is available in \\textbf{Supplementary material} for reproducibility.",
    "keywords": [
      "Machine Learning; Deep learning; Graph learning; Self-supervised learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=XBQSCeMSMA",
    "forum_url": "https://openreview.net/forum?id=XBQSCeMSMA",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "KA2Rit4ky1",
    "title": "PDETime: Rethinking Long-term Multivariate Time Series Forecasting from the Perspective of Partial Differential Equations",
    "authors": [
      "Shiyi Qi",
      "Zenglin Xu",
      "Yiduo Li",
      "Liangjian Wen",
      "Qingsong Wen",
      "Qifan Wang",
      "Yuan Qi"
    ],
    "abstract": "Recent advancements in deep learning have led to the development of various approaches for long-term multivariate time-series forecasting (LMTF). Most of these approaches can be categorized as either historical-value-based methods, which rely on discretely sampled past observations, or time-index-based methods that model time indices directly as input variables. However, real-world dynamical systems often exhibit nonstationarity and suffer from insufficient sampling frequency, posing challenges such as spurious correlations between time steps and difficulties in modeling complex temporal dependencies.\nIn this paper, we treat multivariate time series as  data sampled from a continuous dynamical system governed by partial differential equations (PDEs) and propose a new model called PDETime. \nInstead of predicting future values directly, PDETime employs an encoding-integration-decoding architecture: it predicts the partial derivative of the system with respect to time (i.e., the first-order difference) in the latent space and then integrates this information to forecast future series. This approach enhances both performance and stability, especially in scenarios with extremely long forecasting windows. Extensive experiments on seven diverse real-world LMTF datasets demonstrate that PDETime not only adapts effectively to the intrinsic spatiotemporal nature of the data but also sets new benchmarks by achieving state-of-the-art results.",
    "keywords": [
      "long-term multivariate time series forecasting"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=KA2Rit4ky1",
    "forum_url": "https://openreview.net/forum?id=KA2Rit4ky1",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The paper presents PDETime, a novel approach for long-term multivariate time-series forecasting that models the series as a continuous dynamical system governed by partial differential equations (PDEs). Instead of directly forecasting future values, PDETime predicts the partial derivatives in the latent space, integrating this information over time to generate forecasts.",
        "strengths": "1. The application of PDEs in time-series forecasting introduces a unique perspective for capturing continuous dynamical patterns.\n2. The authors test PDETime on a wide range of datasets, demonstrating the model’s adaptability.",
        "weaknesses": "1. **Fairness in Comparison**: My most concern is that the experiments may lack fairness due to differences in historical input length (\\(H\\)) between PDETime and baseline models. While PDETime’s \\(H\\) is optimized, baseline models use a fixed input length, which can skew results since (1) different input lengths impact the number of samples in the test set for each model, potentially affecting comparability, and (2) input length significantly influences forecasting performance. It’s recommended to either standardize \\(H\\) across models or optimize it for all baselines.\n2. **Code Availability**: The absence of released code reduces the credibility and reproducibility of the results, as reviewers and readers cannot verify the findings independently.\n3. **Unclear Loss Weighting**: The weight description for the loss function in Equation 14 is unclear, making it challenging to understand the balance between different loss components.\n4. **Lack of Efficiency Analysis**: There is no discussion of PDETime’s computational efficiency, such as runtime or memory consumption, which is essential for understanding its scalability and practicality for long-term forecasting tasks."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "In this paper, the authors propose PDETime, a novel model for time-series\nforecasting. The methods novelty lies in the fact, that it does not simply\npredict values for a specific time point, but that it generates an encoding\n$\\alpha_t$ which can be interpreted as a first deviation. Then, the dynamics\nof the hidden representation $z_t$ of a time in the interval $[t_0,t] $ are computed\nvia an integral solver over $\\int_{t_0,t} \\alpha_t $ and this is then used to decode via $\\text{Decoder}(z_t) + x_{t_0}$.",
        "strengths": "+ The idea of capturing the time-dynamics and treating time-series forecasting as an initial-condition problem is a cool idea.\n+ The results are promising, PDETime always outperforms all competitors\n+ The approach could be potential starting point for a new direction in time-series forecasting",
        "weaknesses": "- Please write a clear problem formulation: In Time-Series forecasting, what do\n  you have given, what do you want to predict/which objective do you want to\n  optimize? What are the domains your inputs and outputs live in. I had to\n  somehow guess here sometimes at the beginning, especially it was not clear to\n  me at the beginning what $s$ is, and whether it is given or not.\n- I really like the PDE motivation, but what you at the end do is simply to\n  learn first derivations at time points instead of learning the values\n  them self. As your spatial input $s$ is computed in the encoder and you are\n  only considering the derivation with respect to time for fixed s, one could\n  argue that at the end you are only considering ODEs with respect to time.\n  However, then what you do is already established in the literature for\n  time-series forecasting with irregular time-points, see [1], [2], [3].\n\n[1] De Brouwer, E., Simm, J., Arany, A., & Moreau, Y. (2019). GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series. Advances in neural information processing systems, 32.\n\n[2]Scholz, Randolf, et al. \"Latent Linear ODEs with Neural Kalman Filtering for Irregular Time Series Forecasting.\" (2023).\n\n[3] Schirmer, Mona, et al. \"Modeling irregular time series with continuous recurrent units.\" International conference on machine learning. PMLR, 2022.\n- As your approach is fundamentally different to established transformer models,\n  it is important to integrate a runtime-study. What does the performance gain\n  over PatchTST cost with respect to efficiency?"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The authors propose a novel model, PDETime, for long-term multivariate time series forecasting. PDETime assumes that the multivariate time series is sampled from a continuous dynamical system governed by a partial differential equation (PDE). Unlike existing approaches that rely solely on historical observations or discrete time points, PDETime leverages both observations and time for forecasting.\n\nPDETime encodes the partial derivatives of the system, solves them for future time points in a latent space using the proposed PDE solver, and then decodes the results back to the data space with a decoder. Experiments conducted on seven multivariate datasets with varying forecast horizons show significant improvement in forecasting accuracy compared to baseline models.",
        "strengths": "**S1** Viewing multivariate time series as instances of partial differential equations (PDEs) is an intriguing approach. This perspective is particularly relevant for multivariate time series in which sensors at different locations measure the same thing (e.g., electricity data).\n\n**S2** Authors proposed a new PDE solver which can potentially avoid the problems with Neural-ODE solvers\n\n**S2** The experimental results on various datasets indicate that the approach outperforms a range of baselines in multivariate longterm forecasting. Various ablation studies have been shown in the experiments",
        "weaknesses": "**W1. Writing Quality:**\n\n1. The paper contains undefined variables when first introduced. E.g. $\\mathbf{X}{his}$, $\\mathbf{c}{t}$, and $\\tau_t$ in line 8, as well as $\\mathbf{W}{\\tau}$, $\\mathbf{W}c$, $\\mathbf{W}x$, $\\mathbf{W}$, $\\mathbf{b}\\tau$, $\\mathbf{b}c$, $\\mathbf{b}x$, and $\\mathbf{b}$. Please define any variable when it is first introduced.\n2. Section 3.1 has an incorrect sub-section title. The section is titled \"Problem Formulation\" but describes the model's overview. Problem formulation typically refers to the specific problem the paper addresses.\n3. Some statements are overly vague. For instance, in lines 502-505, \"Finally, our approach of rethinking long-term ... promising direction for future research\" lacks clarity. Please mention what new perspectives do the authors wish to explore?\n4. Please review the order of all tables in the paper; Table 1 should appear before Table 3.\n5. Typos:\n\n5.1 line 52: hiders $\\rightarrow$ hinders\n\n5.2 line 294: $\\textbf{x}{x_0} \\rightarrow \\textbf{x}{t_0}$\n\n5.3 line 151: $\\textbf{u}(t + \\Delta) \\rightarrow \\textbf{u}(t + \\Delta t)$\n\n5.4 lines 77-80: \"Furthermore, as shown in .... capture temporal dependencies\" first sentence doesn't complete the thought\n\n5.5 Eq. 10, $\\mathcal{L}($: \"(\" did not close\n\n5.6 Alg 2, line 4: $t'$ appeared twice\n\n6. There are many other notational issues which require careful attention from authors.\nex. time index have subscript at some places like eq. 2 and superscript in eq. 1\n\n**W2.** Motivation for considering multivariate time series as an instance of PDE is missing. Can authors provide clear motivation behind this approach? Would this assumption hold for multivariate time series data with variables measuring different aspects, such as a physiological dataset where one variable tracks pulse and another tracks blood pressure?\n\n**W3. Model Scope:**\n\n1. Why is the model limited to long-horizon predictions? How does it perform on short horizons?\n2. The authors state that existing models rely on either historical observations or time points alone. However, the Informer model uses both local and global time information as embeddings. Can authors clarify this?\n\n**W3. Encoder Architecture:**\n\n1. What is the motivation behind using this particular encoder architecture?\n2. The encoder’s design is challenging to understand due to insufficient explanations:\n- Clearly define and distinguish between $X^{(k)^i}$ and $\\mathbf{X}^{(K)^i}$\n- Provide a detailed explanation of the operations in Equation 7\n- Explain the rationale behind adding $\\mathbf{c}_t^{(k)}$ again after concatenation\n- Explain the discrepancy between Alg 1 and Eq 7 (see line 3, Alg 1)\n\n3. Eq. 6; why (6a) and (6b) LHS have subscript $t$ and no information of it on RHS?\n4. What is For loop in Alg 1? Is it clearly mentioned in main text? \n\n**W4. Solver:**\n\n1. The paper does not clearly explain how the proposed solver circumvents the issues associated with the Euler solver. Please provide a comparison between their proposed solver and the Euler/Neural-ODE solver\n2. Figure 2b does not effectively illustrate the solver. A clearer figure would help.\n3. The PDE solver is claimed to be a novel and key contribution. To substantiate this, could the authors demonstrate the advantages of this solver over existing Neural ODE solvers (e.g., Chen et al., 2018) through a toy example?\n4. In line 270, the authors state that Figure 3 illustrates the solver’s advantages over the Euler solver. However, it is difficult to see this comparison, as Figure 3 primarily shows results for hyperparameters $k$, $N$, and $S$.\n\n**W5. Experiments:**\n\n1. Please clarify the input and prediction sequence lengths for the results in Table 1. Comparing Tables 1 and 7, it appears the forecasting horizon is 336 (please correct me if this is incorrect)\n2. There is a discrepancy in the third decimal place of Patch TST results between these tables if the forecasting horizon is 336\n3. Please inform about Table 7 in paragraph after Section 4.2, otherwise the text is confusing"
      },
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The paper provides a novel approach, PDETime, for long-term multivariate time-series forecasting (LMTF) by modeling time series data as samples from a continuous dynamical system governed by partial differential equations. PDETime introduces an encoding-integration-decoding architecture to predict future values in the latent space by estimating the partial derivative of the system with respect to time. The method employs a neural solver to address the nonstationarity and sampling limitations common in traditional historical-value- and time-index-based models. Finally, the author provides experimental results demonstrating that PDETime achieves state-of-the-art performance across seven benchmark datasets. In my opinion, the paper is organized and easy to follow, but I have some questions.",
        "strengths": "1. The paper presents a unique approach for time-series forecasting by framing the task as an initial value problem governed by PDEs, which provides a theoretically grounded approach for capturing spatiotemporal dependencies.\n2. The architecture effectively leverages the latent space to model complex temporal dependencies, mitigating issues of error accumulation that are common in autoregressive methods.\n3. Extensive evaluations across seven real-world datasets reveal that PDETime outperforms leading historical-value-based and time-index-based models, including Transformer and CNN models, demonstrating robust, stable performance over varying forecasting lengths.\n4. The study includes comprehensive ablation experiments on key components (e.g., Solver, temporal and spatial features) and evaluates the impact of hyper-parameters, enhancing the reproducibility and scientific rigor of the results.",
        "weaknesses": "1. The use of numerical solvers (e.g., Euler Solver) could introduce sensitivity to hyper-parameter tuning and potential limitations in modeling high-frequency or abrupt temporal changes.\n2. The encoding-integration-decoding process, along with meta-optimization, may add computational overhead, especially for large datasets or models with high-dimensional data, limiting scalability."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper proposes a novel method for multivariate time series forecast by treating the input continuous dynamical system governed by a latent PDE equation. Instead of predicting future values directly, PDETime predicts the derivative and uses Euler method to integrate the derivative, and applies encoder and decoder for input embedding and yielding the final output. This approach enhances both performance\nand stability, especially in scenarios with long-horizon forecast. Extensive experiments on seven diverse real-world LMTF datasets demonstrate that PDETime not only adapts effectively to the intrinsic spatiotemporal characteristics.",
        "strengths": "The key idea of this paper is novel: the author introduces a PDE solver to solve time series instead of using historical data as input, which may be able to capture the inter-series relationship for enhanced accuracy. The paper is also well written with solid experiments.",
        "weaknesses": "The literature review setion seems a little over-compressed, especially the multivariate time series literature. In fact, there is much more literature in the past 2 years that seeks to provide forecast from multiple perspectives. For example, Transformer-based methods, linear interpolation methods (D-linear) and auxiliary series construction (CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables, ICML 2024) and so on. I would suggest that the author should enrich their literature on multivariate time series, and compress the neural PDE literature because it is only utilized as a component of the model. \n\nAlso the benchmark methods are not sufficient as there are many papers in 2024 that address LMTF from LLM perspectiuve, including LLM4TS, GPT4TS, CATS and so on. I strongly encourage the author to add these benchmarks into their comparison. \n\nI would gladly increase my rating if my concerns are addressed."
      }
    ],
    "rating_avg": 4.8,
    "confidence_avg": 3.6,
    "decision": "Reject",
    "meta_review": "This paper introduces PDETime, a novel approach to long-term multivariate time-series forecasting using partial differential equations (PDEs) to model the temporal dynamics of latent representations. While the perspective of treating time series as samples from a PDE is intriguing, the paper has critical issues that limit its impact and clarity. The work lacks sufficient empirical rigor, with comparisons to established baselines potentially biased due to differences in input length and insufficient benchmarking against recent advanced methods. While the PDE-based modeling is positioned as novel, the approach seems more akin to ordinary differential equations (ODEs), with limited exploration of spatial derivatives, reducing the distinctiveness of the method. Additionally, concerns about the computational efficiency and scalability of the proposed solver are unaddressed, raising questions about its practicality for real-world use. The presentation has several weaknesses, including unclear problem formulation, inconsistent notation, and an over-reliance on vague claims rather than explicit justifications. While the experimental results appear promising, the lack of code availability and detailed runtime analyses further hinders reproducibility and assessment. These shortcomings, combined with the overextension of the claimed contributions, justify a decision of Reject.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "ia7XI8qvBv",
    "title": "CELI: CONTROLLER-EMBEDDED LANGUAGE MODEL INTERACTIONS",
    "authors": [
      "Jan-Samuel Wagner",
      "Dave DeCaprio",
      "Hosein Barzekar",
      "Mark Anthony Martinez II",
      "Hisham Hamadeh",
      "Scott Ogden"
    ],
    "abstract": "We introduce Controller-Embedded Language Model Interactions (CELI), a framework that integrates control logic directly within Language Model (LM) prompts, facilitating complex, multi-stage task execution. CELI addresses limitations in existing prompt engineering and workflow optimization techniques by embedding control flow into the LM's operational context, enabling dynamic adaptation to evolving task requirements. Our framework transfers control from the traditional programming execution environment to the LMs, allowing them to autonomously manage computational workflows while maintaining seamless interaction with external systems and functions. CELI supports arbitrary function calls with variable arguments, bridging the gap between LMs' adaptive reasoning capabilities and conventional software paradigms' structured control mechanisms. To evaluate CELI's versatility and effectiveness across diverse problem domains, we conducted three case studies: code generation (HumanEval benchmark), hierarchical content generation (Wikipedia-style articles), and multi-table data harmonization and reconciliation (supply chain auditing with inconsistent datasets). Results demonstrate significant performance enhancements across diverse domains. CELI achieved a 4.9 percentage point improvement over the best reported score of the baseline GPT-4 model on the HumanEval code generation benchmark. In hierarchical content generation, 78% of CELI-produced Wikipedia-style articles reached first draft quality when optimally configured. For multi-table data harmonization, CELI achieved perfect data cleaning and harmonization in a supply chain audit task, while detecting 64% of customer-manufacturer dispute discrepancies, similar to two human reviewers. These outcomes underscore CELI's potential for optimizing AI-driven workflows across diverse computational domains. CELI represents a paradigm shift in LM utilization, offering a flexible yet robust solution for managing intricate tasks that require both nuanced natural language processing and precise programmatic execution.",
    "keywords": [
      "AI agents",
      "artificial intelligence",
      "machine learning",
      "natural language processing",
      "autonomous systems",
      "intelligent automation",
      "large language models",
      "AI problem-solving",
      "adaptive AI",
      "multi-task AI",
      "AI workflow optimization"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=ia7XI8qvBv",
    "forum_url": "https://openreview.net/forum?id=ia7XI8qvBv",
    "reviews": [],
    "rating_avg": null,
    "confidence_avg": null,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "ZGRZ5GPKWX",
    "title": "DNASpeech: A Contextualized and Situated Text-to-Speech Dataset with Dialogues, Narratives and Actions",
    "authors": [
      "Chuanqi Cheng",
      "Hongda Sun",
      "Xinrong Hu",
      "Rui Yan"
    ],
    "abstract": "In this paper, we propose contextualized and situated text-to-speech (CS-TTS), a novel TTS task to promote more accurate and customized speech generation using prompts with Dialogues, Narratives, and Actions (DNA). While prompt-based TTS methods facilitate controllable speech generation, existing TTS datasets lack situated descriptive prompts aligned with speech data. To address this data scarcity, we develop an automatic annotation pipeline enabling multifaceted alignment among speech clips, content text, and their respective descriptions. Based on this pipeline, we present DNASpeech, a novel CS-TTS dataset with high-quality speeches with DNA prompt annotations. DNASpeech contains 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances, along with over 18 hours of high-quality speech recordings. To accommodate more specific task scenarios, we establish a leaderboard featuring two new subtasks for evaluation: CS-TTS with narratives and CS-TTS with dialogues. We also design an intuitive baseline model for comparison with existing state-of-the-art TTS methods on our leaderboard. Comprehensive experimental results demonstrate the quality and effectiveness of \\dataname, validating its potential to drive advancements in the TTS field.",
    "keywords": [
      "Text-to-Speech",
      "Voice Generation",
      "Prompt"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=ZGRZ5GPKWX",
    "forum_url": "https://openreview.net/forum?id=ZGRZ5GPKWX",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper proposes a new TTS task and benchmark to produce contextualized and situated synthetic speech using dialogues, narratives and actions.",
        "strengths": "The new dataset would be useful to the TTS community and will be made available\nThe data creation methodology seems reasonable as does the design of the DNA Speech model.",
        "weaknesses": "1. The precise meaning of situated and contextualized is not very clear from my reading of the paper. Further, it is not very clear how actions in particular aid in situated and contextualized TTS. \n\n2. From the data pipeline, it is not clear whether the obtained subtitles exactly match the speech, or are machine generated in some way. There seem to be many automated portions, for example, obtaining subtitles through OCR, getting Dialogues, Actions, Narratives and Characters from the original movie scripts, speech denoising etc. For all of these steps, there are no objective measures of quality reported, which casts doubt on the quality of data used.  Furthermore, the only quality evaluation used involves training a TTS models using DNASpeech and evaluating it. \n\n3. The proposed ASR filtering based on Whisper could be potentially aggressive because the authors remove all non perfect matches. This means that the data obtained is selected based on Whisper's biases for movie transcription, which is not ideal."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduces DNASpeech, a novel contextualized and situated text-to-speech (CS-TTS) dataset designed to enhance TTS performance by incorporating prompts from dialogues, narratives, and actions (DNA). DNASpeech provides rich multimodal prompts aligned with speech clips, filling a gap in current datasets that lack comprehensive contextual information for TTS tasks. The dataset includes 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances, along with over 18 hours of high-quality speech recordings. To validate DNASpeech, the authors propose a leaderboard and evaluation benchmarks featuring two subtasks: CS-TTS with narratives and CS-TTS with dialogues. The experimental results demonstrate the potential of DNASpeech to drive advancements in controllable and expressive TTS systems.",
        "strengths": "The integration of dialogues, narratives, and actions (DNA) as contextual prompts is an innovative addition to existing TTS datasets, providing richer and more varied situational context. The dataset is constructed using a detailed and well-validated annotation pipeline, with emphasis on quality control through denoising, ASR verification, and manual assessment. The alignment method that combines both coarse-grained and fine-grained techniques is robust and well-implemented. The dataset addresses a crucial need for more context-aware TTS systems and offers a structured evaluation through the established leaderboard. This contribution is likely to inspire further research in controllable TTS using diverse prompts. Additionally, the paper provides a thorough explanation of the dataset construction and the challenges involved, with the use of visual aids to depict the pipeline and dataset characteristics aiding in comprehension.",
        "weaknesses": "The experiments primarily focus on validating the dataset using specific subtasks (narratives and dialogues), but they could benefit from broader model diversity and more diverse metrics beyond MOS evaluations. Including results from a larger variety of baseline models would make the evaluation more comprehensive. While the paper claims that DNASpeech can generalize well for different TTS tasks, the experimental evidence supporting this claim is limited, and testing with a wider set of models and comparing performance on tasks beyond CS-TTS (e.g., emotional TTS) would strengthen this assertion. Additionally, the dataset's reliance on movie scripts might limit its applicability for general conversational TTS, as the movie-based context might not fully represent day-to-day conversational dynamics."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "This paper introduces DNASpeech, a novel contextualized and situated text-to-speech (CS-TTS) dataset that incorporates comprehensive descriptive prompts aligned with speech data. The dataset contains \"DNA\" prompts - Dialogues (conversational context), Narratives (environmental scenes), and Actions (speaker's expressions/actions) - along with high-quality speech recordings. DNASpeech includes 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances totaling over 18 hours of speech. The authors developed an automated annotation pipeline for aligning speech clips, content text, and descriptions. They also established a leaderboard with two evaluation subtasks: CS-TTS with narratives and CS-TTS with dialogues. The paper proposes a baseline model and demonstrates DNASpeech's effectiveness through extensive experiments comparing it with existing TTS methods.",
        "strengths": "1. The paper introduces a novel and valuable contribution to TTS research through DNASpeech, a contextualized and situated text-to-speech dataset that incorporates comprehensive \"DNA\" (Dialogues, Narratives, Actions) prompts.\n\n2. The authors develop an innovative automatic annotation pipeline that enables efficient multifaceted alignment among speech clips, content text, and corresponding descriptions, making the dataset construction process systematic and reproducible.\n\n3. The dataset is substantial and diverse, containing 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances with over 18 hours of high-quality speech recordings, providing rich resources for TTS research.\n\n4. The paper establishes a clear evaluation framework through a leaderboard featuring two specific subtasks (CS-TTS with narratives and CS-TTS with dialogues) and provides an intuitive baseline model for comparison.",
        "weaknesses": "1. Although the authors compared the dataset and two models on the other datasets, the dataset's reliance on movie scenes rather than real-world scenarios might limit its applicability to authentic speech patterns and natural conversations.\n2. The experimental evaluation metrics are somewhat limited, primarily focusing on MOS scores. Additional objective metrics could provide more comprehensive performance assessment such as spectral distortion or character error rates.\n3. The paper lacks detailed analysis of the baseline model's architecture choices and their impact on performance.\n4. The comparison with existing methods could be more extensive, particularly in analyzing how different types of prompts affect the speech generation quality. \n5. Similarly, for the public dataset comparison, the author did not select the SOTA models for the comparison. It would be great to see the comparison against them."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "This paper introduces a novel text-to-speech (TTS) dataset, DNASpeech, which is designed to support contextualized and situated TTS (CS-TTS) tasks. The dataset includes high-quality speech recordings annotated with Dialogues, Narratives, and Actions (DNA) prompts, aiming to enhance the accuracy and expressiveness of speech synthesis. Experimental results are provided to validate the quality and effectiveness of DNASpeech.",
        "strengths": "1. This paper proposes the \"DNASpeech\" dataset. It addresses a significant gap in existing TTS datasets by providing a rich dataset with contextualized and situated prompts, which is crucial for advancing TTS research.\n\n2. The paper describes the comprehensive automatic annotation pipeline that aligns speech clips with detailed dialogue, narrative, and action descriptions, which is a complex and valuable contribution.",
        "weaknesses": "1. In Sec. 3.2, the authors individually apply information extraction for both speech and scripts in the movie in step 2. Then in step 3, they attempt to align them in two stages.\n* 1.1 Why \"more than 800 million potential matches are required\"? Since you can align the movie and script by movie titles or other meta information. And for the \"DNA\" prompt, why did the authors choose to extract them from the scripts with such a heuristic algorithm？ What is the accuracy of the alignment? Other methods such as extracting speech attributes directly are not considered.\n* 1.2 \"Following the script writing paradigm, we extract four key elements from each movie script: Dialogues Narratives, Actions, and Characters.\" How do you extract them? Please illustrate it in detail.\n* 1.3 All data is from the movie. So there is a risk of domain bias. Because the movie can not cover all diverse accents, languages, or speaking styles.\n\n2. This paper is an extension of textual-prompt-based text-to-speech synthesis. The authors propose to extend the descriptive prompt of speech to three dimensions: 1) dialogue, 2) narrative, and 3) action. However, it is only an incremental work of the existing prompt-tts paradigm by extending the annotation pipeline. So it lacks novelty.\n\n3. In the experiments:\n* 3.1 It is better to categorize into three types: 1) None-Prompt TTS, 2) natural language description prompt-based TTS, and 3) speech prompt-based TTS.\n* 3.2 The method for CS-TTS in Sec. 4.1.2 is not clear. What is for \"but includes classification tasks for emotion, pitch, energy, and speed during training\"? For emotion, how do you obtain the label? Furthermore, it is not clear how the author leverages the \"DNA\" prompt as a condition to guide the generation process.\n* 3.3 It lacks an ablation study for the attribution controllability for the proposed \"DNA\" attributes.\n\n4. The obtained dataset contains about 18 hours including 2395 distinct characters, indicating that only 0.45 minutes for a single character. It is small to train a good TTS system."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This work introduces a new TTS task called **Contextualized and Situated Text-To-Speech** (CS-TTS), which incorporates contextual descriptions into speech generation, aiming to enable TTS models to produce more expressive speech. As the lacking of CS-TTS datasets, they created a new CS-TTS dataset called **DNASpeech**. Each speech sample in DNASpeech is accompanied by three types of contextualized prompts: **Dialogues** provide conversational context, **Narratives** describe the environment surrounding the speaker, and **Actions** detail the speaker’s actions and expressions. For dataset construction, they developed an automated annotation pipeline, with human evaluations to validate the approach.\n\nAdditionally, they established a leaderboard to assess the performance of current TTS systems, and introduced a baseline model adapted to the CS-TTS.",
        "strengths": "1. This work introduces a new dataset called DNASpeech, specifically created for the innovative CS-TTS task.\n2. An automatic annotation pipeline is presented, utilizing techniques likes OCR, speech denoising, ASR to ensure the quality of produced dataset. Additionally, human evaluation is conducted to confirm the effectiveness of the pipeline.",
        "weaknesses": "## Weaknesses \n\n1. The paper asserts that contextualized descriptions lead to more accurate and expressive speech generation. However, there is only one experiment validating the effectiveness of CS-TTS, and it shows no significant improvement when using contextualized descriptions. For example, in evaluating the alignment between speech and environmental information, the MOS-E score gap between prompt-based TTS methods and non-prompted TTS is less than 0.1. StyleTTS, the best non-prompted TTS model, performs comparably to the prompt-based models. This makes it difficult to confirm the quality of the proposed dataset and the effectiveness of CS-TTS.\n2. The work introduces a TTS dataset where each sample includes three types of contextual prompts : **Dialogue**, **Narrative**, and **Action**. It is claimed that **Action** describes the speaker's actions and expressions, while **Narrative** provides environmental context, as mentioned in lines 74-76. However, based on the descriptions and examples given in the paper, these categories are difficult to differentiate. Take Figure 1 for an example, the Action \"showing the gun to JURORS\" corresponds to the Narrative \"He picks up a revolver...\", which is also an action. There is also confusion as to why the speaker's emotions are categorized under Action. Furthermore, in lines 142-144, the authors state that MEAD-TTS highlights environmental information (MEAD-TTS seems to focus on Action since it uses templates like \"A <gender> says with a <emotion level> <emotion> tone\" to write fine-grained prompts), yet in Table 1, MEAD-TTS's prompt is categorized under Actions, not Narratives, which contradicts the definitions provided in lines 74-76. Additionally, it seems like DailyTalk's focus is more on Dialogues than Narratives, given that DailyTalk focuses on chat history.\n3. One of the core contributions of the work is the automatic annotation pipeline for building the CS-TTS dataset from movie scripts. However, the description of this pipeline is unclear, making it difficult to understand how different type of prompts are extracted from movie scripts.\n\n## Suggestions \n\n1.  It would be better to use $\\cite$ or other citation formats instead of $\\citet$ when the cited paper is not the subject or object in a sentence. It becomes harder to read when citations are embedded in the main text. For example, in lines 32-34, it could be:\n> Text-to-speech (TTS) aims to convert input text into human-like speech, attracting significant attention in the audio and speech processing community (Shen et al. 2018; Ren et al. 2020; Shen et al. 2023; Ju et al. 2024).\n2. Many TTS-related datasets are mentioned in the related work section. However, the descriptions seem to be directly copied from the original papers, resulting in inconsistent types of information for each dataset. The writing does not highlight the core differences between previous work and this paper. It would be better to reorganize Section 2.1.\n3. To improve the integrity of the experiments, it would be helpful to explain the source of the human evaluators and what the interface or  instructions shown to the evaluators.\n4. When citing papers published at conferences, it is better to reference the conference version rather than the arXiv version. For instance, FastSpeech2 was published at ICLR 2021, but this paper cites its arXiv version.\n\n## Typos\n\n1. In Table 1, \"MM-TTS\" should be \"MEAD-TTS\", as the former refers to the TTS system and the latter is the name of the dataset.\n2. A period is missing at the end of line 361."
      }
    ],
    "rating_avg": 4.6,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "RDLvnUJ5JZ",
    "title": "TF-score: Time-series Forecasting using score-based diffusion model",
    "authors": [
      "Haksoo Lim",
      "Jaehoon Lee",
      "Jaesik Choi"
    ],
    "abstract": "Diffusion models have emerged as powerful generative models, capable of synthesizing high-quality images by capturing complex underlying patterns. Building on this success, these models have been adapted for time-series forecasting, a domain characterized by intricate temporal dependencies. However, most existing works have focused primarily on empirical performance without sufficient theoretical exploration. In this paper, we address this gap by introducing a generalized loss function within the diffusion-based forecasting framework. Leveraging this foundation, we introduce TF-score, a score-based diffusion model designed to capture the interdependencies between historical data and future predictions. Extensive experiments across six benchmark datasets show that TF-score consistently surpasses leading baselines, including prior diffusion-based models. Furthermore, we extend existing guidance sampling strategies into a our score-based formulation, achieving performance gains across multiple datasets while providing a detailed analysis of the trade-offs involved.",
    "keywords": [
      "Time-series forecasting",
      "Diffusion models",
      "Signal processing"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=RDLvnUJ5JZ",
    "forum_url": "https://openreview.net/forum?id=RDLvnUJ5JZ",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces TF-score, a score-based diffusion model designed for time-series forecasting. The model applies conditional generative diffusion modeling to time-series forecasting tasks, using historical data as conditioning factors and the complete data sequence as the generation target. Additionally, a mask vector is incorporated into the loss function to separate historical and future data.",
        "strengths": "​1. **Appropriate Application of Diffusion Models**: The paper effectively adapts diffusion models for time-series forecasting, a relatively novel domain for such models, building on their successes in generative tasks.\n\n​2. **Unified Framework**: The authors integrate existing diffusion models for time-series forecasting into a continuous score-based framework, enhancing the theoretical foundation of their approach.\n\n​3. **Experimental Performance**: TF-score outperforms multiple benchmark models across various datasets, validating its effectiveness in practical forecasting tasks.",
        "weaknesses": "​1. **Excessive Background**: The paper dedicates substantial space to explaining the background of diffusion models, the time-series forecasting task, and how diffusion models are applied in this area. The authors even include descriptions of existing embedding and guidance methods and the computation of the CRPS metric. This leaves limited space to discuss their own contributions, making the paper feel more like a course report than a research paper.\n\n​2. **Lack of Innovation**: The paper appears to mainly apply existing conditional generation diffusion models to time-series forecasting tasks, with limited novelty. The only new element is the mask vector, which seems overly simplistic. Only about one-fifth of a page out of the 10-page main text is dedicated to introducing this new method, and this so-called new method merely adds a weight mask when calculating the sequence loss. Besides, the section that introduces this new method is even titled **ANALYSIS OF EXISTING METHOD**. Therefore, I don’t consider this to be an innovation."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes using a score-based diffusion model for time series forecasting. The main claimed contribution is the simultaneous generation of both historical context and future predictions using diffusion models. Another claimed contribution is the continuous score of the SDE form for each generation.",
        "strengths": "S1. the method details are presented clearly\nS2. the proposed method is simple, and it is easy to follow",
        "weaknesses": "W1. The investigation is insufficient. Many related works on time series diffusion models are missing, e.g., [1,2]. Some works have used score-based diffusion models for time series prediction [3]. \n\n- [1] NIPS'22 Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement\n- [2] ICML'23 Non-autoregressive conditional diffusion models for time series prediction \n- [3] ICLR'24 Interpretable Diffusion for General Time Series Generation\n\n\nW2. the statement below is not very convincing. Using the historical context as a condition to generate the future part (without generating the past window) can still capture the internal structure of the total sequence. \n> \"our method considers both the historical context and future predictions simultaneously and thereby captures the internal structure of total sequence,\" \n\nIn contrast, generating historical context may be limited:\n1) bring some unexpected bias to degrade the prediction performance when there is some unrelated historical information;\n2) increase computational burden when a long history context is included. In the experiments, the authors only use small window sizes (<100) for evaluations. And there is no analysis of computational efficiency. \n\nThus, generating the whole time series using a score-based diffusion model is not fully convincing."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This work builds on a growing literature of time series forecasting methods that use diffusion models for forecasting. First, the authors propose an analysis of families of related work, to introduce the forecasting problem and to cluster methods based on their loss function formulation. Here it is argued that generating the entire sequence (history + future values) has advantages over autoregressive approaches that use the history to predict future values.\n\nThe authors propose TF-score, a method that is supposed to generalize existing methods, and that can also incorporate a guidance mechanism. Experiments on well-known datasets indicate that the proposed method is on par with and sometimes superior to alternatives from the state of the art, including two diffusion-based methods. An ablation study complements the experiments, indicating that proper configuration of the proposed method is dataset dependent.",
        "strengths": "* I think the proposed objective of this paper is important, because in principle it could subsume existing diffusion-based time-series modeling approaches by presenting a more general formulation\n* Experiments and ablation studies are thorough, by considering several datasets and several alternatives from the literature, not limited to diffusion-based approaches to forecasting.\n* Despite some typos and a mathematical notation that can be improved, the article reads well and is easy to follow",
        "weaknesses": "* Sec.2.1 is very informal, and more importantly, uses precious space to outline methods that are not directly used in this work (only to review existing work in Sec. 2.2). Since score-based diffusion models through the lenses of stochastic differential equations are well known today, I suggest to make this part more compact, but at the same time more formal. Please, also double check grammar/typos (e.g., line 108: $f$ is an affine and $w$, …) and mathematical notation (check the commas at the end of the equations, e.g. line 108, line 141). Concerning the informal tone: see the comment below $L_{SM}$, the score term there is just not analytically available, therefore Song et al. condition the score on the initial sample, such that it becomes accessible, resulting in $L_{DSM}$. It is not a matter of computational effort or the need for statistical methods.\n\n* Sec. 2.2: pay attention to notation overload: $T$ is used both to indicate diffusion time as well as forecast horizon. Check expression clarity: for example, lines 155,157 do not read well and are vague.\nIn my opinion, eq.1 and eq.2 are not clear. For eq.1, the input to the score network is $x^{\\text{hist}}$, as well as the nosy $x^{\\text{pred}}$. Does it mean that $x^{\\text{hist}}$ is not used as a conditioning signal, which would be a natural choice? For eq. 2, the input is $x^{\\text{hist}}$ and $x^{\\text{total}}$: since $x^{\\text{hist}} \\in x^{\\text{total}}$, couldn’t we see this as an “inpainting\" approach?\nSince the goal of this section, also according to what claimed in the introduction, is to review and categorize in two classes existing approaches, I think it would be useful to provide the reader with more insights than just the two loss functions.\n\n* Sec. 3: this part contains, to the best of my knowledge, several mathematical mistakes. The first expressions in Sec. 3.1 do not make sense to me, especially regarding the score term which is manipulated without care, and with arbitrary choice of variables that are not compatible with score-based diffusion formalism. What does it mean to take the gradient with respect to $x^{\\text{pred}}$ of the log of the conditional density of $x^{\\text{pred}}_t$ given $x^{\\text{hist}}$? Are you trying to make the correspondence between $x^{\\text{hist}}$ and $x_0$ (that is the clean data) and $x^{\\text{pred}}_t$ to a noisy version of the clean data $x_t$? Note the problem: $x_t$ can be obtained in closed form from $x_0$, whereas obtaining $x^{\\text{pred}}_t$ from $x^{\\text{hist}}$ is exactly the problem you are trying to solve.\nSimilarly, eq. 3 and eq. 4 are shaky. In Eq. 3, how do you compute the gradient of the score (this time properly defined) with respect to $x^{\\text{pred}}$, which you do not have access to? In Eq. 4, isn’t it redundant to provide $x^{\\text{hist}}$ as an input to the score network $s(\\cdot)$, as it is already contained in $x^{\\text{total}}$?\nAlso, in line 212 it is said that weights are ignored for computational convenience. However, the weight $\\lambda$ is very important, as it determines what exactly you are optimizing: for example by setting $\\lambda(t)=g(t)^2/2$, minimizing the loss corresponds to maximum likelihood training [1, Sec. 2].\nFinally, it is necessary to delve into the details of the masking mechanism discussed in lines 251-252. It is used to discern past from future elements, which zeros out the future. This, in my opinion, is equivalent to the setup I alluded to in Sec.2 comments: essentially you can imagine $x^{\\text{total}}$ as an image, of which you zero out a region, leaving you with a portion that corresponds to $x^{\\text{hist}}$, which is amenable to an “inpainting” interpretation.\n\nNext, in sec 3.2.1 authors speak about generalizations of existing schemes to make the point that their approach is different from DiffWave. This is too strong of a claim, in my opinion.\n\nFinally, in sec 3.2.2 the authors could have discussed in more detail why the proposed method performs (slightly) better than diffusion-based alternatives such as TimeGrad and CSDI. For example, the initial message about classifying existing methods in two categories, and the intended take home message as to modeling $x^{\\text{total}}$ would have been stronger if properly compared and discussed.\n\n[1] Vahdat, Arash and Kreis, Karsten and Kautz, Jan, “Score-based Generative Modeling in Latent Space”, NeurIPS 2021.\n\n* Sec. 4: apart from traditional guidance, in Sec. 4.2 observation self-guidance is discussed (often attributing ideas to work from Song and Ho, which to the best of my knowledge do not refer to time-series data). The expression at line 380, which authors say implement Bayes rule, is ill defined: $p(a|b) \\sim p(b|a) p(a|b)$? The authors should double check, and consequently double check expression at line 383.\n\nOverall, I think the authors should do a better job at explaining the conditioning signal used for their guided method. Only the variant presented in the equation at line 397 is well defined. As a side note, it would have helped quite a lot numbering the various equations used throughout this paper."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces TF-score, a score-based diffusion model for time series forecasting with specific design in capturing the interdependencies between historical data and future predictions. The authors state the relationship between DDPM-based time series forecasting model and score-matching based methods and unify the framework by deriving the continuous score SDE form. Extensive experiments are conducted to show better performance against multiple baseline models.",
        "strengths": "- The proposed conditional score matching view of time series forecasting loss function is valid, and the idea of weighing errors for history and future generations is insightful.\n- The experiments are extensive, covering multiple time series forecasting baselines, multiple guidance sampling methods, and various diffusion step settings, showing efficacy of the proposed model.",
        "weaknesses": "- The similar idea of using score matching model seems to have been proposed by an earlier work [1], so the novelty and contribution of this paper may be quite limited. \n- The experiment part largely follows the experiment in [1], and the most advanced time series diffusion models are missing for comparison. To name just a few, [2][3][4][5]\n- Applying guidance sampling application on the proposed method cannot prove the superiority against baselines, since similar modifications are not applied on baseline models. It's unclear whether the proposed method can outperform baseline methods in these settings. \n\n[1] Yan, Tijin, et al. \"Scoregrad: Multivariate probabilistic time series forecasting with continuous energy-based generative models.\" https://arxiv.org/abs/2106.10121v1.  \n[2] Li, Yan, et al. \"Generative time series forecasting with diffusion, denoise, and disentanglement.\" Advances in Neural Information Processing Systems 35 (2022): 23009-23022.  \n[3] Shen, Lifeng, and James Kwok. \"Non-autoregressive conditional diffusion models for time series prediction.\" International Conference on Machine Learning. PMLR, 2023.  \n[4] Fan, Xinyao, et al. \"MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process.\" The Twelfth International Conference on Learning Representations. 2024.  \n[5] Ashok, Arjun, et al. \"TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series.\" The Twelfth International Conference on Learning Representations. 2024."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "AecVG5CXdp",
    "title": "Novel RL Approach for Efficient Elevator Group Control Systems",
    "authors": [
      "Nathan Vaartjes",
      "Vincent Francois-Lavet"
    ],
    "abstract": "The management of elevator traffic in large buildings is crucial for ensuring low passenger travel times and energy consumption. We optimize the Elevator Group Control System (EGCS) using a novel Reinforcement Learning (RL) approach. Existing methods, including heuristic-based and pattern detection algorithms, often fall short in handling the complex and stochastic nature of elevator systems. This research proposes an end-to-end RL-based approach. A custom elevator simulation environment representing the 6-elevator, 15-floor system at Vrije Universiteit Amsterdam (VU) is developed as a Markov Decision Process (MDP). \nKey innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of $\\textit{infra-steps}$ to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. Additionally, we explore various ways of adapting the discounting factor to the $\\textit{infra-step}$ formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm.",
    "keywords": [
      "Elevator Control",
      "Reinforcement Learning",
      "Applied Reinforcement Learning",
      "Partially Observable Markov Decision Process",
      "Dueling Double Deep Q-learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=AecVG5CXdp",
    "forum_url": "https://openreview.net/forum?id=AecVG5CXdp",
    "reviews": [
      {
        "rating": "1",
        "confidence": "3",
        "summary": "This paper introduces a reinforcement learning (RL) approach to optimize elevator group control systems (EGCS). \nBy incorporating infra-steps to model continuous passenger arrivals, \nthe RL-based method outperforms traditional rule-based systems in minimizing passenger wait times. \nThe study demonstrates significant potential for real-world applications in dynamic, high-traffic environments.",
        "strengths": "- This feature, which models continuous passenger arrivals, creates a learning environment for the RL agent that mirrors real-life complexities.\n- The paper's approach is designed to avoid combinatorial complexity, ensuring efficient decision-making through a well-structured action space. This design choice provides a sense of relief about the model's efficiency.\n- The simulation design is based on the actual data set.",
        "weaknesses": "- This paper is still in the stage of considering the use of reinforcement learning, and the comparison with existing methods is insufficient."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper addresses the optimization of complex elevator dispatching using a novel reinforcement learning (RL) approach. By modeling the problem as a Markov Decision Process (MDP) and introducing infra-steps to simulate continuous passenger arrivals, the authors capture the inherent uncertainties and complexities of elevator systems.\nThe paper compares fixed and variable discounting strategies, finding that the fixed approach provides greater stability and effectiveness in managing varying time intervals between actions. Additionally, the research evaluates branching and combinatorial RL agent architectures, demonstrating that the combinatorial architecture leads to more efficient decision-making.\nEmpirical results show that the proposed RL-based solution outperforms modern rule-based systems in a simulated environment with six elevators and fifteen floors. The RL agent utilizes a Dueling Double Deep Q-Learning algorithm to efficiently adapt to complex traffic patterns, significantly reducing passenger travel times. These promising findings underscore the potential for practical implementation of RL-based control in real-world elevator systems.",
        "strengths": "The paper demonstrates significant strengths through its innovative approach to elevator dispatching using a novel reinforcement learning (RL) framework. \nBy introducing infra-steps to simulate continuous passenger arrivals and formulating the problem as a Markov Decision Process (MDP), it effectively captures the complexities of elevator systems.\nThe comprehensive comparison of fixed and variable discounting strategies, along with the exploration of branching and combinatorial RL architectures, reflects methodological rigor and originality. \nThe research is presented with clarity, supported by detailed diagrams and equations, which enhance understanding. Furthermore, the study has considerable significance, \noffering a practical reduction in passenger travel times and bridging theoretical and practical applications in real-world elevator management systems.",
        "weaknesses": "1. Experimental Comparison : The paper only compares the proposed method against the classical ETD algorithm. It does not include comparisons with recent RL-based approaches, making it difficult to evaluate the method's novelty and effectiveness in the broader RL research context.\n2. Experimental Setup : The experiments are conducted using a single dataset, which limits the capacity to demonstrate the method's adaptability to diverse scenarios or environments. Testing across various conditions would better demonstrate robustness and versatility.\n3. Results Clarity : The results do not clearly show how the proposed algorithm outperforms previous methods. Adding more detailed analysis and comparison metrics would help elucidate the specific advantages.\n4. Action space : The paper mentions a significant reduction in action space design but does not offer direct comparisons with previous algorithms. Including these comparisons would strengthen the explanation and highlight improvements."
      },
      {
        "rating": "5",
        "confidence": "5",
        "summary": "The paper proposed an RL algorithm for elevator group control systems. The authors proposed a new action space to handle the combinatorial complexity of elevator dispatching. The infra-steps are proposed to handle continuous passenger arrivals. Overall it is a good application paper for RL, the writing is clear and the modification is reasonable in practice.",
        "strengths": "1.\tThe authors focus on a very practical and meaningful real-world problem, which should be encouraged in the RL community.\n2.\tThe writing is very clear. Especially, the authors explained many definitions the elevator control very well.\n3. The proposed new action space and infra-steps look simple but effective, which might benefit the empirical RL research very much. The significance is beyond the elevator group control.",
        "weaknesses": "1. The notations are sometimes confusing. For example, in equation (1) $G^\\pi$ is a conditional expectation, which is not correct. $\\pi$ is not a random variable. $\\pi$ is a function and will change the state-action distribution. A common practice is to write $G$ as a function of $\\pi$.\n\n2. The contribution of infra-step is not very clear. The empirical results have shown that the fixed discounting works better than the variable discounting."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper studies the group elevator control problem by introducing RL. \nThe paper is clearly written and easy to follow, \nhowever contribution is minor.",
        "strengths": "The topic of the paper is interesting. The paper is clearly written and easy to follow.",
        "weaknesses": "The contribution of the paper is minor in the sense that the details of the key elements proposed method are missing. For example the deep neural networks are not given. The other limitation is that the quality of the simulation model used for training the elevator group control algorithms is not clear."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 4.0,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "omzijInU1T",
    "title": "Feature Learning in Attention Mechanisms Is More Compact and Stable Than in Convolution",
    "authors": [
      "Baiyuan Chen"
    ],
    "abstract": "Robustness is a crucial attribute of machine learning models, A robust model ensures consistent performance under input corruptions, adversarial attacks, and out-of-distribution data. While the Wasserstein distance is widely used for assessing robustness by quantifying geometric discrepancies between distributions, its application to layer-wise analysis is limited since computing the Wasserstein distance usually involves dimensionality reduction, which is not suitable for models like CNNs that have layers with diverse output dimensions. To address this, we propose $\\textit{TopoLip}$, a novel metric that facilitates layer-wise robustness analysis. TopoLip enables theoretical and empirical evaluation of robustness, providing insights into how model parameters influence performance. By comparing Transformers and ResNets, we demonstrate that Transformers are more robust in both theoretical settings and experimental evaluations, particularly in handling corrupted and out-of-distribution data.",
    "keywords": [
      "Feature Learning",
      "Attention",
      "Convolution",
      "Transformer",
      "ResNet",
      "Lipschitz Continuity",
      "Wasserstein Distance",
      "Topological Data Analysis"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=omzijInU1T",
    "forum_url": "https://openreview.net/forum?id=omzijInU1T",
    "reviews": [
      {
        "rating": "6",
        "confidence": "2",
        "summary": "This paper presents a theoretical and empirical comparison of attention mechanisms and convolutional layers, focusing on their feature learning properties, including Lipschitz continuity, intrinsic dimensionality, and stability. It claims that attention mechanisms yield more stable and compact representations than convolutional layers and validates this through theoretical bounds and experiments on various architectures (Vision Transformers (ViTs) and ResNets).",
        "strengths": "* The paper provides a rigorous theoretical analysis of the feature learning characteristics of attention versus convolution",
        "weaknesses": "* Since attention and convolutional architectures can be combined in practice, it would have been useful to explore hybrid models or discuss scenarios where attention layers supplement convolutional layers, as is common in many architectures.\n\n* Training on CIFAR-10 may not yield strong performance for ViTs, as they typically require large amounts of training data. Could this limitation have impacted the results?"
      },
      {
        "rating": "1",
        "confidence": "4",
        "summary": "The authors propose a mean-field regime study of attention and convolution, and argue the attention mechanism is more robust to variations in input data distributions, enabling more stable feature learning. They observe such conditions do not actually hold in ViTs, which are more aligned with ResNets in terms of behaviour. They demonstrate lower intrisinc dimensionality in feature learning of attention mechanisms wrt convolutional ones, however these characteristics do not persist in comparisons of ViTs and ResNets.",
        "strengths": "Strengths: \n- Originality: I do not know other papers performing the same type of analyses. The authors develop or apply the theory in novel ways to draw some conclusions about the attention mechanism and convolutional layers. Therefore the work could be considered novel. \n- Quality: the theoretical developments look sound with respect to the assumptions that are made ...\n- Clarity: the paper is clearly written and easy to follow. \n- Significance: unclear, not a strength point ...",
        "weaknesses": "I start the list of weaknessess with a complementary comment to the strenghts. \n\n- Quality: ... however the experimental results are limited and undermine the usefulness of the theory. \n- Significance: the significance or importance of the paper is not particularly clear. There does not seem to be any useful consequence of the theory developed. It is not clear what point the authors are trying to make because the lower variance and Wasserstein-lipschitz condition impact on possible applications (e.g. training stability and convergence, data efficiency, robustness, generalization, differential privacy etc.) are not mentioned or discussed (and if they are, it's more to state their irrelevance for real applications.  The authors should strongly motivate the utility of their study and how it produces insights that can lead to future useful developments. For instance, the authors could consider Differentially Private (DP) SGD training ,where batch normalization (BN) is not allowed and the lipschitzness (of the per-sample gradients in this case) has a strong impact on the training accuracy. If the authors could find a relationship between their wasserstein-lipschitzness and the ones of the per-sample gradients, it could have some practical impact on DP. Similarly, it would be interesting if the authors could find at least some toy practical applications in which their findings could show their possible impact.\n\n\n- The CIFAR-10 experiments are conducted on extremely small models, it's unclear whether the findings generalise to both larger scale datasets or larger models. Furthermore, the training of transformers (and of CNNs too) is strongly regularised with augmentations and training tricks. It's not so clear whether the findings hold under such forms of regularization (see question about training details)\n- Seveal works compare different aspects of transformers robustness and generalization. Many works have found that the claimed ability of attention mechanisms to focus on the whole of an input has little to no impact on the robustness of the learnt features, outlining that training tricks like pre-training and training procedures have larger impact than the inductive biases of the convolutional/attention mechanisms  [1,2,3]\n\n[1] https://arxiv.org/abs/2207.11347\n[2] https://arxiv.org/pdf/2310.16764\n[3] https://arxiv.org/abs/2310.19909"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "The submission investigates the output variance and smoothness of non-residual (i.e., without skip connections) Attention and Convolutional layers. The authors demonstrate that attention is more compact and stable than convolution. Specifically, after presenting background on Transformers, Convolutions, and ResNets, the submission reviews recent results on the smoothness properties of attention in a mean-field framework. In Theorems 1 and 2, the input variance of the activations is derived, while Theorems 3 and 4 provide an upper bound on the Lipschitz constant of attention and convolution. Finally, experimental results on toy models (Conv and Attn), ResNet18, and a small ViT are presented, where activation variance, Wasserstein distances, and intrinsic dimensions are recorded across layers during training.",
        "strengths": "- The paper addresses an interesting question: understanding the theoretical differences in smoothness between attention-based and convolutional models, two widely used components of modern deep learning.\n- The authors acknowledge that the theoretical findings do not transfer well to models with skip connections and normalization.\n- The theoretical results appear rigorously proved.",
        "weaknesses": "In my view, this submission is not yet ready for publication at ICLR.\n\n*In terms of writing:*\n- The paper is challenging to follow. For instance, while the abstract and introduction repeatedly reference \"feature learning,\" this term is not mentioned again in the rest of the paper. Another example is the mention of masked attention (l. 361), whereas no mask is used in ViTs. The motivation for using the $W_2$ metric, in my opinion, is not well explained (l. 307).\n- The paper consists mostly of background material until page 5.\n- The persistent homology part discussed in Section 2.5 is not referenced again in the rest of the submission.\n- The related work section is very brief, with insufficient discussion of prior work. For instance, [2] also discusses the Lipschitz constant of self-attention.\n\n*Regarding the theoretical contributions:*\n- For attention, I am unsure about the novelty relative to previous work on the smoothness of attention, particularly Theorem 3.5 from Castin et al. (2024) and computations by Geshkovski et al. (2024).\n- Theorem 5 should be replaced by a concrete result concerning deep ConvNets and Transformers rather than the actual theorem followed by a discussion (which I personally find unclear).\n\n*Regarding the experimental contributions:*\n- I am not convinced that the empirical results validate the tightness of the bounds in the theorems, or that they are sufficiently related to the rest of the paper.\n- Additional experimental details in Section 4 are needed. How is activation variance computed? Over how many samples? Are these training or test samples?\n- Generally, stacking attention layers without residual connections is very poor practice, as shown in [1] (which, in my opinion, should be discussed in the paper)."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper provides an analysis on the feature map variance between basic convnets and attention only networks. The author then expands this analysis to known architectures using the same building blocks such as the ResNet18 and the Vision Transformer. The author finds that their attention networks feature embeddings have a lower variance than those produced by convolutional nets, however this does not hold when expanding the attn network to a vision transformer architecture.",
        "strengths": "- The paper provides an interesting result, showing that attention mechanisms learn feature maps with lower variance than convolutional networks.\n- The paper is mathematically correct from my understanding",
        "weaknesses": "- I am struggling to understand what is different between the attn -> vit models as well as the conv -> resnet. Is it the exclusion of residual connections and normalisation layers?\n- There is no indication of the performance of the networks in accuracy, assuming they are at convergence I would expect the accuracies to be in the low to mid 90's for cifar-10, but the paper does not indicate whether the networks reached this performance or not. In my opinion, this is an important detail as otherwise we may be comparing underfitted networks which presumably do not have the correct feature space configuration for optimal accuracy.\n- The author presents the statistics, but does little analysis into what this means. Do we want low variance or high variance in our feature space?\n- (Minor) The figures 11-17 in the appendix are interesting visualisations of the feature space, but would be more informative if we zoomed in on the blobs of features to see if there is separation within the blobs, even if it is small.\n- (Minor) To add onto the above point, visualisations of the class specific latent representations would be more interesting than the entire space to determine whether despite high or low variance of features, is our feature space semantic."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper compares variance, stability, and intrinsic dimensionality of the representations learned via attention or convolution in transformers and CNNs. The authors first study this in a theoretical setting imposing assumptions on the input and formulation of the models, deriving the variance and Lipschitz constants for each of attention and convolution. They then provide arguments about intrinsic dimensionality and how they believe different components of the models (e.g., MLP or batch norm) impact these properties. In the end, they provide experimental observations on variance and stability of two attention and two convolution models as well as ViT and ResNet.",
        "strengths": "Understanding various aspects of success of commonly-used and successful models such as CNNs and transformers is important. This paper focuses on the stability of these models, which could shed light on another factor that plays a role in the remarkable success of transformers. Theoretical results in a clear setup are helpful, despite (some inevitable) shortcomings.\n\nSome examples of interesting and insightful observations/findings provided in the paper:\n- Dependency of variance of the outputs in the attention on dimensionality, while this variance only depends on the input variance in convolution (line 285).\n- The implications of Theorem 4, showing that the Lipschitz constant of the Attn with practically-relevant values of the variables ends up being much smaller than that of Conv (line 346).\n- Potentially intriguing arguments provided in 3.4 for intrinsic dimensionality,\n- The experiments are ample for a theoretical paper (but not if the authors believe their main results come from the experiments) and the experimental setup seems suitable to investigate the main questions posed by the paper.",
        "weaknesses": "**The key weakness:** The main motivation of the paper seems lost. The paper does not motivate why a comparison on the variance of stability of transformers and CNNs is a question worth a paper, especially given the limited scope of the setup. Unless the introduction motivates this question, the paper essentially seems like it could be a section of a more thorough study, e.g., on the stability of transformers or on the comparison between CNNs and transformers. While it is not impossible that I am missing something, I do not see a way that this paper could be prepared for publication at ICLR without a major reconsideration of the content and the presentation.\n\n**Other major weaknesses:**\n\n1. Related to the key weakness above: the second paragraph of the introduction seems to jump from some general statements about transformers and CNNs (first paragraph) to questions the authors pose, without explaining why the questions are raised or why their answers matter. Same issue applies to the contributions listed in the introduction.\n\n2. The literature review seems insufficient and shallow. The literature reviewed in the Related Work section is both old, and not closely related to what the authors study. While there are many recent theoretical works, both on feature learning, and on the behavior of transformers (see, e.g. [1-3]), not many prior works related to the topic of the paper are referenced. I believe this is yet another reason the manuscript does not motivate the main questions.\n\n3. Some claims and statements seem unsubstantiated. I presume the authors do know why those statements hold, but the arguments are not communicated well. E.g., in line 295, the authors claim that they “*prove* attention can have a lower activation variance” through the analysis that follows. However, the analysis is on the Lipschitz constant, and the experiments do not seem to actually *prove* that (even in the sense of a “strong empirical evidence”, which still should not be referred to as a “proof”, especially in a paper that presents itself as a theoretical study). \n\n4. The experiments do not show convincing evidence of the theory. While some match, some do not, and the experiments in the appendix seem inconsistent. Moreover, the inconsistencies are not properly discussed.\n\n\n**Other weaknesses:**\n\na. The Preliminaries section is unnecessarily long. Batch normalization or MLP for instance, are basic concepts that could be stated in one line and the definitions could be moved to the appendix.\n\nb. How did the authors use persistence homology or TDA in the paper at all? The abstract claims the use of TDA, and there is a subsection on persistence homology, but there seems to be no analysis based on TDA.\n\nc. Some basic definitions and concepts are provided in the core theoretical results of the paper, where they do not belong. e.g., the definition of the Wasserstein distance, or Theorem 5, are too basic to be stated in detail in section 3. They should be either in the preliminaries or in the appendix.\n\nd. While intuitive, the arguments in section 3.4 lack rigor. The authors later claim (line 485) that they “prove” that attention leads to a lower intrinsic dimensionality, while I do not see anything close to a proof in section 3.4 (and, again, experiments are not a proof, especially if the evidence is not thorough and strong). \n\ne. There results in the appendix are not properly referenced, nor are they discussed in the main paper.\n\nf. The authors claim that their analysis of the stability “has nothing to do with the model performance”) line 383, while there is often a tradeoff between expressivity (hence performance) and stability.\n\ng. Arguments and explanations in lines 451-456 are not clear.\n\nh. The discussion of Figure 2 seems unclear, with statements that do not clearly reference the evidence from the experiments.\n\ni. This is a minor point, but I would not call Theorem 1 and 2 “theorem”, but rather a “proposition”, since they are direct results of the assumption that the input is a 0-mean Gaussian.\n\n\n\n**References:**\n\n[1] Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., & Vladymyrov, M. (2023, July). Transformers learn in-context by gradient descent. In International Conference on Machine Learning (pp. 35151-35174). PMLR.\n\n[2] Abbe, E., Bengio, S., Boix-Adsera, E., Littwin, E., & Susskind, J. (2024). Transformers learn through gradual rank increase. Advances in Neural Information Processing Systems, 36.\n\n[3] Radhakrishnan, A., Beaglehole, D., Pandit, P., & Belkin, M. (2024). Mechanism for feature learning in neural networks and backpropagation-free machine learning models. Science, 383(6690), 1461-1467."
      }
    ],
    "rating_avg": 3.6,
    "confidence_avg": 3.2,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "0Yfjerm9Zp",
    "title": "Enhancing LLM Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference",
    "authors": [
      "Hanqi Yan",
      "Jiazheng Li",
      "Yulan He"
    ],
    "abstract": "As large language models (LLMs) are increasingly applied to complex reasoning tasks, achieving both accurate task performance and faithful explanations becomes crucial. However, LLMs often generate unfaithful explanations, partly because they do not consistently adhere closely to the provided context. Existing approaches address this problem either rely on superficial calibration, such as decomposed Chain-of-Thought prompting, or require costly retraining to improve model faithfulness. In this work, we propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to ensure that LLM-generated rationales are logically coherent and comprehensive. These rewards are derived from a domain-specific proposal distribution, allowing for optimised sequential Monte Carlo approximations. Our evaluations across three different reasoning tasks show that this method, which allows for controllable generation during inference, improves both accuracy and faithfulness of LLMs while keeping computational costs similar to those of existing decoding techniques. This method offers a promising path towards making LLMs more reliable for reasoning tasks without sacrificing performance or efficiency.",
    "keywords": [
      "interpretability",
      "faithfulness",
      "Large language model",
      "constrained generation"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=0Yfjerm9Zp",
    "forum_url": "https://openreview.net/forum?id=0Yfjerm9Zp",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The paper proposes an approach to do faithful rationale generation in LLMs. It uses a steering-based approach to make the outputs more faithful to the reasoning of the llm in classification. The idea is to weight token logits using 2 kinds of reward models: A \"local\" one that tries to match tokens to those suggested by a domain-specific expert model and a \"lookahead\" one that does an MTCS type search and re-weights logits based on rewards from unrolled sequences.  \n\nExperiments are performed on a couple of QA type datasets, demonstrating that each method makes improvements in classification accuracy and faithfulness of rationales. Some qualitative analyses are also presented.",
        "strengths": "1. MTCS type inference is a hot topic right now, and it is indeed an important frontier for LLMs to improve on.\n2. At a surface level, experimental results seem to show large gains.",
        "weaknesses": "Section 3 is pretty badly written, it is pretty hard to get the details of the approach. Instead of invoking irrelevant sophisticated-sounding terminology like \"Feynman-Kac\" formulas it would be better to describe the method in more detail. The math especially is confusing, see below.  \n\nThe paper seems to show some positive experimental results, but I am concerned about whether we are looking at a meaningful comparison. The proposed methods rely on domain experts. Looking at table 8 in the appendix these are generally models that have been fine-tuned for the task in some way (and not just on the validation sets as the main section claims, some have access to external datasets). So it shouldn't be that surprising that a method that is given access to an expert which has more signal will do better than the backbone pre-trained model. A fair comparison would have to be with an approach that does vanilla fine-tuning of LLama or mixtral model. \n\nIn terms of novelty: The authors have not really cited relevant work in the controlled decoding space:\n\nhttps://arxiv.org/abs/2310.17022\n\nhttps://sea-snell.github.io/ILQL_site/\n\nThese works already do something more sophisticated than just token reweighting by a reward score. So what is the novel contribution here? 2 possibities:\n1. Focusing on the faithfulness problem.\n2. The \"lookahead\" idea of the reward model. I dont recall having seen this before, but it feels like a simplification of a full-blown MCTS. I would also call this a poor man's version of ILQL.\n\nSo we are just left with #1 then, unless I missed something. And this is something I consider of limited novelty (more like an application for a particular problem, though one with interesting implications from the steering perspective)."
      },
      {
        "rating": "1",
        "confidence": "3",
        "summary": "The work aims to improve the faithfulness of the LLM-generated rationales for reasoning tasks. They propose an inference-based method where an LLM is guided to generate more faithful rationales by both local and global rewards. Both rewards are provided by additional expert models which are trained on the downstream tasks. Experiments demonstrate the effectiveness of the method in achieving higher accuracy and faithfulness.",
        "strengths": "1. Faithful rationales are important for explainability and model control, which makes this work well-motivated.\n2. The proposed method is training-free (although with reliance on trained expert models), making their method portable.\n3. A comprehensive set of experiments is conducted to showcase the effectiveness of their proposed method.",
        "weaknesses": "1. The method requires the model to generate the answer prior to the rationale, which provides no guarantee that the decision is made based on the rationale. The model could still suffer from inherent biases.\n2. The method is limited to reasoning tasks with constrained answer space, limiting its generalization to more open-ended tasks.\n3. The method is poorly introduced. It would be very helpful if the authors could explain what exactly Eq.1-3 are doing in plain words."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This work proposes an inference-time method to improve the performance and faithfulness of general (instruction-tuned) large language models (LLMs). Specifically, the method uses expert models to provide fine-grained and lookahead rewards to search and reweight possible tokens or continuations proposed by the LLM. With the help of expert models trained on the target task or domain, the proposed method can improve both the accuracy and faithfulness of the zero-shot answers of two instruction-tuned models on three reasoning tasks.",
        "strengths": "The direction this paper explored has been receiving increasing interest recently: improving the quality of LLM answers at inference time without modifying the model weights directly. The proposed method improves the zero-shot accuracy and faithfulness of two strong general instruction-tuned models (Llama-3-8B and Mistral-7b-Instruct-v0.3) on three reasoning tasks. The experiment showing the benefits of going beyond local/token-level rewards and taking into account the global/lookahead reward is interesting.",
        "weaknesses": "- There needs to be more details explaining the proposed method, the motivation of each part, the equations and variables, the relation to related work, and the implementation details. Specifically:\n  - Section 3.3: how does the Feynman-Kac Formulae model inspire the faithfulness-seaking search framework? The connection is not straightforward. The notation of eq 1 is ambiguous. What does posterior P_t(st) mean exactly? How is it used in the proposed method? Also, the equation itself needs more explanations on what it is computing and why in this way.\n  - Section 3.4 (Local constraint): line 179 I find it hard to follow the motivation. How \"certain attributes can be implicitly conveyed over longer spans rather than the individual token\" is connected to \"Instead, domain-specific experts tend to demonstrate better accuracy in knowledge-rich tasks.\"? If the domain expert has better accuracy why not just use the expert to predict the scores? Why bother to use them to improve the backbone LLM? In lines 180-181, it says \"we introduce a set of classification label words C from these expert models ...\", how is C constructed? What is the motivation behind token masking?\n  - Section 3.4 (Lookahead Reweight): Equation 3 is hard to understand without proper explanations. $m$ and $x_i$ are not explained in the texts. $s_{t+l}=s_{t-1}||w_t$ is more confusing: $s_{t+l}$ has $t+l$ tokens while $s_{t-1}||w_t$ has $t$ tokens. What does equality mean here?\n- Many experimental details are missing, and important experiments are missing.\n  - Missing baselines: the performance and faithfulness of the expert models alone. If the faithfulness or accuracy of the expert models are better than the backbone LLM, why do we even need to use the expert models to improve the backbone LLM?\n  - Evaluation details: how is the original model evaluated? If it is a zero-shot evaluation. What is the exact prompt and task format used? How to extract answers from the outputs to calculate the accuracy? The backbone LLMs are state-of-the-art instruction-tuned models. However, the task performance as well as the faithfulness are quite low, so the authors need to provide more details on the evaluation.\n  - What is the choice of hyperparameter n (number of rollouts) and how is it chosen?\n- The writing of the paper could be improved for better readability. First, the paper is not properly scoped. For example, in lines 16-18, it says \"... to ensure that LLM-generated rationales are logically coherent and comprehensive.\" However, there is no result discussing the logical coherence or comprehensiveness of answers in the paper. Another example is line 108: it says \"We firstly introduce the faithfulness definition in our context,\", but there is no clear definition in section 3.2."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "They tackle the rationale generation tasks in LLMs' reasoning process. Specifically, they propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to instruct LLMs to generate good rationale. The key problem addressed is that LLMs often produce unfaithful explanations, especially when they fail to incorporate essential contextual information. \n\n+ **Local Reward**:  this component ensures coherence with the immediate context, often by using a domain-specific expert model.\n+ **Global reward**: This assesses the plausibility of the current token in relation to desirable future attributes\n\nThe search algorithm, especially for lookahead reweight seems interesting.\n\nPlease forgive me if I misunderstand something. I spent much time for reading the paper but to be honest, I am not an expert in this area. I will available on the rebuttal time for author's response and will read their response. I am also open to other reviewers' opinions.",
        "strengths": "1. The paper introduces a novel probabilistic inference method with a dual-reward mechanism, combining local and global reward. This is a very novel solution. \n2. The paper is well-written. I am not an expert in this domain but I can get their core contributions. \n3. The experiment design is clear: they design the ablation study in Section 5.1 to justify the local and global rewards for the final performance. Although I suggest authors could do better by choosing more LLMs in different model size to better support their experimental design.",
        "weaknesses": "1. There are several related works that are missing or less discussed: \n        + Evaluating Human Alignment and Model Faithfulness of LLM Rationale\n        +  On Measuring Faithfulness or Self-consistency of Natural Language Explanations\n2. Figure 2 about the distribution of domain-specific words is unclear to me. \"showing that our method can respond more actively to those domain-specific words\" Why does this part matters to the experimental results."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.25,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "zNVefjN3EP",
    "title": "OpenCarbonEval: How much $CO_2$  will your large model exhale in training process?",
    "authors": [
      "Zhaojian Yu",
      "Yinghao Wu",
      "Zhuotao Deng",
      "Xinchun Yu",
      "Yansong Tang",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Data, model and hardware are crucial components in the development of large scale machine learning models. The training of such models necessitates substantial computational resources, energy consumption, and raw materials, resulting in significant environmental implications. However, the environmental impact of these models has been largely overlooked due to a lack of assessment and analysis of their carbon footprint. In this paper, we present OpenCarbonEval, a carbon emission estimation framework to quantify the environmental implications of large scale machine learning models given their total training computations and hardware configurations.\nIn OpenCarbonEval, we conducted a comprehensive dynamic analysis of the interrelationships among data, models, and hardware throughout the model training process, aiming to forecast the carbon emission of large scale models more accurately. We validated our approach on real-world dataset, and experimental results demonstrate that OpenCarbonEval can predict energy costs and carbon emissions more accurately than previous methods. Furthermore, it can be seamlessly applied to various machine learning tasks without a precision decline. By quantifying the environmental impact of large-scale models, OpenCarbonEval promotes sustainable AI development and deployment, contributing to a more environmentally responsible future for the AI community.",
    "keywords": [
      "Large-scale model",
      "Carbon footprint",
      "Sustainable AI"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=zNVefjN3EP",
    "forum_url": "https://openreview.net/forum?id=zNVefjN3EP",
    "reviews": [
      {
        "rating": "3",
        "confidence": "2",
        "summary": "This paper proposes a tool named OpenCarbonEval to estimate energy consumption and carbon emissions during the training process of large ML models. The authors present a new formulation for estimating the training carbon footprint of various models and evaluate the effectiveness of their approach in comparison to related work.",
        "strengths": "- Timely problem\n- Good motivation",
        "weaknesses": "- Novelty\n- Soundness\n- Insufficient hardware details"
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The paper presents OpenCarbonEval, an innovative approach for estimating the carbon footprint of training large ML models, with claims to improve prior models by incorporating hardware-specifications, embodied and operational carbon estimation, and dynamic power consumption. It introduces an α parameter to model dynamic power consumption and introduces an open-source dataset of 110 models across multiple large-scale ML tasks for validation of the proposed approach.",
        "strengths": "1.\tRelevant Topic: The environmental impact of large ML models is an important concern, and OpenCarbonEval’s focus on a general framework for carbon footprint estimation. OpenCarbonEval showcases improved error rate in comparison to LLMCarbon across various large-scale ML models.\n\n2.\tMulti-Domain Scope: The method’s attempt to generalize across model types, hardware types, and tasks, potentially making it more versatile than existing carbon estimation among the estimation methods. \n\n3.\tDataset Creation: OpenCarbonEval contributes an open resource by curating a dataset of carbon emissions containing 110 records.",
        "weaknesses": "1.\tInadequate Justification for the α Parameter: The derivation of the α parameter lacks theoretical depth, as the paper does not substantiate the choice of logarithmic modeling. Providing empirical or theoretical evidence for using f(t)=ln(1+αt) would strengthen its validity; a comparison with alternative functions could clarify this choice.\n2.\tLimited Model Generalization: OpenCarbonEval does not convincingly show its ability to generalize across diverse ML tasks and architectures. The adaptability of the α parameter remains unclear, particularly for models outside the initial dataset. Additional validation across a wider range of model types by extending Table 1, 2 will reinforce its versatility. Detailed results for the validation of the method is required.\n3.\tLack of Explanation for Equations: The paper lacks the connection between equation (2) and equation (3), and also lacks the explanation of how the Lcomputation is used to estimate the energy consumption E. Moreover, the Clifelong needs to be elaborated in terms of how it is attained. \n4.\tComparison with results for LLMCarbon: Can the authors present the analysis of same models and hardware combinations presented in Table 4 in the LLMCarbon paper?\n5.\tJustification or Citation for Assumption: The assumption of 1-year GPU lifespan for the embodied carbon estimation lacks justification or citation from a reliable source. \n6.\tOverlooked Factors in Operational Carbon Calculation: OpenCarbonEval does not account for essential factors like Power Usage Effectiveness (PUE) in data centers, leading to potential underestimations of emissions. Including PUE in calculations would create a more realistic operational carbon estimate.\n7.\tSimplistic Treatment of Training Dynamics: OpenCarbonEval applies Little’s Law simplistically, assuming a steady state in training dynamics, which oversimplifies the training process. More practical grounding, perhaps through empirical evidence, would enhance applicability in ML contexts. LLMCarbon addresses this by using detailed hardware efficiency and optimal parallelism settings, providing a robust framework for accurately modeling training dynamics.\n8.\tEmbodied Carbon Calculation: OpenCarbonEval’s approach to embodied carbon appears oversimplified, lacking in-depth parameters that affect emissions, such as hardware-specific manufacturing and lifetime estimates. Moreover, the Clifelong needs to be elaborated in terms of how it is attained."
      },
      {
        "rating": "5",
        "confidence": "1",
        "summary": "The training of machine learning (ML) models significantly contributes to global carbon emissions. This paper introduces OpenCarbonEval, an advanced estimation tool designed to quantify the carbon impact of large-scale ML models based on their total training computations and hardware configurations. The tool's accuracy is validated using real-world datasets, and experimental results demonstrate that OpenCarbonEval provides more precise predictions of energy consumption and carbon emissions than previous approaches.",
        "strengths": "1. The paper works on an important topic.\n2. The paper identifies the shortcoming of preivous works (Faiz et al., 2023): the polynomial approximation for the system efficiency and hardware utiliation estimation is not accurate.",
        "weaknesses": "1. **Simplified yet more accurate formulation???**: The functions presented in Equations (3) through (7) lack clarity in their intended function and accuracy. While polynomial approximations may lack precision, Equation (7) is simplified even further than LLMCarbon, containing only a single parameter compared to the multi-parameter nature of polynomial approximations. Why is this single-parameter approach purported to yield higher accuracy? The authors are encouraged to offer detailed explanations or empirical validation demonstrating how and why Equation (7) leads to improved accuracy over traditional polynomial approximations.\n\n2. **Consideration of GPU count and parallelism settings**: The paper does not discuss varying GPU counts in training configurations, appearing to assume a single-GPU setup. It also does not address different training parallelism types, such as data, tensor, pipeline, or expert parallelism, all of which may affect results depending on GPU count. Without incorporating these parallelism factors, it is unclear how OpenCarbonEval achieves greater accuracy. How does this work account for different parallelism strategies, and are there empirical results confirming its accuracy across these configurations? Additionally, Figure 4 lacks context: how many GPUs are represented, why do some GPUs exhibit smaller variance, and how many GPUs are used for training in Tables 1 and 2?\n\n3. **Lack of model architecture information**: The study appears to consider only the number of parameters in ML models, without accounting for architecture specifics. While scaling laws suggest that architecture does not impact model accuracy, it significantly affects training throughput across various architectures (see Megatron paper: https://parsa.epfl.ch/course-info/cs723/papers/Megatron.pdf). The authors should provide empirical evidence to demonstrate that model architecture does not impact the carbon footprint of training.\n\n4. **Dataset limitations**: The dataset used is limited and lacks comprehensive real-world data. Among the 863 entries in the provided table (https://epochai.org/data/notable-ai-models?view=table), only 176 entries include training times, 158 provide GPU counts, and only 31 report hardware utilization, leaving most entries without training times or hardware utilization data. With such limited information, how is \\( f(x) \\) in Equation (5) trained and validated? Furthermore, 603 of the 863 entries are classified as \"likely,\" \"speculative,\" or \"no confidence.\" Does OpenCarbonEval rely on these uncertain data points for validation while claiming higher accuracy? The authors should discuss the limitations associated with the dataset quality and address the impact on the reliability of their conclusions."
      }
    ],
    "rating_avg": 3.6666666666666665,
    "confidence_avg": 2.3333333333333335,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "CFKZKjrQ5r",
    "title": "FCoReBench: Can Large Language Models Solve Challenging First-Order Combinatorial Reasoning Problems?",
    "authors": [
      "Chinmay Mittal",
      "Krishna Kartik",
      "Parag Singla",
      "Mausam ."
    ],
    "abstract": "Can the large language models (LLMs) solve challenging first-order combinatorial\nreasoning problems such as graph coloring, knapsack, and cryptarithmetic? By\nfirst-order, we mean these problems can be instantiated with potentially an infinite\nnumber of problem instances of varying sizes. They are also challenging being\nNP-hard and requiring several reasoning steps to reach a solution. While existing\nwork has focused on coming up with datasets with hard benchmarks, there is\nlimited work which exploits the first-order nature of the problem structure. To\naddress this challenge, we present FCoReBench, a dataset of 40 such challenging\nproblems, along with scripts to generate problem instances of varying sizes and\nautomatically verify and generate their solutions. We first observe that LLMs, even\nwhen aided by symbolic solvers, perform rather poorly on our dataset, being unable\nto leverage the underlying structure of these problems. We specifically observe\na drop in performance with increasing problem size. In response, we propose a\nnew approach, SymPro-LM, which combines LLMs with both symbolic solvers\nand program interpreters, along with feedback from a few solved examples, to\nachieve huge performance gains. Our proposed approach is robust to changes in the\nproblem size, and has the unique characteristic of not requiring any LLM call during\ninference time, unlike earlier approaches. As an additional experiment, we also\ndemonstrate SymPro-LM’s effectiveness on other logical reasoning benchmarks.",
    "keywords": [
      "llms",
      "logical-reasoning",
      "first-order-reasoning",
      "neuro-symbolic"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=CFKZKjrQ5r",
    "forum_url": "https://openreview.net/forum?id=CFKZKjrQ5r",
    "reviews": [
      {
        "rating": "3",
        "confidence": "2",
        "summary": "This paper focuses on the problem-solving ability of LLM on first-order combinatorial problems in natural language form, arguing that no existing benchmark could reveal this challange properly. To stress the significance of this issue, this paper proposes a new benchmark, FCoReBench, which covers 40 challanging problems in varying sizes and correspounding solutions. In responding to the poor performance of current LLMs on FCoReBench, this paper further proposes a new framework, SymPro-LM, to push forward the potential capacity of language models by combining symbolic solvers, program interpreters and the LM backbone. The experimental results show a significant improvement in various aspects, indicating the valuable attempt of assembling different augmented modules.",
        "strengths": "- The problems covered in FCoReBench are relatively comprehensive, highlighting a valuable research direction. It would be interesting to see more generalized problems to be addressed once VLM are taken into consideration.\n- A corresponding responce framework has been developed for the issue proposed, and the experimental results are promising.\n- The experimental section in section 7 features thorough verification and comprehensive chart presentations.\n- The discussion in section 8 is insightful. It would be benificial to list the problems in each situation in the appendix, and even better, to illustrated them with diagrams in the main text. This would help to elucidate the dataset's relevance to the central issue.",
        "weaknesses": "- The construction part of the dataset issue in Section 4 requires manual labor, which is quite labor-intensive. Could it be automated using LLM?\n- The current agent can only solve first-order logic. Higher-order logic requires individual generation, which is resource-intensive and difficult to scale.\n- There is a lack of innovation in the proposed framework SymPro-LM, which merely combines existing symbolic solvers and program generation. It would be better to consider a more specific design.\n\nWriting aspects:\n- There are issues with the section layout and organization; the section titles are inconsistent and not uniformly formatted (e.g. section 5 and 5.1, section 7 and 7.1). The table layout on page 7 is also peculiar.\n- The overall language used in writing is subpar, being rather colloquial and informal. E.g.:\n  - In Section 3, as a problem definition, there should not be such an emphasis on the subject \"We.\" The problem should be described objectively and rigorously from a third-party perspective.\n  - In Section 4, the term \"the author\" should be used less frequently to avoid potential privacy issues. Instead, use \"agent\" or \"process\" to emphasize actions rather than the actors, which would be more formal. If necessary, flowcharts can also be used to represent the selection, polishing, and construction processes, which would greatly assist readers in understanding the overall procedures.\n- This paper primarily focuses on the benchmark, as emphasized in the title; thus, the experimental section should mainly focus on verifying the performance of the benchmark in various aspects. The current writing approach is centered around SymPro-LM. If this focus is to be maintained, the emphasis of the entire article should be placed on SymPro-LM."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "Introduces FCoReBench which consists of generators and evaluators for 40 combinatorial optimization problems such as sudoku, graph coloring etc. Evaluates existing prompting approaches and LLM augmentation approaches on the dataset. Proposes a new framework SymPro-LLM which when given a problem, output a program that converts the problem to symbolic representation, which is then passed to a symbolic solver to get the solution.",
        "strengths": "The proposed SymPro-LLM can work with different instances from the same first order combinatorial optimization problem without the need to re-evaluate using LLMs.\n\nThe proposed dataset is difficult for existing LLMs. The instances are based on combinatorial reasoning problems, which are mostly NP-Hard problems.\n\nThe proposed dataset is lifted such that unlimited new instances can be generated.",
        "weaknesses": "While I find the proposed approach of using LLM to output program to formulate models interesting, I am not convinced the experimentations conducted provide enough insight to LLM reasoning abilities. From the examples shown in figure 2, NL(C), NL(X), NL(Y) seems to be pseudo code for formulating the problem. The task of the LLM therefore becomes translating the pseudo code to python, which does not require the same level of reasoning as solving the problems.\n\nThe paper does not evaluate enough existing models for the new proposed benchmark dataset. For example, the state-of-the-art GPT-4o and GPT-o1 are not evaluated. The paper also include limited analysis of why the existing approaches fail on the proposed dataset. \n\nThe writing and presentation require more clarity and focus. For example, section 7 presents results across different LLM models, different frameworks/styles of prompting, different datasets/problem classes, and different experimental setups. It is unclear to me what the key takeaways from these results are."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "The paper introduces FCoReBench, a benchmark designed to evaluate the capabilities of LLMs in solving first-order combinatorial reasoning problems. The benchmarks include NP-hard problem instances like graph coloring and knapsack, with varying instance sizes. Current LLMs struggle with these tasks, particularly as the problem size increases. To address this limitation, this paper proposes SymPro-LM, a hybrid approach that combines LLMs with symbolic solvers, enhancing performance by leveraging the strengths of both methods. \n\nThe proposed approach achieved a 21.61% improvement over few-shot prompting, a 3.52% improvement over Program-aided Language models (PAL), and a 16.83% enhancement over Logic-LM. Additionally, incorporating feedback from solved examples boosts SymPro-LM's performance by 21.02% after four rounds, compared to 12.5% for PAL. SymPro-LM also excels on three non-first-order logical reasoning benchmarks, outperforming existing baselines on two datasets and remaining competitive on the third, highlighting the effectiveness of integrating LLMs with symbolic solvers.",
        "strengths": "Using LLM to solve logic puzzles and combinatorial problems is a very important and interesting direction. This paper contributes a well-established dataset for this field which can be valuable to the research community. The paper also proposes a framework that combines extant solvers such as Z3 with LLMs. The experiment results seem convincing and promising.",
        "weaknesses": "1. The name \"first order\" is a bit confusing. Does it mean it is related to first-order logic? If so, it would be great to elaborate on this connection. Otherwise, a more detailed definition should be provided. It is not clear from the paper what the difference is between first-order problems and second-order ones.\n\n2. Whether the level of contribution of this paper meets the standard of ICLR is questionable. It is not clear whether this paper proposed novel methodologies. The main contribution according to the paper seems to be the establishment of a dataset.\n\nMinor: The fonts in Figure 4 should be larger."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces a problem set designed to assess LLMs' ability to solve first-order combinatorial reasoning problems. It argues that current symbolic-solver-aided LLMs perform poorly on this problem set and proposes a novel approach that combines a symbolic solver with a program interpreter to improve reasoning capabilities, demonstrating superior performance on the problems.",
        "strengths": "The paper aims to address an important problem. The proposed approach is conceptually sound, and the experimental results indicate promising improvements in the reasoning capabilities of LLMs when using the technique.",
        "weaknesses": "This paper has several critical issues that require the authors' attention:\n\n1. Misalignment Between Title and Content: While the title suggests a focus on the proposed problem set, the main body primarily discusses the technique, SymPro-LM. After reviewing the entire paper, it appears more as a technique paper rather than a benchmark paper. I suggest revising the title and reorganizing the structure to more accurately reflect its focus on methodology.\n\n2. Lack of Clarity on Incremental Contributions of the Problem Set: Although the problem set seems useful, the paper does not clearly articulate its unique contribution. Existing symbolic-solver-aided LLM approaches have already addressed similar reasoning problems, and some may have been tested on benchmarks containing first-order combinatorial reasoning problems. It is essential to compare the proposed problem set with these existing benchmarks, highlighting overlaps and differences. However, this paper provides limited detail on this aspect.\n\n3. Scope Restriction and Generalizability of the Technique: While the paper narrows its focus to first-order combinatorial reasoning problems, conceptually, the proposed technique has broader applicability across various reasoning tasks. Given the absence of any domain-specific adaptations, I recommend either expanding the paper’s scope and conducting a more comprehensive evaluation across diverse reasoning problems, or explaining the reason of the scope restriction.\n\n4. Use of an Outdated LLM in Evaluation: The LLM used in the evaluations appears a bit outdated. I suggest incorporating recent models, such as GPT-4o and o1, to provide a more relevant assessment.\n\n5. Unclear Criteria for Problem Selection in the Problem Set: The criteria for including specific problems in the problem set are not well-defined. For example, while the paper includes problems from the industry track of SAT competitions, it does not explain the exclusion of others (e.g., the main track). Furthermore, recent SAT competitions no longer feature an industry track, making the rationale for this selection unclear."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.0,
    "decision": "Reject",
    "meta_review": "This paper introduces a new benchmark, FCoReBench, consisting of 40 combinatorial optimization problems whose constraints, inputs, outputs, and examples are all stated in natural languages. Additionally, this paper also proposes a new framework, SymPro-LM, which outperforms existing prompting methods like few-shot prompting and program-aided prompting.  Introducing new datasets and prompting frameworks to improve the reasoning capability of LLMs are valuable contributions. However, several important concerns are not addressed properly. For instance, to what extent the 40 combinatorial problems are new compared to existing reasoning tasks? Although the problem is stated in natural language, the form is still rigid -- clear separations regarding constraints, I/O instructions, and examples have to be specified, making it not far from a piece of pseudo-code. Furthermore, the key motivation is not very clear; on one hand, it suggests the contribution of benchmark, the novelty of which is a bit questionable; on the other hand, the authors want to show the new framework, SymPro-LM, significantly outperforms existing techniques on a newly crafted benchmark. A more systematic comparison of existing benchmarks where baseline approaches were evaluated would be expected.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "O6znYvxC1U",
    "title": "Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks",
    "authors": [
      "Ouns El Harzli",
      "Bernardo Cuenca Grau"
    ],
    "abstract": "We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. \n   Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime.\n   From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate.",
    "keywords": [
      "infinite bayesian neural networks",
      "kernel theory",
      "random matrix theory"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=O6znYvxC1U",
    "forum_url": "https://openreview.net/forum?id=O6znYvxC1U",
    "reviews": [
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper studies bayesian neural networks in the linear ($P/N = \\text{constant}$ for data $P$ and width $N$) regime and sub-linear ($P/(N N_0) = \\text{constant}$) regime. The authors operate under a spectral universality assumption, which treats the eigenfunctions of the limiting kernel as random with a covariance determined by the posterior kernel. The authors argue that this spectral universality assumption is logically equivalent to a kernel renormalization theory which was derived for deep linear networks. This kernel renormalization is a scale shift in the kernel by a variable $u$ which needs to be solved for self-consistently. In the sublinear regime, the kernels are rank-deficient which the authors acknowledge by allowing for a singular spectrum and utilizing the pseudo-inverse.",
        "strengths": "This paper studies the important problem of characterizing feature learning in nonlinear Bayesian neural networks and provides an original idea to apply a spectral universality idea to characterize feature learning. It further aims to justify some of the recent applications of kernel renormalization theory to nonlinear networks.  Showing that the spectral universality assumption implies and is implied by the kernel renormalization picture is an interesting contribution. The authors also provide a few experiments to support their claims.",
        "weaknesses": "However, the spectral universality assumption is not proven directly and the distribution of kernel eigenfunctions in either the proportional regime or the sublinear regime has not been characterized outside of the spectral universality assumption. The experiments are somewhat limited. I have a number of questions below, which if addressed could lead me to increase my score."
      },
      {
        "rating": "8",
        "confidence": "2",
        "summary": "The paper gives integral formulas describing the outputs of BNNs in the linear and sublinear width regimes.",
        "strengths": "The theoretical contributions are extremely strong, especially around the integral formulas describing the outputs of BNNs in the linear and sublinear width regimes.",
        "weaknesses": "The primary weakness is around the experimental results.  These are restricted to MLPs on very simple datasets.  Though I would be happy to consider an argument that the results should generalise and/or that it would be prohibitively difficult to get results outside this setting.\n\nThe experimental results are presented very poorly.  For instance:\n* The plots do not have labelled x and y-axes.\n* The legends are very confusing. As an example: \"on the left (respectively, on the right).\"\n\nThere are also numerous prior works around deep kernel processes and machines that would be worth discussing in the related work:\n* https://arxiv.org/abs/2010.01590\n* https://arxiv.org/abs/2108.13097\n\nwhile this work uses a very different theoretical approach, it ultimately addresses similar conceptual issues."
      },
      {
        "rating": "6",
        "confidence": "4",
        "summary": "The authors propose combining ideas from random matrix theory to evaluate limits of infinite width Bayesian neural networks (such as the classic work by Neal (1996)) when the number of observations also goes to infinity as well as the number of input dimensions. The paper provides novel claims of the behavior of the limit in sublinear regimes.",
        "strengths": "Using Marchenko-Pastur to evaluate this type of limits, seems like a good idea. Combining these approaches with Mercer's theorem seems valuable. The ideas of random matrix theory in general seem underutilized in the ML community.",
        "weaknesses": "1. It is not immediately clear how the integrals in Theorem 3.4 can be explicitly evaluated. \n2. Notation for $\\Phi^*$ in Theorem 3.4 was never defined.\n3. Between lines 300 and 303, the authors claim that for __small__ datasets the computations would not be too computationally intensive. However, in footnote 3 (p. 6), they claim that the limit can be approximated using __large__ objects. It is not completely clear what has to be large or small for the evaluations to be feasible. \n4. Section 4 appears to be incomplete. It is not evident from the writing which of the figures correspond to the two examples they mention at the beginning of the section. \n5. In the first experiment they mention, with the \"linear teacher\", the authors do not specify how the $\\beta$ is defined."
      }
    ],
    "rating_avg": 6.333333333333333,
    "confidence_avg": 3.0,
    "decision": "Accept (Poster)",
    "meta_review": "This paper investigates Bayesian neural networks (BNNs) in certain asymptotic limits, drawing connections between kernel-theoretic approaches and statistical mechanics. It presents novel integral formulas for BNN predictors in linear and sublinear width regimes and extends renormalization theory, originally developed for linear networks, to nonlinear BNNs. Reviewers point to weaknesses in the  empirical validation, noting that the experiments are limited to simple datasets and lack detailed explanation. Furthermore, the core spectral universality assumption remains unproven, and the practical applicability of the derived integral formulas is not entirely clear due to computational concerns. Despite these limitations, the theoretical contributions are strong and original. The paper attacks the crucial problem of characterizing feature learning in nonlinear BNNs and offers a novel approach by combining spectral universality with kernel renormalization. The derivation of integral formulas for BNN predictors in different width regimes and the extension of renormalization theory represent useful advancements in the theoretical understanding of BNNs, and therefore I recommend publication.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "xN6z16agjE",
    "title": "Evaluating word representation for hypernymy relation: with focus on Arabic",
    "authors": [
      "Randah Alharbi",
      "Husni A. Al-Muhtaseb"
    ],
    "abstract": "Hypernymy relation is one of the fundamental relations for many natural language processing and information extraction tasks. A key component of the performance of any hypernymy-related task is word representation. Traditional word embeddings capture word similarity but fall short of representing more complex lexical-semantic relationships between terms, such as hypernymy. To overcome this, recent studies have proposed hypernymy-specific representations. In this study, we conduct an evaluation of several types of word representations to determine the most effective approach for modeling hypernymy relationships in Arabic. We use an Arabic training corpus and several datasets to assess traditional embedding, hypernymy-specific embedding, and contextual embedding across several hypernymy-related tasks, including hypernymy detection. The results indicate that different embeddings have different effects on the performance. Moreover, the performance is affected by the selected datasets. This highlights that there is a need for further research to develop more robust word representation and benchmark datasets.",
    "keywords": [
      "Word representation",
      "hypernymy relation",
      "hypernymy specific embedding",
      "hypernymy detection."
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=xN6z16agjE",
    "forum_url": "https://openreview.net/forum?id=xN6z16agjE",
    "reviews": [
      {
        "rating": "3",
        "confidence": "3",
        "summary": "Authors try to evaluate different algorithms, which create hypernymy relation representations in Arabic. They select AraBERT corpus as a base for training all embedding models and train several models on this data. As a baseline for contextual embeddings, BERT is used, while for classic embeddings GloVe is used. For hypernymy-specific embedding LEAR, GLEN, Princare and Poincare Glove is used. After that, a simple feedforward models are trained for all embeddings for three tasks: hypernymy detection, hypernymy directionality detection and semantic relation classification. Results show, that Poincare GloVe performs best on hypernomy detection and hypernomy directionality detection tasks. In semantic relation classification tasks Poincare GloVe performs worse. Overall, there is no best representation for all tasks.",
        "strengths": "1. The goal of the paper is easy to understand, as it provides valuable insight into which representations are best for hypernym-based tasks (none are best overall)\n\n2. Experiment design makes sense and is mostly without issues. There is a minor issue stemming from the limited resources available to the authors of the paper, but I will touch upon them in the weaknesses part of the review.",
        "weaknesses": "1. Authors are very constrained in resources, having to resort to halving the size of the training dataset for some of the algorithms. This raises questions to the validity of the collected information, since Poincare GloVe, the best algorithm in Hypernymy Directionality and Hypernymy Detection tasks, has seen only half as many data samples, which can possibly make the results non-representative. However, due to the simplicity of the Poincare GloVe, most likely it won’t impact the results as much, thus, making this just a minor issue.\n\n2. The quality of the text's presentation is poor; it contains numerous typos and improperly formatted tables. Table 7 has incorrectly formatted items in header, Table 8 has incorrectly formatted dataset names, in table 7 the highest F1 score for ASRD dataset is incorrectly attributed to 100D Poincare GloVe, which has the score of 0.88 instead of Poincare Embedding, which have the score of 0.89. On the line 070 BERT has no citation available, on the line 220 the sentence starts from lowercase, on the line 291 the word Assess is incorrectly capitalized, line 480 is cut in half, etc. Both Introduction and Related Work sections are hard to read, since they are written in a big wall of text instead of separate paragraphs on groups of algorithms. Some of the citation years are in brackets (lines 065-068), some are not (line 079)."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper focuses on evaluating and improving word vector representations specialized for hypernym relations in Arabic. Your research captures the gaps in hypernym aspects of performance by conducting multiple sets of experiments on different datasets, and this aspect of the research is important for the NLP task.",
        "strengths": "1. **Relevance**: Focusing on hypernym in Arabic is timely and relevant, and it addresses a less explored area of NLP.\n2. **Experiments**: The paper presents a comprehensive experimental evaluation of multiple word representation techniques, demonstrating a solid methodological framework.\n3. **New findings**: The findings presented in the paper provide new insights into how Arabic word embeddings can be enhanced to better detect hypernym.",
        "weaknesses": "1. **Literature Review**: Although the paper discusses related work, a more comprehensive literature review would have positioned the paper's contribution more effectively. I suggest the authors report precision and recall specifically for elements that have both spatial and logical relationships, compared to those with only one type of relationship.\n2. **Clarity**: Some sections may lack clarity, particularly in explaining the significance of the research methodology and findings. \n3. **Results Interpretation**: The results section may need to discuss the significance of the findings in more depth, especially in the context of existing models."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper addresses the challenge of modeling hypernymy relations specifically focusing on Arabic. The authors evaluate various word representation methods to determine the most effective for Arabic hypernymy tasks. They compare traditional embeddings, hypernymy-specific embeddings, and contextual embeddings using an Arabic corpus and multiple datasets to assess their impact on tasks such as hypernymy detection.",
        "strengths": "1. Word representations for hypernymy are essential for a variety of tasks in NLP and information extraction.\n\n2. By concentrating on the Arabic language, the paper contributes to a less explored area, providing insights for non-English NLP research.\n\n3. The research conducts an evaluation of multiple types of embeddings, including traditional, hypernymy-specific and contextual embeddings.",
        "weaknesses": "1. The paper primarily focuses on evaluating existing word representations rather than introducing a novel approach or method for hypernymy modeling. The novelty is very limited.\n\n2. The written quality of this paper is poor. The authors should carefully revise this paper for better presentations. For example, the citation format is incorrect.\n\n3. The paper seems to provide an evaluation of performance effects without a deep analysis of why certain embeddings perform better or worse in specific contexts or tasks.\n\n4. The impact of this work in the ICLR community is limited. Maybe an NLP workshop on the Arabic language is more suitable."
      }
    ],
    "rating_avg": 3.0,
    "confidence_avg": 3.3333333333333335,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "wVTJRnZ11Z",
    "title": "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach",
    "authors": [
      "Qian Chen",
      "Lei Li",
      "Qian Li",
      "Jianghua Wu",
      "Akang Wang",
      "Ruoyu Sun",
      "Xiaodong Luo",
      "Tsung-Hui Chang",
      "Qingjiang Shi"
    ],
    "abstract": "A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. \nHowever, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivalence and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables.\nTo address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance.",
    "keywords": [
      "integer linear programming",
      "symmetry",
      "machine learning",
      "graph neural networks"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=wVTJRnZ11Z",
    "forum_url": "https://openreview.net/forum?id=wVTJRnZ11Z",
    "reviews": [
      {
        "rating": "5",
        "confidence": "2",
        "summary": "The authors study the problem of solving Integer Linear Programs (ILPs) with symmetry among variables. They first show that if a permutation from the set of all variable permutations is a formulation symmetry of the ILP, then under an assumption of permutation equivalence and invariance for the GNN, the network cannot predict the optimal solution of the ILP.\n\nTo address this, they propose a feature augmentation algorithm that assigns unique augmented features to each orbit, sampling a distinct feature value within an orbit without replacement. They compare their methods against previously proposed augmentation schemes empirically and based on some principles for three ILP benchmark problems.",
        "strengths": "Their method appears to perform better in terms of their proposed metric than existing methods on certain tasks. They also introduce the problem clearly, making it easily understandable for someone outside the field, while establishing a good motivation through negative results about GNNs and formulation symmetries.",
        "weaknesses": "They don't discuss any limitation of their orbit-based augmentation, making their method's application scope appear narrow restricting to ILPs with formulation symmetry. \n\nAdditionally, the approach relies on detecting symmetry groups and orbits, which as they note may be computationally expensive.  It would also be interesting to see how their method performs for different evaluation metrics (maybe beyond $\\ell_1$ distances). \n\nAs this area is new to me, their contributions do not seem sufficiently novel in terms of the algorithm, and the experiments provided also seem limited and hence I recommend a reject but with a confidence score of 2."
      },
      {
        "rating": "6",
        "confidence": "5",
        "summary": "This paper improves a weakness of graph network approaches for predicting the solutions of integer linear programs (ILPs) with symmetric variables. Because graph networks are permutation equivariant, they cannot distinguish between exchangeable variables in the ILPs (or more concretely, variables such that, when permuted, the cost constraint is still satisfied and the objective is unchanged). The authors use feature augmentation to break the symmetries between these equivalent variables, specifically emphasizing distinguishability, “isomorphic consistency”, and “augmentation parsimony”. Past works have used feature augmentation to break these symmetries, but without adhering to these principles. They demonstrate the effectiveness of their techniques relative to alternatives on solving synthetic ILPs, with training data generated by a classical solver.",
        "strengths": "This paper addresses a meaningful issue (symmetry-breaking in integer linear programs) in a new way (using inputs to GNNs that break only the required orbit symmetries, in accordance with the three defined desiderata). Although I believe these two pieces are individually not new (see “weaknesses”), their combination is. The “Orbit+” approach is also a novel way of enhancing “augmentation parsimony”. The paper is generally clear, and the writing style and notations are both enjoyable to read. Their experimental results on the chosen tasks beat the chosen ML baselines. Although I am not convinced by the necessity of isomorphic consistency for most applications, in which a logical loss function can be chosen (which is unaffected by swapping equivalent nodes), the use of SymILO to enforce it by adjusting training labels is also new.",
        "weaknesses": "I think the biggest weakness of this paper is that, in short, many of its central ideas have already been introduced and (in some cases) thoroughly explored in papers of which the authors seem unaware. (This is understandable, given that they do not appear in the ILP literature and use different terminology to describe the problem, but nonetheless they exist — I hope the authors may be inspired by the perspectives of these papers, and can also articulate the novelty of their work relative to them.) This line of work (see the references below; although not the earliest, [2] or [4] may be the most accessible starting points) goes by the name “symmetry-breaking”, and articulates the precise issue that the authors encounter for ILPs, but in a much more general way, for all group equivariant networks. The principles of distinguishability and augmentation parsimony are explored under different names, e.g. in [3]. There is a related line of work on breaking symmetries of sets, termed “multiset equivariance” [5]. Works such as these and [4] make clear subtleties of the problem that aren’t discussed in this paper, such as the difference between the graph automorphism group and the node orbits, and articulate methods for addressing the equivalent nodes of ILPs in ways that subsume the method presented here.\n\nI believe that orbit-equivariant graph neural networks [6] are also a slightly more fleshed out version of the “Orbit” approach. \n\nAs noted under questions, I also find the discussion of isomorphic consistency confusing, as it is (assuming I understand correctly) not well-motivated under orbit-invariant loss functions, and importantly, not necessarily even possible to achieve. Is “relaxed equivariance” [2] more suitable? \n\nFinally, as also noted under questions, there seem to be weaknesses with the experiments — namely, the choice of loss function, and the premise/lack of comparison to non-ML baselines. \n\nReferences:\n1. Smidt, T. E., Geiger, M., and Miller, B. K. Finding symmetry breaking order parameters with euclidean neural networks. Phys. Rev. Research, 3: L012002, Jan 2021. doi: 10.1103/PhysRevResearch\n2. Kaba, S.-O. and Ravanbakhsh, S. Symmetry breaking and equivariant neural networks. In Symmetry and Geometry in Neural Representations Workshop, NeurIPS, 2023.\n3. Xie, Y. and Smidt, T. Equivariant symmetry breaking sets. TMLR 2024.\n4. Hannah Lawrence, Vasco Portilheiro, Yan Zhang, and Sekou-Oumar Kaba. Improving equivariant networks with probabilistic symmetry breaking. In ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling, 2024.\n5. Zhang, Y., Zhang, D. W., Lacoste-Julien, S., Burghouts, G. J., and Snoek, C. G. M. Multiset-equivariant set prediction with approximate implicit differentiation. In International Conference on Learning Representations, 2022.\n6. Morris, M., Grau, B. C., & Horrocks, I. (2024). Orbit-equivariant graph neural networks. ICLR 2024."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes to augment the vertice features in ILP graph representation based on orbit of the symmetry group. Some theory is provided and numerical results are conducted with the comparison with the random feature technique (Chen et al. 2022) and the positional ID technique (Han et al. 2023).",
        "strengths": "1. The paper is in general well-written and is easy to follow.\n\n2. The idea of augmenting features based on orbits is reasonable. I agree with the authors that only vertices in the same orbit need to be separated with augmented features, and the cardinality of the augmented feature space is much smaller than $n!$ if the number of orbits is much larger than one.\n\n3. The reported numerical results look better than baseline methods.",
        "weaknesses": "1. There is no comparison with conventional methods based on symmetry group and orbits, such as Ostrowski et al. (2011) cited by the authors. In addition to Ostrowski et al. (2011), there should actually be a much richer literature in this direction.\n\n2. There is no report on the cost of computing the symmetry group. I expect to see a trade-off between the size of the symmetry group and the improvement from \"no augmentation\"."
      },
      {
        "rating": "8",
        "confidence": "2",
        "summary": "The authors propose a novel feature augmentation method for ILP's with symmetries that are described by bipartite graphs for solving with GNN's. The augmentations obey some important symmetry properties but are also more parsimonious than existing methods. Empirical results suggest that these augmentations help the GNN's break the symmetry better than competing methods.",
        "strengths": "The main idea in Section 4.2 is explained well. The numerical results are decent and convey the practical benefits of the method. The introduction is also nicely written.",
        "weaknesses": "While the numerical results do suggest that the method helps break symmetries better than others, it is hard to tell if this makes a difference in the end result (objective of rounded solution). I think such a comparison should be added (either way). A more complete description or explanation of the augmentation procedure in general could be useful for those not in the field. Some minor improvement for minor writing weaknesses are suggested below."
      }
    ],
    "rating_avg": 6.0,
    "confidence_avg": 3.25,
    "decision": "Accept (Poster)",
    "meta_review": "Symmetry in integer linear programs (ILPs) poses challenges for graph neural networks, which have recently emerged as a promising approach for solving ILPs and yet struggle to distinguish symmetric variables. This work addresses this by proposing an orbit-based feature augmentation scheme that groups symmetric variables and assigns augmented features from a discrete uniform distribution. Numerical simulations showcase the advantages of the proposed framework both in training and predictive performance.\n\nDespite some differing opinions on the merits and execution of the ideas, many reviewers agree that the paper presents novel ideas and makes valuable contributions to the field. While the individual components of the proposed framework may not be novel, the paper meaningfully advances ILPs by combining these concepts in an innovative way.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "CKqiQosLKc",
    "title": "Sampling from Energy-based Policies using Diffusion",
    "authors": [
      "Vineet Jain",
      "Tara Akhound-Sadegh",
      "Siamak Ravanbakhsh"
    ],
    "abstract": "Energy-based policies offer a flexible framework for modeling complex, multimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the optimal policy is a Boltzmann distribution derived from the soft Q-function, but direct sampling from this distribution in continuous action spaces is computationally intractable. As a result, existing methods typically use simpler parametric distributions, like Gaussians, for policy representation — limiting their ability to capture the full complexity of multimodal action distributions. In this paper, we introduce a diffusion-based approach for sampling from energy-based policies, where the negative Q-function defines the energy function. Based on this approach, we propose an actor-critic method called Diffusion Q-Sampling (DQS) that enables more expressive policy representations, allowing stable learning in diverse environments. We show that our approach enhances exploration and captures multimodal behavior in continuous control tasks, addressing key limitations of existing methods.",
    "keywords": [
      "Reinforcement learning",
      "Diffusion models"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=CKqiQosLKc",
    "forum_url": "https://openreview.net/forum?id=CKqiQosLKc",
    "reviews": [
      {
        "rating": "6",
        "confidence": "3",
        "summary": "The authors have developed a new actor-critic algorithm called Diffusion Q-Sampling (DQS), which uses a diffusion-based model to sample from energy-based policies in actor-critic framework. The goal is to address current limitation of capturing complexity of multimodal action distributions in continuous action spaces. This novel algorithm is shown to be very effective for learning multimodal behaviors and improved sample efficiency.",
        "strengths": "1. The novel approach is able to learn multimodal actions which is valuable especial when multiple optimal trajectory exists.\n2. By explicitly sampling from the Boltzmann distribution of the Q function, DQS is shown better abilities for balancing exploration and exploitation.\n3. Through experiments on maze tasks and Deepmind control suites benchmarks, results have confirmed the advantages of DQS.",
        "weaknesses": "1. As pointed out by the authors, temperature of DQS needs to be manually tuned unlike SAC as it would be computationally very expensive to compute the likelihoods under diffusion model. \n2. No ablation study. Maybe beneficial to have some ablation studies, for example, how sensitive DQS is to different temperature values, K (number of monte carlo samples and how is it relates to computation cost)? or isolate the contribution of techniques introduced, etc."
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper proposes a novel framework for sequential decision-making using diffusion models for\nsampling from energy-based policies and a new actor-critic algorithm for training diffusion\npolicies based on that framework.This algorithm improves the high-cost issue of sampling from\ncontinuous action spaces in traditional maximum entropy reinforcement learning methods. It has\nbeen validated in the authors' custom maze navigation and DeepMind Control Suite tasks.",
        "strengths": "Proposing a novel Boltzmann policy iteration which is more efficiency and still bound to recover the optical policy",
        "weaknesses": "Lack of novelty：Simply integrating Diffusion into the traditional SAC which lacks innovation. \n\nBenchmark in a custom environment lacks persuasiveness and the test is not quantified to data."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper proposes a diffusion-based sampling method that uses a negative Q-function as an energy function for sampling, thus allowing for more expressive policy representations. Based on this approach, an actor-critic method called **Diffusion Q-Sampling (DQS)** is proposed that enables stable learning in diverse environments. Experiments show that the method enhances exploration in continuous control tasks and effectively captures multimodal behaviours, overcoming key limitations of existing methods. However, the core sampling method used in this paper is iDEM leading to a lack of innovation, and the experimental results are insufficient, the multimodal experiments may be problematic (results of the Q-score method), and the baseline algorithm is too few and too simple.",
        "strengths": "- This article proposes sampling with a diffusion strategy obeying a Boltzmann distribution to balance exploration and exploitation, focusing on a very cutting-edge area;\n- This paper does a multimodal experiment to show that DQS has some multimodality, a point that may be of interest to the RL community;\n- The writing of the paper is easy to follow.",
        "weaknesses": "- The related work is not presented carefully enough, and some are only cited. In particular, the related work under Online diffusion is particularly scarce, and each needs the author to summarise their approach, and where the flaws lie. In addition, **diffusion & online RL** related work also need you to expand, I found a recent paper accepted in NeurIPS24 is also under this setting Diffusion Actor-Critic with Entropy Regulator (Wang et al.). \n\n- You mention that the Q-score method does not have an exact distribution, but isn't Eq. (21) of the original paper a Boltzmann distribution? Is the representation in your paper not quite correct.  It's better to clarify your statement about the Q-score method and explain how it relates to Eq. (21) in the original paper. \n\n- The two proofs in 4.1 about policy improvement and policy iteration do not depend on the diffusion model, this is essentially a mathematical proof of a policy obeying a Boltzmann distribution. May I ask what is the essential difference between your proofs and the one in the Soft Actor-Critic Algorithms and Applications (Haarnoja et al.) paper? \n\n- With the experiments in 5.1, I remain sceptical about the results of QSM. I think with the addition of some tricks to fully learn the bias of Q with respect to a, the QSM can get the same results as you did (e.g., do some random sampling to update the bias of Q with respect to a to get it to school in full action space).\n\n- 5.2 There is too little BASELINE for experimental comparisons. To prove your excellent performance, add Proximal Policy Optimisation Algorithms (Schulman et al.), Diffusion Actor-Critic with Entropy Regulator (Wang et al.), Policy Representation via Diffusion Probability Model for Reinforcement Learning (Yang et al.). At least a few difficult scenarios are tested on MuJoCo environment (Humanoid, Ant) and compared with the above algorithms.\n\n### Minor note.\n- Please label all formulas in PRIMARY with the serial number, then you look at the expression for the Q function, where does the discount factor go?\n\n- Equation (4) has incorrect parentheses."
      },
      {
        "rating": "3",
        "confidence": "4",
        "summary": "The authors introduce a new algorithm for continuous RL environments, Diffusion Q-Sampling (DQS). \nDQS makes use of an existing method, iterated Denoising Energy matching (iDEM). \nThe key idea is to use iDEM to learn a score function which can be used in a reverse diffusion process to sample actions. \nThe score function is trained such that the reverse diffusion process approximately samples from a Boltzmann distribution with respect to the Q-function of the current policy. \n\nThe authors give two theoretical results, corresponding to policy improvement and policy iteration respectively, to justify their choice of training rule for the action-value function and the diffusion model. \n\nThe authors then give experimental results for their method, DQS. \nIn the first set of results, they compare DQS to SAC (soft actor-critic) and QSM (Q-score matching) in terms of the diversity of behaviors learned. They demonstrate that, in a goal reaching maze environment, DQS can successfully learn a diverse set of solutions, while SAC and QSM learn a more concentrated set of solutions. \nIn the second set of results, they compare DQS to SAC and QSM on 8 tasks from the DeepMind control suite. They demonstrate that on many of these tasks, QSM dominates the other methods.",
        "strengths": "Originality - The application of iDEM is (to this reviewer's knowledge) novel; although other methods seek to use diffusion model policies, they typically use other methods for fitting the diffusion model. The application of iDEM is novel. \n\nQuality - The empirical results given are strong. The first set of results demonstrates well that DQS can indeed learn a policy which has support on multiple different solution types for problems. The second set of results shows that DQS can learn well, and outperform baseline methods in terms of sample efficiency. \n\nClarity - In general, the authors writing is clear. The method is well-explained, and seems reproducible.  \n\nSignificance - The authors propose an effective new algorithm for continuous control. This algorithm seems particularly useful for the setting where compute is not a bottleneck, and multimodal policies are explicitly desired.",
        "weaknesses": "046 - The authors give methods of policy representations in the continuous setting. I would suggest that they mention SQL, which allows for the training of expressive policies which come from neither noise injection nor parametric family. These are trained via Stein-variational gradient descent. \n\n071 - The claim is made that \"[Diffusion models] have been extensively applied to solve sequential decision-making tasks, especially in offline settings where they can model multimodal datasets from suboptimal policies or diverse human demonstrations.\" No citations are given for these techniques - please include citations to the literature to which you are referring. \n\n191 - I would encourage the authors to say more about the role of the reverse SDE (3) in generation. Specifically, please be clear about how (3) is used to generate samples, rather than assuming this knowledge on the part of the reader. \n\n205 - Missing tildes over the x's in the expectations in Eq. (4). \n\n210 - Subscript below the S in equation (5) should be a capital K. \n\n260 - Lemma 1 is false, and its proof is invalid. Lemma 1 states that, for any action-value function, the policy which is Boltzmann with respect to that action-value function has a dominating action-value function. This statement is incorrect, and obviously so. Let $\\pi^*$ be the optimal policy, with action-value function $Q^*$. Then we know that $Q^*$ satisfies the Bellman optimality operator, $T^* Q(s,a) = r + \\gamma \\mathbb{E}[ \\max_{a'} Q(s',a') | s, a]$, where the expectation is taken over next-states $s'$ conditional on state-action pair $s,a$. If Lemma 1 were true, it would mean that the Boltzmann policy $\\pi_B$ with respect to $Q^*$ has an action-value function which dominates $Q^*$. But note that $T^* Q^*(s,a) \\geq T^{\\pi_B} Q^*(s,a)$, which can be seen by expanding definitions and using the fact that the maximum over $a'$ dominates any expectation with respect to $a'$, except if that expectation only places mass on the argmax actions. From this it also follows that this inequality is strict somewhere provided $Q^*$ is non-uniform somewhere. But since $T^* Q^* = Q^*$, it follows that $Q^* \\geq T^{\\pi_B} Q^*$. But monotonicity of the Bellman operator, it follows that, for all $n$, $Q^* \\geq [T^{\\pi_B}]^n Q^*$. Taking limits as $n \\to \\infty$, we obtain that, $Q^* \\geq Q^{\\pi_B}$, with strict inequality somewhere provided $Q^*$ is not flat. This contradicts the stated result. \n\nWe now turn to the proof given in A.1, and examine the error of reasoning. In the first two lines of  (10), the expectation of $\\log(\\pi_{new})$ is taken with respect to $\\pi_{new}$, and the expectation of $\\log(\\pi_{old})$ is taken with respect to $\\pi_{old}$. However, in the third line of (10), the expectation of both terms is taken with respect to $\\pi_{new}$. This allows the authors to express this term as a KL-divergence, a step critical to their proof. However, the term should instead be a difference of entropies, which in general is not non-negative (as the KL-divergence is).  \n\n265 - The proof of Theorem 1 is invalid. The proof relies heavily on the same argument as in Lemma 1, which is faulty. \n\nIn general, it seems like the authors fail to appreciate that results from the entropy regularised setting and the classical setting cannot be freely interchanged. The optimal policy is Boltzmann only if an entropy regularisation term is included in the Bellman backup, (7). When there is no such entropy term in the backup, the optimal policy will simply be the classical optimal policy, which in general is deterministic (or has support only on argmax actions). Similarly, the Boltzmann improvement map only gives improvement with entropy regularisation. Otherwise it can result in a strictly worse policy, as explained above.  \n\nI would suggest that the authors either cut their theoretical results entirely, or think about replacing the Bellman backup in (7) with the entropy regularized backup - however this would result in a substantial change to the algorithm, which may be too late at this stage."
      }
    ],
    "rating_avg": 3.75,
    "confidence_avg": 3.5,
    "decision": "N/A",
    "meta_review": "",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "vjbIer5R2H",
    "title": "Improved Risk Bounds with Unbounded Losses for Transductive Learning",
    "authors": [
      "Bowei Zhu",
      "Shaojie Li",
      "Huayi Tang",
      "Yong Liu"
    ],
    "abstract": "In the transductive learning setting, we are provided with a labeled training set and an unlabeled test set, with the objective of predicting the labels of the test points. This framework differs from the standard problem of fitting an unknown distribution with a training set drawn independently from this distribution. In this paper, we primarily improve the generalization bounds in transductive learning. Specifically, we develop two novel concentration inequalities for the suprema of empirical processes sampled without replacement for unbounded functions, marking the first discussion of the generalization performance of unbounded functions in the context of sampling without replacement. We further provide two valuable applications of our new inequalities: on one hand, we firstly derive fast excess risk bounds for empirical risk minimization in transductive learning under unbounded losses. On the other hand, we establish high-probability bounds on the generalization error for graph neural networks when using stochastic gradient descent which improve the current state-of-the-art results.",
    "keywords": [
      "concentration inequality",
      "generalization bounds",
      "graph neural networks",
      "transductive learning",
      "unbounded losses"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=vjbIer5R2H",
    "forum_url": "https://openreview.net/forum?id=vjbIer5R2H",
    "reviews": [
      {
        "rating": "1",
        "confidence": "5",
        "summary": "This paper derives risk bounds for transductive learning scenarios with specific applications to Graph Neural Networks (GNNs) under unbounded loss functions. The work focuses on theoretical guarantees for both sub-Gaussian and sub-exponential loss functions.",
        "strengths": "- Novel analysis of unbounded loss functions in the transductive learning setting\n- Mathematical rigour in deriving the theoretical bounds\n- Practical applications to GNN scenarios",
        "weaknesses": "**Weaknesses:**\n\n- Limited scope of unbounded loss functions:\n\n  - Analysis is restricted to sub-Gaussian and sub-exponential functions (and sub-Weibull in appendix)\n  - Other important classes of unbounded loss functions are not addressed\n\n\n- Insufficient comparison with prior work:\n\n  - The paper overlooks crucial related work, particularly [1] (Maurer & Pontil, 2021). While [1] focuses on inductive settings, their theoretical foundations appear relevant. A comparative analysis between Theorems 1 and 2 and the results in [1] is needed.\n\n- Limited Contribution:\n  - The current contribution of theoretical analysis in the GNN framework is limited. The current results are general and independent of the Graph properties. \n\n**Minor Comments:**\n\n-  In Assumption 3, \"α-Hölder\" is misspelled\n- Add the explanation of Hoeffding's reduction method to the appendix\n- Use \"Boundedness\" instead of \"Boundness\"\n- Use \"techniques\" instead of \"technologies\"\n- Line 221 \"We mainly follows the traditional technique...\" --> \"We mainly follow the traditional technique\"\n\n---\n\n**References:**\n\n- [1] Maurer, A., & Pontil, M. (2021). Concentration inequalities under sub-gaussian and sub-exponential conditions. Advances in Neural Information Processing Systems, 34, 7588-7597."
      },
      {
        "rating": "8",
        "confidence": "3",
        "summary": "The paper studies the transductive learning problem, where the learner receives a subset of labeled samples drawn without replacement from a dataset, alongside unlabeled samples for which the goal is to predict the labels. As the samples are not independent and the loss function may be unbounded, the authors develop concentration inequalities for the supremum of empirical processes sampled without replacement for unbounded functions. They use these inequalities to derive tighter risk bounds for transductive learning problems and graph neural networks.",
        "strengths": "This paper is the first to derive concentration inequalities for the supremum of empirical processes sampled without replacement for unbounded functions, presenting a novel result. Furthermore, these concentration inequalities are utilized to refine the risk bounds for transductive learning and graph neural networks found in the literature.",
        "weaknesses": "The paper lacks numerical results to support the derived risk bounds. Additionally, as it does not provide lower bounds for the risk, it remains unclear whether the resulting bounds could be further improved.\n\nThere are some typos in the paper:\n\nLine 221: we mainly \"follows\"\nLine 222: we \"introduced\"\nLine 405: w_1^{T+1}   ->   w^{T+1}"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "In this work, the authors establish generalization bounds for the transductive setting, applicable even in cases with unbounded loss. The core technical contribution is a novel tail bound for the relevant empirical process. Using these results, the authors then derive generalization bounds for graph neural networks.",
        "strengths": "The transductive setting has gained renewed attention recently, as many practical problems are better suited to transductive learning than the traditional iid statistical framework. In this context, the work is particularly relevant.\n\nThe concentration bounds presented are non-trivial, and their proof involves sophisticated mathematical tools.",
        "weaknesses": "1. The authors do not offer any motivation for addressing unbounded loss. A discussion on why this is an important and relevant problem to study would be beneficial.\n\n2. It is unclear what the significance of Theorems 3 and 4 is. Let’s consider Theorem 3 as an example. Given other terms, the upper bound can at best be\n$$ \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{m^2 u}.$$\nThe authors claim this bound is state-of-the-art for $m = o(N^{2/5})$. If we set $m = N^{1/5} = o(N^{2/5})$, then $u = N - N^{1/5} \\leq N $, and the upper bound is at least\n$$ \\geq \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{N^{2/5} \\cdot N} \\geq N^{3/5} \\log\\left( \\frac{1}{\\delta}\\right).$$\n\nGiven that this is the highest the proven upperbound can be, it is difficult to see why such a bound would be of interest as $N$ can be quite large, making the bound potentially vacuous. I may likely be missing something here, and I would be happy to engage with the authors during the discussion session to gain further clarity and adjust my score."
      },
      {
        "rating": "1",
        "confidence": "5",
        "summary": "The paper studies potentially improved risk bound for transductive learning following the conventional localized method. The main difference the author claims is that the risk bounds are for unbounded functions. However, such claim, together with the technical results for  unbounded functions, are very questionable. Furthermore, there are no detailed comparison to the current state-of-the-art risk bounds for the main results in Theorem 3 and Theorem 4.",
        "strengths": "The paper studies potentially improved risk bound for transductive learning following the conventional localized method. The main difference the author claims is that the risk bounds are for unbounded functions. However, such claim, together with the technical results for  unbounded functions, are very questionable.",
        "weaknesses": "There are several major technical drawbacks.\n\n\n\n1. While this paper claims that the risk bounds for transductive learning are for unbounded loss functions, the assumptions required for the results are essentially designed for bounded loss functions. For example, the main results Theorem 3 and Theorem 4 need the assumption that $E[f^2] \\le B E[f]$. It is well known that such assumption, $E[f^2] \\le B E[f]$, holds mainly for bounded loss functions, such as that in the classical local Rademacher complexity work (Bartlett Local Rademacher Complexities, AOS 2005). It turns out that while the paper claims risk bounds for \"unbounded loss\", but the results rely on the assumption which mainly hold for bounded loss functions.\n\n2. It is well known that Rademacher complexity or local Rademacher complexity based methods derive distribution-free risk bounds that do not need distributional assumptions. In contrast, the risk bounds in the main results Theorem 3 and Theorem 4 require sub-Gaussian and sub-exponential loss functions. It is not clear which loss functions are  sub-Gaussian or sub-exponential, and such restriction on the loss functions can significantly limit the application scope of the derived bounds.\n\n3. There are no detailed comparison to the current state-of-the-art risk bounds for the main results in Theorem 3 and Theorem 4, such as the existing transductive bounds in (Tolstikhin et al. 2014, Localized Complexities for Transductive Learning. COLT 2014). Without comparison to prior art, the significance of these results is not clear and questionable.\n\n4. The risk bounds in the main results, Theorem 3 and Theorem 4, do not convergence to 0 under the case that $m = N^{\\alpha}$ or \n$m = N^{\\alpha}$ with $\\alpha \\in (0,1/2]$, and they even diverge to $\\infty$ if $\\alpha \\in (0,1/2)$.  This is in a strong contrast to existing risk bounds for excess risk bounds where such bounds should always at least converge to $0$, and it is really misleading to claim such risk bounds are improved ones."
      }
    ],
    "rating_avg": 3.25,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "The paper studies the problem of obtaining risk bounds for transductive learning for bounded losses. The problem has interesting mathematical aspects, but there were several concerns pointed out by the reviewers. In particular, the paper does not provide adequate comparison to some related work, and the significance of the bounds is also not fully clear (it still requires a variance condition on the loss, and the regime in which it provides improved bounds is not clear). Therefore, the paper is not ready for acceptance.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "RlpJmARXqj",
    "title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization",
    "authors": [
      "Rohan Iyer"
    ],
    "abstract": "Large language models (LLMs) have revolutionized how we interact with technology, but their personalization to individual user preferences remains a significant challenge, particularly in on-device applications. Traditional methods often depend heavily on labeled datasets and can be resource-intensive. To address these issues, we present Adaptive Self-Supervised Learning Strategies (ASLS), which utilizes self-supervised learning techniques to personalize LLMs dynamically. The framework comprises a user profiling layer for collecting interaction data and a neural adaptation layer for real-time model fine-tuning. This innovative approach enables continuous learning from user feedback, allowing the model to generate responses that align closely with user-specific contexts. The adaptive mechanisms of ASLS minimize computational demands and enhance personalization efficiency. Experimental results across various user scenarios illustrate the superior performance of ASLS in boosting user engagement and satisfaction, highlighting its potential to redefine LLMs as highly responsive and context-aware systems on-device.",
    "keywords": [
      "Individual user preferences",
      "On-Device LLM"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=RlpJmARXqj",
    "forum_url": "https://openreview.net/forum?id=RlpJmARXqj",
    "reviews": [
      {
        "rating": "3",
        "confidence": "4",
        "summary": "This paper introduces an Adaptive Self-Supervised Learning Strategy (ASLS) framework for dynamic, on-device personalization of large language models (LLMs). The framework includes two layers: a user profiling layer that gathers interaction data and a neural adaptation layer that fine-tunes the model in real-time based on this data, enabling responses tailored to specific user contexts.",
        "strengths": "1.\tStudying on-device LLM personalization has significant practical application significance.\n2.\tThe author claimed that the proposed method can achieve real-time model fine-tuning.",
        "weaknesses": "1. The quality of writing in the method section needs improvement. In particular, inconsistent use of symbols and lack of explanations make the methodology difficult to understand. It is recommended that the authors revise this section and include an illustrative diagram to improve readability.\n2. Equation (2) indicates that the personalization approach in this paper still relies on label fitting, suggesting it has not eliminated the dependence on extensive labeled data as claimed in the introduction.\n3. This paper's general approach looks like customizing personalized parameters for LLMs based on user interaction data, and this core idea seems quite similar to HYDRA [1]. What are the main innovations presented in this paper, and what are its advantages?\n4. The readability of the experimental results and analysis is poor. The experimental tables are difficult to understand; for example, in Table 1, why do different methods correspond to different datasets? Regarding the analysis of experiments, the statement \"Significant enhancements observed in user engagement metrics\" does not provide a clear definition of what engagement metrics are.\n5. There is a lack of comparison with some baselines in the experiments, such as [1].\n[1] HYDRA: Model Factorization Framework for Black-Box LLM Personalization."
      },
      {
        "rating": "3",
        "confidence": "5",
        "summary": "The paper proposes a Self-Supervised Learning method to update LLMs by user interaction data for personalization on-device. The model contains two layers, a user layer to generate uer presentation by interaction data and a fine-tuning adaptation layer for dynamic modelling. The method is verified on various datesets.",
        "strengths": "1. The paper focus on decreasing computation as personalizing based on LLMs on-device.\n 2. The paper conducts experiments on multiple datasets from various domains.",
        "weaknesses": "1. The definition of on-device personalization is not clear in the paper.\n2. The metrics are not explained and defined, the reviewer cannot know what is the objective of this paper.\n3. The comparison is not fair according to Table1 which compare various methods under different datasets, and no explain of such setting is provided.\n4. The explain of layers of the method is vague, the review cannot follow due to lack of explanation and connection between section3.1-3.3"
      },
      {
        "rating": "3",
        "confidence": "3",
        "summary": "This paper proposes an adaptive self-supervised learning strategy (ASLS), which consists of a user analysis layer for collecting interaction data and a neural adaptation layer for real-time model fine-tuning.",
        "strengths": "**S1.** The problem of \"dynamically personalizing LLMs\" is interesting and valuable.  \n**S2.** The authors claim that the method does not require extensive labeled data.",
        "weaknesses": "**W1.** Regarding the introduction section, it does not provide a clear motivation and the logic is somewhat unclear. For example, the fourth paragraph is very confusing, as it includes too many disparate elements—recommendation, multi-modal object recognition, and fairness.\n\n**W2.** The connections between different parts of the method are also unclear, with issues of inconsistent notation. Additionally, there is no clear demonstration of how it better addresses the problem of dynamic updates; it still seems that updates are needed for each proposed module when new data appears.\n\n**W3.** The experiments are also confusing. For example, in Table 1, why do different methods correspond to different datasets? How does this ensure fair comparison?\n\n**W4.** It is not clear how the paper demonstrates self-supervised learning. According to equation (2), it still appears to be supervised learning.\n\n**W5.** The selection of baselines may also need improvement. For instance, the chosen recommendation baseline, PALR, is not the current state-of-the-art LLM-based recommendation method."
      },
      {
        "rating": "5",
        "confidence": "3",
        "summary": "This paper introduces Adaptive Self-Supervised Learning Strategies (ASLS) to address the challenge of personalizing large language models (LLMs) for individual users, particularly in resource-constrained on-device applications. Traditional personalization approaches often require labeled datasets and substantial computational resources. In contrast, ASLS leverages self-supervised learning for dynamic personalization, using a user profiling layer to gather interaction data and a neural adaptation layer for real-time model fine-tuning. This setup enables continuous learning from user feedback, aligning model outputs more closely with user-specific contexts while minimizing resource demands. Experimental results demonstrate ASLS’s effectiveness in enhancing user engagement and satisfaction, positioning it as a promising approach for creating responsive, context-aware on-device LLMs.",
        "strengths": "1. The paper focuses on a interesting and promising problem: On-Device LLM Personalization\n2. The structure of the paper is reasonable",
        "weaknesses": "1. It's hard to understand the whole workflow because the paper lacks a workflow figure\n2. Method is simple. Althought I approve simple but effective method, I can't get where is the \"self-supervised learning\" and can't understand why it is for \"on-device LLM\". This does not seem to be a method designed specifically for on-device LLM."
      }
    ],
    "rating_avg": 3.5,
    "confidence_avg": 3.75,
    "decision": "Reject",
    "meta_review": "This paper proposes Adaptive Self-Supervised Learning Strategies (ASLS) for the dynamic, on-device personalization of large language models (LLMs). The authors claim that ASLS leverages self-supervised learning for real-time adaptation through a user profiling layer and a neural adaptation layer. The goal is to personalize LLMs for individual users while minimizing resource demands. The paper evaluates its method across multiple datasets, reporting improvements in user engagement and satisfaction.\n\nThe primary strength of this submission lies in addressing a timely and practically significant challenge—on-device LLM personalization. This problem aligns with emerging demands for privacy-preserving and resource-efficient AI. The authors claim to use innovative self-supervised techniques for real-time fine-tuning, and they attempt to validate their approach with experiments.\n\nHowever, the weaknesses of the paper outweigh its strengths. First, there are critical issues in the clarity and rigor of the methodology. The method section is poorly written, with inconsistent notation, a lack of illustrative diagrams, and vague explanations of the proposed layers. Several reviewers noted confusion regarding how the self-supervised learning aspect was implemented and how it supports real-time on-device applications. Furthermore, the core contribution appears to be incremental and closely resembles prior work, such as the HYDRA framework, with insufficient clarity on the novel aspects of the method.\n\nThe experimental evaluation also raises significant concerns. The comparisons are not rigorous, with no clear rationale for using different datasets across baselines and no inclusion of state-of-the-art benchmarks. Additionally, the evaluation metrics, such as \"engagement score\" and \"satisfaction rate,\" are not well-defined, leaving the results open to interpretation. Reviewers also criticized the experimental presentation, noting that tables were difficult to interpret and lacked explanation.\n\nWhile the problem itself is promising, the paper falls short in presenting a robust and innovative solution. It lacks the theoretical clarity, methodological rigor, and experimental depth required for acceptance.\n\nThe primary reasons for recommending rejection are the lack of clarity and novelty in the proposed method, the insufficient and poorly explained experimental results, and the weak comparisons with relevant baselines. The authors need to address these issues comprehensively in a future submission to make their work competitive for publication.",
    "author_remarks": "",
    "decision_comment": ""
  },
  {
    "id": "v2nEL42Pvb",
    "title": "SSGNN: Simple Yet Effective Spectral Graph Neural Network",
    "authors": [
      "Ram Samarth B B",
      "Rishabh Sabharwal",
      "Sundeep Prabhakar Chepuri",
      "Punit Rathore"
    ],
    "abstract": "Spectral GNNs leverage graph spectral properties to model graph representations but have been less explored due to their computational challenges, especially compared to the more flexible and scalable spatial GNNs, which have seen broader adoption. However, spatial methods cannot fully exploit the rich information in graph spectra. Current Spectral GNNs, relying on fixed-order polynomials, use scalar-to-scalar filters applied uniformly across eigenvalues, failing to capture key spectral shifts and signal propagation dynamics. Though set-to-set filters can capture spectral complexity, methods that employ them frequently rely on Transformers, which add considerable computational burden. Our analysis indicates that applying Transformers to these filters provides minimal advantage in the spectral domain. We demonstrate that effective spectral filtering can be achieved without the need for transformers, offering a more efficient and spectrum-aware alternative. To this end, we propose a $\\textit{Simple Yet Effective Spectral Graph Neural Network}$ (SSGNN), which leverages the graph spectrum to adaptively filter using a simplified set-to-set approach that captures key spectral features. Moreover, we introduce a novel, parameter-free $\\textit{Relative Gaussian Amplifier}$ (ReGA) module, which adaptively learns spectral filtering while maintaining robustness against structural perturbations, ensuring stability. Extensive experiments on 20 real-world graph datasets, spanning both node-level and graph-level tasks along with a synthetic graph dataset, show that SSGNN matches or surpasses the performance of state-of-the-art (SOTA) spectral-based GNNs and graph transformers while using significantly fewer parameters and GFLOPs. Specifically, SSGNN achieves performance comparable to the current SOTA Graph Transformer model, Polynormer, with an average 55x reduction in parameters and 100x reduction in GFLOPs across all datasets. Our code will be made public upon acceptance.",
    "keywords": [
      "Spectral Graph Neural Networks",
      "Graph Representation Learning"
    ],
    "venue": "ICLR",
    "year": 2025,
    "pdf_url": "https://openreview.net/pdf?id=v2nEL42Pvb",
    "forum_url": "https://openreview.net/forum?id=v2nEL42Pvb",
    "reviews": [
      {
        "rating": "5",
        "confidence": "4",
        "summary": "The authors propose SSGNN, a simple yet effective spectral-based Graph Neural Network that captures rich spectral information through an adaptive set-to-set filtering approach, offering a more efficient alternative to transformer-based methods. The method introduces a parameter-free Relative Gaussian Amplifier (ReGA) module for robust spectral filtering, and demonstrates superior or comparable performance to state-of-the-art models while using significantly fewer parameters (55x reduction) and computational resources (100x reduction in GFLOPs) across 20 real-world datasets.",
        "strengths": "The paper presents an interesting transformation from scalar-to-scalar to set-to-set methodology, building upon the Spectral Former framework. By introducing a learnable parameter W to capture relationships between different frequency domain eigenvalues, the authors aim to enhance model performance through better consideration of inter-frequency domain relationships.",
        "weaknesses": "- However, the methodology lacks clarity regarding the eigenvalue computation process, which appears to rely on time-consuming SVD operations, raising concerns about computational efficiency.\n\n- Given that your proposed method emphasizes simplicity and reduced learnable parameters, it would be particularly valuable to demonstrate its effectiveness on large-scale graphs. While the reduction in parameter count is noteworthy, the real advantage of a simpler model should be its ability to scale effectively to larger, real-world graph applications. Therefore, I strongly recommend including comprehensive experiments on large-scale graph datasets to validate the method's practical utility. This would not only strengthen your contribution but also clearly differentiate your work from existing methods that may struggle with scalability.\n\n- From a comparative standpoint, although the spectral approach shows promise, the evaluation lacks comprehensive comparisons with important baseline methods, particularly SGC and SSGC. These baselines are especially relevant as they also prioritize simplicity and efficiency. To make your contribution more compelling, consider expanding the experimental section to include: (1) comparisons with these relevant baselines, (2) clear documentation of the eigenvalue computation process and its efficiency, and (3) thorough scalability analysis on large-scale graphs that would demonstrate the practical advantages of your simplified approach. This would help readers better understand the unique benefits of your method in real-world applications where scalability is crucial."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper studies the improvements of spectral GNNs, which aims to design an expressive filter based on spectral graph theory for effective graph representations. \nThe paper points out that existing SOTA spectral GNNs bring more computational burden, though they can learn the filters better. Thus, the paper proposes a novel efficient framework, namely SSGNN, which only applies simple linear transformation instead of Transformers on the spectrum. Moreover, SSGNN incorporates a parameter-free Relative Gaussian Amplifier to the decoder to enhance adaptive filter learning and maintain stability. The paper conducts extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of SSGNN.",
        "strengths": "1、\tThe motivation of this paper is significant and valuable for Spectral GNNs.\n2、\tSSGNN achieves significant efficiency in terms of computation and parameters.\n3、\tExperimental results in most cases seem promising.",
        "weaknesses": "1. The novelty of the method is somewhat limited, especially the direct application of existing eigen-correction, eigenvalue encoding, and convolution framework without any transfer challenge.\n2. The description of “set” of “set-to-set filtering” is ambiguous. Specformer applies Transformer on eigenvalue encodings, which enables filters to capture relative dependencies among the eigenvalues. Thus, the “set” of “set-to-set” in Sepcformer means the set of eigenvalues. However, SSGNN learns the spectral filter through linear transformations, so eigenvalues don’t interact with each other. Thus, what’s the meaning of “set”?\n3. The roles of the two linear transformations (namely W_{eig} and W_1W_h) respectively playing in encoder and decoder are not clear. In other words, why do authors include W_1 and W_h in the decoder instead of the encoder?\n4. The paper ignores some essential experiments.\n（1）As mentioned in Line 217, different heads allow the decoder to learn diverse spectral filtering patterns. However, there is no visualization of the diverse spectral filters learned by different heads to verify this conclusion.\n（2）There is no ablation study on the effectiveness of re-center adjustment in Equation 4 and the effectiveness of Relative Gaussian Amplifier in Equation 6.\n（3）Authors don’t verify the stability of SSGNN on OOD benchmarks such as DrugOOD[1], where many model stability studies are validated on this dataset.\n5. The symbols are ambiguous. For example, \\epsilon is used in Line 182, Line 240-241, and Line 307 simultaneously, making the paper more difficult to read. Moreover, it’s not clear on which \\epsilon the ablation experiments reported in Figure 4 are conducted.\n\n[1] Ji, Yuanfeng, et al. \"Drugood: Out-of-distribution dataset curator and benchmark for ai-aided drug discovery–a focus on affinity prediction problems with noise annotations.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 7. 2023."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes a Simple Yet Effective Spectral Graph Neural Network (SSGNN), which simplifies the set-to-set graph filter, e.g., Specformer, without performance degeneration. The key component is a parameter-free Relative Gaussian Amplifier (ReGA), which not only improves model performance but also maintains the robustness against graph perturbations. Extensive experiments on both node-level and graph-level datasets demonstrate the superiority of SSGNN in terms of effectiveness and efficiency over baselines.",
        "strengths": "1. The design of the ReGA component in SSGNN has some novelty. It not only avoids negative eigenvalues but also improves the robustness of SSGNN.\n\n2. This paper conducts extensive experiments and SSGNN also shows competitive performance. For example, in the ZINC dataset, SSGNN has a RMSE value of 0.0592.",
        "weaknesses": "1. This paper claims that SSGNN uses a **simplified set-to-set approach** to capture key spectral features. However, there is no interaction between different eigenvalues in SSGNN. Specifically, the matrix $Z_{eig} \\in \\mathbb{R}^{N \\times (d+1)}$ indicates the $(d+1)$ dimensional representation of each eigenvalue. The transformation $W_{eig}$ is applied on the channels of a single eigenvalue, i.e., $Z_{eig}W_{eig}$. Therefore, SSGNN is not a set-to-set approach.\n\n2. SSGNN involves a lot of tricks in the training process, such as eigen-correction. However, both Specformer and Polynormer do not use eigen-correction for data preprocessing. In this case, it is necessary to make a comprehensive ablation study to validate the roles of each trick. We need to verify whether the performance improvement mainly comes from ReGA rather than eigen-correction.\n\n3. It would be better if the authors could provide a comparison of the time and space overhead between different methods.\n\n4. How many parameters does SSGNN have in Table 5? Generally, we will control the number of parameters around 50K in the ZINC dataset."
      },
      {
        "rating": "5",
        "confidence": "4",
        "summary": "This paper proposes SSGNN, a simple and effective GNN model which can achieve good performance with much reduced parameters compared to transformers. With basic spectral encoder-decoder structure, it further incorporates a REGA module to strengthen the representational capabilities and the robustness against spectral perturbation. Experiments demonstrate its effectiveness and superiority on model parameters.",
        "strengths": "1-\tThe method is simple and effective on multiple graph downstream tasks.\n\n2-\tThe experiments are comprehensive and solid.\n\n3-\tThe theoretical analysis of ReGA is interesting and novel.",
        "weaknesses": "1-\tThe time and space cost in pre-computation and training stage of SSGNN seems to be a bottleneck for large graphs. Though top-k techniques can be applied, it’ll lose spectral frequencies which is essential for down-streaming tasks. When comparing parameter amounts and GFLOPS, it’ll be fair to show the training and pre-computation cost for baselines, transformers and other GNNs.\n\n2-\tIt needs experiments to demonstrate the contribution of ReGa, which is the most novel part in SSGNN. Will the removal of it greatly influence the performance?\n\n3-\tPresentation can be improved. “. .” appears in line 373 and line 398 appears incomplete sentences “* means” what?"
      }
    ],
    "rating_avg": 5.0,
    "confidence_avg": 4.0,
    "decision": "Reject",
    "meta_review": "In this submission, the authors proposed a new member of spectral GNNs with advantages in computational efficiency. However, some questions are unresolved:\n\n1) The experimental part is not solid enough. Although the authors provide more analytic experimental results, the advantage of the proposed method compared with the existing GNNs and graph-oriented Transformer models is not significant or consistent.\n\n2) As a kind of graph spectral filtering-based method, the representation power of the proposed model is not analyzed in details. For example, whether the proposed method can represent arbitrary graph filters is unknown.\n\n3) As the authors claim, the key technical contribution of this submission is the ReGA module. However, the ablation studies provided by the authors imply that the other mechanisms, such as re-centering and eigen-correction, significantly impact the model performance. \n\n4) The writing of the proposed submission is unsatisfying. There still are some obvious typos in the revised paper, and the organization of the proposed method can be improved.\n\nOverall, the motivation and analytic part of the proposed method needs to be enhanced, and the submission requires a next-round review.",
    "author_remarks": "",
    "decision_comment": ""
  }
]