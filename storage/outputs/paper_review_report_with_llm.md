# è«–æ–‡ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: 2025å¹´11æœˆ06æ—¥ 20:24

## æ¤œç´¢æ¡ä»¶

- **å­¦ä¼š**: NeurIPS 2025
- **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: æŒ‡å®šãªã—
- **æ¤œç´¢è«–æ–‡æ•°**: 5539ä»¶
- **è©•ä¾¡è«–æ–‡æ•°**: 5539ä»¶
- **ãƒ©ãƒ³ã‚¯å¯¾è±¡è«–æ–‡æ•°**: 82ä»¶

## è©•ä¾¡åŸºæº–

- **ç ”ç©¶èˆˆå‘³**: graph generation, graph algorithms, network modeling, data structures, machine learning, graph theory, computational complexity, random graphs
- **æœ€å°é–¢é€£æ€§ã‚¹ã‚³ã‚¢**: 0.2
- **æ–°è¦æ€§é‡è¦–**: ã¯ã„
- **ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–**: ã¯ã„

## ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨åŒç¾©èª

å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦LLMãŒç”Ÿæˆã—ãŸåŒç¾©èªã‚’ä½¿ç”¨ã—ã¦è«–æ–‡ã‚’æ¤œç´¢ã—ã¾ã—ãŸã€‚

### graph generation

**åŒç¾©èª**:
- graph synthesis
- graph creation
- graph modeling
- network generation
- graph construction

### graph algorithms

**åŒç¾©èª**:
- graph theory
- graph traversal
- network algorithms
- grafos
- graph data structures

### network modeling

**åŒç¾©èª**:
- network simulation
- graph modeling
- topology analysis
- network architecture
- nm

### data structures

**åŒç¾©èª**:
- data organization
- data models
- data formats
- ds
- data representation

### machine learning

**åŒç¾©èª**:
- artificial intelligence
- ai
- deep learning
- ml
- predictive analytics

### graph theory

**åŒç¾©èª**:
- graph mathematics
- network theory
- graph algorithms
- gt
- graph structures

### computational complexity

**åŒç¾©èª**:
- algorithmic complexity
- complexity theory
- np-completeness
- computational hardness
- cc

### random graphs

**åŒç¾©èª**:
- stochastic graphs
- probabilistic graphs
- random networks
- rg
- graph theory

## çµ±è¨ˆæƒ…å ±

- **å¹³å‡ç·åˆã‚¹ã‚³ã‚¢**: 0.431
- **æœ€é«˜ã‚¹ã‚³ã‚¢**: 0.518
- **æœ€ä½ã‚¹ã‚³ã‚¢**: 0.296
- **å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼è©•ä¾¡**: 4.27/10

## ãƒˆãƒƒãƒ—è«–æ–‡

### 1. Flatten Graphs as Sequences: Transformers are Scalable Graph Generators

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.702** |
| OpenReviewç·åˆ | 0.405 |
| ã€€â”œ é–¢é€£æ€§ | 0.250 |
| ã€€â”œ æ–°è¦æ€§ | 0.392 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.625 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 1.000 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.900 |
| OpenReviewè©•ä¾¡ | 4.50/10 |

**è‘—è€…**: Dexiong Chen, Markus Krimmel, Karsten Borgwardt

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: graph generation, transformers, autoregressive modeling, language models, LLMs

#### æ¦‚è¦

We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.405 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.250ã€ æ–°è¦æ€§ 0.392ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.625ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.50/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯Graph Generationã«ç‰¹åŒ–ã—ã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³ã«ç›´æ¥é–¢é€£ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹AutoGraphã¯ã€å¾“æ¥ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦åŠ¹ç‡çš„ã§ã‚ã‚Šã€ç‰¹ã«å¤§è¦æ¨¡ãªã‚°ãƒ©ãƒ•ã«å¯¾ã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å®Ÿç”¨æ€§ã‚‚é«˜ãã€ç”Ÿæˆé€Ÿåº¦ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã®å‘ä¸ŠãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã„ã¦ã‚‚æœ‰ç”¨ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=eszmES7j1F)
- [PDF](https://openreview.net/pdf?id=eszmES7j1F)

---

### 2. A Unified Framework for Fair Graph Generation: Theoretical Guarantees and Empirical Advances

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.651** |
| OpenReviewç·åˆ | 0.427 |
| ã€€â”œ é–¢é€£æ€§ | 0.213 |
| ã€€â”œ æ–°è¦æ€§ | 0.525 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.615 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.900 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.700 |
| OpenReviewè©•ä¾¡ | 4.50/10 |

**è‘—è€…**: Zichong Wang, Zhipeng Yin, Wenbin Zhang

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Fairness, Graph Generation, GNN

#### æ¦‚è¦

Graph generation models play pivotal roles in many real-world applications, from data augmentation to privacy-preserving. Despite their deployment successes, existing approaches often exhibit fairness issues, limiting their adoption in high-risk decision-making applications. Most existing fair graph generation works are based on autoregressive models that suffer from ordering sensitivity, while primarily addressing structural bias and overlooking the critical issue of feature bias. To this end, we propose FairGEM, a novel one-shot graph generation framework designed to mitigate both graph structural bias and node feature bias simultaneously. Furthermore, our theoretical analysis establishes that FairGEM delivers substantially stronger fairness guarantees than existing models while preserving generation quality. Extensive experiments across multiple real-world datasets demonstrate that FairGEM achieves superior performance in both generation quality and fairness.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.427 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.213ã€ æ–°è¦æ€§ 0.525ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.615ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.25/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯Graph Generationã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³ã«éå¸¸ã«é–¢é€£ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯FairGEMã‚’ææ¡ˆã—ã¦ãŠã‚Šã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’å…‹æœã™ã‚‹ç‚¹ã§æ–°è¦æ€§ãŒã‚ã‚Šã¾ã™ãŒã€å®Ÿç”¨æ€§ã«ã¤ã„ã¦ã¯å®Ÿé¨“çµæœãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€å…·ä½“çš„ãªå¿œç”¨ä¾‹ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚„ã‚„ä½ã‚ã®è©•ä¾¡ã¨ãªã‚Šã¾ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=T85ADT8a2y)
- [PDF](https://openreview.net/pdf?id=T85ADT8a2y)

---

### 3. Hierarchical Semantic-Augmented Navigation: Optimal Transport and Graph-Driven Reasoning for Vision-Language Navigation

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.649** |
| OpenReviewç·åˆ | 0.422 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.513 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.594 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.900 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.700 |
| OpenReviewè©•ä¾¡ | 3.60/10 |

**è‘—è€…**: Xiang Fang, Wanlong Fang, Changshuo Wang

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Hierarchical Semantic-Augmented Navigation

#### æ¦‚è¦

Vision-Language Navigation in Continuous Environments (VLN-CE) poses a formidable challenge for autonomous agents, requiring seamless integration of natural language instructions and visual observations to navigate complex 3D indoor spaces. Existing approaches often falter in long-horizon tasks due to limited scene understanding, inefficient planning, and lack of robust decision-making frameworks. We introduce the \textbf{Hierarchical Semantic-Augmented Navigation (HSAN)} framework, a groundbreaking approach that redefines VLN-CE through three synergistic innovations. First, HSAN constructs a dynamic hierarchical semantic scene graph, leveraging vision-language models to capture multi-level environmental representationsâ€”from objects to regions to zonesâ€”enabling nuanced spatial reasoning. Second, it employs an optimal transport-based topological planner, grounded in Kantorovich's duality, to select long-term goals by balancing semantic relevance and spatial accessibility with theoretical guarantees of optimality. Third, a graph-aware reinforcement learning policy ensures precise low-level control, navigating subgoals while robustly avoiding obstacles. By integrating spectral graph theory, optimal transport, and advanced multi-modal learning, HSAN addresses the shortcomings of static maps and heuristic planners prevalent in prior work. Extensive experiments on multiple challenging VLN-CE datasets demonstrate that HSAN achieves state-of-the-art performance, with significant improvements in navigation success and generalization to unseen environments.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯5ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡3.60/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.422 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.513ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.594ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.40/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€è¦–è¦šã¨è¨€èªã®çµ±åˆã«åŸºã¥ããƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ç‰¹ã«ã‚°ãƒ©ãƒ•ç”Ÿæˆã«é–¢é€£ã™ã‚‹éšå±¤çš„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚·ãƒ¼ãƒ³ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹ãŸã‚ã€é–¢é€£æ€§ãŒé«˜ã„ã§ã™ã€‚æ–°è¦æ€§ã«é–¢ã—ã¦ã¯ã€æœ€é©è¼¸é€ã¨ã‚°ãƒ©ãƒ•é§†å‹•ã®æ¨è«–ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ç‹¬å‰µçš„ã§ã‚ã‚Šã€å¾“æ¥ã®æ‰‹æ³•ã®é™ç•Œã‚’å…‹æœã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿç”¨æ€§ã¯é«˜ã„ã‚‚ã®ã®ã€ç†è«–çš„ãªæ çµ„ã¿ãŒå®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã©ã®ç¨‹åº¦é©ç”¨ã§ãã‚‹ã‹ã¯ã€ã•ã‚‰ãªã‚‹æ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=ypVW5jvguX)
- [PDF](https://openreview.net/pdf?id=ypVW5jvguX)

---

### 4. Topology-aware Graph Diffusion Model with Persistent Homology

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.628** |
| OpenReviewç·åˆ | 0.471 |
| ã€€â”œ é–¢é€£æ€§ | 0.250 |
| ã€€â”œ æ–°è¦æ€§ | 0.600 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.635 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.900 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.50/10 |

**è‘—è€…**: Joonhyuk Park, Donghyun Lee, Yujee Song, Guorong Wu, Won Hwa Kim

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Graph Generation, Diffusion, Topology, Brain Network

#### æ¦‚è¦

Generating realistic graphs faces challenges in estimating accurate distribution of graphs in an embedding space while preserving structural characteristics. However, existing graph generation methods primarily focus on approximating the joint distribution of nodes and edges, often overlooking topological properties such as connected components and loops, hindering accurate representation of global structures. To address this issue, we propose a Topology-Aware diffusion-based Graph Generation (TAGG), which aims to sample synthetic graphs that closely resemble the structural characteristics of the original graph based on persistent homology. Specifically, we suggest two core components: 1) Persistence Diagram Matching (PDM) loss which ensures high topological fidelity of generated graphs, and 2) topology-aware attention module (TAM) which induces the denoising network to capture the homological characteristics of the original graphs. Extensive experiments on conventional graph benchmarks demonstrate the effectiveness of our approach demonstrating high generation performance across various metrics, while achieving closer alignment with the distribution of topological features observed in the original graphs. Furthermore, application to real brain network data showcases its potential for complex and real graph applications.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.471 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.250ã€ æ–°è¦æ€§ 0.600ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.635ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.75/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯Graph Generationã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³ã«ç›´æ¥é–¢é€£ã—ã¦ã„ã¾ã™ã€‚æ–°è¦æ€§ã«ã¤ã„ã¦ã¯ã€ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«åŸºã¥ãã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã‚‹ç‚¹ãŒè©•ä¾¡ã§ãã¾ã™ãŒã€æ—¢å­˜ã®æ‰‹æ³•ã¨ã®æ¯”è¼ƒãŒä¸ååˆ†ãªãŸã‚ã€ã‚¹ã‚³ã‚¢ã¯ã‚„ã‚„æ§ãˆã‚ã§ã™ã€‚å®Ÿç”¨æ€§ã¯ã€å®Ÿéš›ã®è„³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€å…·ä½“çš„ãªå¿œç”¨ä¾‹ã‚„å®Ÿè£…ã®è©³ç´°ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¹ã‚³ã‚¢ã¯ä¸­ç¨‹åº¦ã«ç•™ã¾ã‚Šã¾ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=sye27MizdM)
- [PDF](https://openreview.net/pdf?id=sye27MizdM)

---

### 5. Scaling Epidemic Inference on Contact Networks: Theory and Algorithms

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.626** |
| OpenReviewç·åˆ | 0.514 |
| ã€€â”œ é–¢é€£æ€§ | 0.412 |
| ã€€â”œ æ–°è¦æ€§ | 0.546 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.617 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.25/10 |

**è‘—è€…**: Guanghui Min, Yinhan He, Chen Chen

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Computational Epidemiology, Graph Theory, Algorithm Acceleration

#### æ¦‚è¦

Computational epidemiology is crucial in understanding and controlling infectious diseases, as highlighted by large-scale outbreaks such as COVID-19. Given the inherent uncertainty and variability of disease spread, Monte Carlo (MC) simulations are widely used to predict infection peaks, estimate reproduction numbers, and evaluate the impact of non-pharmaceutical interventions (NPIs). While effective, MC-based methods require numerous runs to achieve statistically reliable estimates and variance, which suffer from high computational costs. In this work, we present a unified theoretical framework for analyzing disease spread dynamics on both directed and undirected contact networks, and propose an algorithm, **RAPID**, that significantly improves computational efficiency. Our contributions are threefold. First, we derive an asymptotic variance lower bound for MC estimates and identify the key factors influencing estimation variance. Second, we provide a theoretical analysis of the probabilistic disease spread process using linear approximations and derive the convergence conditions under non-reinfection epidemic models. Finally, we conduct extensive experiments on six real-world datasets, demonstrating our method's effectiveness and robustness in estimating the nodes' final state distribution. Specifically, our proposed method consistently produces accurate estimates aligned with results from a large number of MC simulations, while maintaining a runtime comparable to a single MC simulation. Our code and datasets are available at https://github.com/GuanghuiMin/RAPID.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.25/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.514 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.412ã€ æ–°è¦æ€§ 0.546ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.617ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.50/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯æ„ŸæŸ“ç—‡ã®æ‹¡æ•£ã‚’ã‚°ãƒ©ãƒ•ç†è«–ã«åŸºã¥ã„ã¦åˆ†æã—ã¦ãŠã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ RAPIDã®ææ¡ˆã¯æ–°è¦æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ—¢å­˜ã®MCã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€å®Œå…¨ã«ç‹¬è‡ªã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚å®Ÿç”¨æ€§ã¯é«˜ã„ã‚‚ã®ã®ã€ç‰¹å®šã®å¿œç”¨ã«é™ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚¹ã‚³ã‚¢ã¯ã‚„ã‚„ä½ã‚ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=qF5IrJfJDS)
- [PDF](https://openreview.net/pdf?id=qF5IrJfJDS)

---

### 6. A Generalized Binary Tree Mechanism for Private Approximation of All-Pair Shortest Distances

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.612** |
| OpenReviewç·åˆ | 0.479 |
| ã€€â”œ é–¢é€£æ€§ | 0.375 |
| ã€€â”œ æ–°è¦æ€§ | 0.475 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.622 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.75/10 |

**è‘—è€…**: Zongrui Zou, Chenglin Fan, Michael Dinitz, Jingcheng Liu, Jalaj Upadhyay

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: differential privacy, graph theory

#### æ¦‚è¦

We study the problem of approximating all-pair distances in a weighted undirected graph with differential privacy, introduced by Sealfon [Sea16]. Given a publicly known undirected graph, we treat the weights of edges as sensitive information, and two graphs are neighbors if their edge weights differ in one edge by at most one. We obtain efficient algorithms with significantly improved bounds on a broad class of graphs which we refer to as *recursively separable*. In particular, for any $n$-vertex $K_h$-minor-free graph, our algorithm achieve an additive error of $ \widetilde{O}(h(nW)^{1/3} ) $, where $ W $ represents the maximum edge weight; For grid graphs, the same algorithmic scheme achieve additive error of $ \widetilde{O}(n^{1/4}\sqrt{W}) $.

Our approach can be seen as a generalization of the celebrated binary tree mechanism for range queries, as releasing range queries is equivalent to computing all-pair distances on a path graph. In essence, our approach is based on generalizing the binary tree mechanism to graphs that are *recursively separable*.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.75/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.479 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.375ã€ æ–°è¦æ€§ 0.475ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.622ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.25/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ã‚°ãƒ©ãƒ•ç†è«–ã«ãŠã‘ã‚‹å…¨å¯¾æœ€çŸ­è·é›¢ã®è¿‘ä¼¼ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹ç ”ç©¶è€…ã«ã¨ã£ã¦èˆˆå‘³æ·±ã„å†…å®¹ã§ã™ã€‚æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ææ¡ˆã¯æ–°è¦æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ç‰¹å®šã®ã‚°ãƒ©ãƒ•ã‚¯ãƒ©ã‚¹ã«é™å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ä¸€èˆ¬çš„ãªå¿œç”¨ã«ã¯é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚å®Ÿç”¨æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã„ã¦ã¯ã•ã‚‰ãªã‚‹æ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=vpJDCWOnPj)
- [PDF](https://openreview.net/pdf?id=vpJDCWOnPj)

---

### 7. Improved Approximation Algorithms for Chromatic and Pseudometric-Weighted Correlation Clustering

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.606** |
| OpenReviewç·åˆ | 0.464 |
| ã€€â”œ é–¢é€£æ€§ | 0.237 |
| ã€€â”œ æ–°è¦æ€§ | 0.617 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.613 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.33/10 |

**è‘—è€…**: Chenglin Fan, Dahoon Lee, Euiwoong Lee

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Correlation Clustering, Chromatic Clustering, Approximation Algorithms, Graph Algorithms

#### æ¦‚è¦

Correlation Clustering (CC) is a foundational problem in unsupervised learning that models binary similarity relations using labeled graphs. While classical CC has been well studied, many real-world applications involve more nuanced relationshipsâ€”either multi-class categorical interactions or varying confidence levels in edge labels. To address these, two natural generalizations have been proposed: Chromatic Correlation Clustering (CCC), which assigns semantic colors to edge labels, and pseudometric-weighted CC, which allows edge weights satisfying the triangle inequality. In this paper, we develop improved approximation algorithms for both settings. Our approach leverages LP-based pivoting techniques combined with problem-specific rounding functions. For the pseudometric-weighted correlation clustering problem, we present a tight $\frac{10}{3}$-approximation algorithm, matching the best possible bound achievable within the framework of standard LP relaxation combined with specialized rounding. For the Chromatic Correlation Clustering (CCC) problem, we improve the approximation ratio from the previous best of $2.5$ to  $2.15$, and we establish a lower bound of $2.11$ within the same analytical framework, highlighting the near-optimality of our result.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯3ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.33/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.464 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.237ã€ æ–°è¦æ€§ 0.617ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.613ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.33/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã‚°ãƒ©ãƒ•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢é€£ã—ã¦ãŠã‚Šã€ç‰¹ã«ç›¸é–¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã‚‹ãŸã‚ã€Graph Generationã«èˆˆå‘³ãŒã‚ã‚‹ç ”ç©¶è€…ã«ã¨ã£ã¦é–¢é€£æ€§ãŒé«˜ã„ã§ã™ã€‚æ–°ã—ã„è¿‘ä¼¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™ºã¯æ–°è¦æ€§ã‚’æŒã£ã¦ã„ã¾ã™ãŒã€æ—¢å­˜ã®ç ”ç©¶ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€å®Œå…¨ã«é©æ–°çš„ã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚å®Ÿç”¨æ€§ã«ã¤ã„ã¦ã¯ã€ç†è«–çš„ãªçµæœãŒå®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã©ã®ç¨‹åº¦é©ç”¨ã§ãã‚‹ã‹ã¯ä¸æ˜ã§ã‚ã‚Šã€ã‚„ã‚„ä½ã‚ã®è©•ä¾¡ã¨ãªã£ã¦ã„ã¾ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=0JSolJVzjd)
- [PDF](https://openreview.net/pdf?id=0JSolJVzjd)

---

### 8. PointTruss: K-Truss for Point Cloud Registration

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.601** |
| OpenReviewç·åˆ | 0.452 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.575 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.630 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.00/10 |

**è‘—è€…**: Yue Wu, Jun Jiang, Yongzhe Yuan, Maoguo Gong, Qiguang Miao ä»–3å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Point cloud registration; compatibility graph; outlier removal; k-truss; correspondence selection

#### æ¦‚è¦

Point cloud registration is a fundamental task in 3D computer vision. Recent advances have shown that graph-based methods are effective for outlier rejection in this context. However, existing clique-based methods impose overly strict constraints and are NP-hard, making it difficult to achieve both robustness and efficiency. While the k-core reduces computational complexity, which only considers node degree and ignores higher-order topological structures such as triangles, limiting its effectiveness in complex scenarios. To overcome these limitations, we introduce the $k$-truss from graph theory into point cloud registration, leveraging triangle support as a constraint for inlier selection. We further propose a consensus voting-based low-scale sampling strategy to efficiently extract the structural skeleton of the point cloud prior to $k$-truss decomposition. Additionally, we design a spatial distribution score that balances coverage and uniformity of inliers, preventing selections that concentrate on sparse local clusters. Extensive experiments on KITTI, 3DMatch, and 3DLoMatch demonstrate that our method consistently outperforms both traditional and learning-based approaches in various indoor and outdoor scenarios, achieving state-of-the-art results.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.00/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.452 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.575ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.630ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯4.00/5ï¼ˆéå¸¸ã«é«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ã‚°ãƒ©ãƒ•ç†è«–ã«åŸºã¥ãæ–°ã—ã„æ‰‹æ³•ã‚’ç”¨ã„ã¦ç‚¹ç¾¤ç™»éŒ²ã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ãŠã‚Šã€ç‰¹ã«k-trussã‚’å°å…¥ã™ã‚‹ç‚¹ãŒé–¢é€£æ€§ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚æ–°è¦æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€æ—¢å­˜ã®æ‰‹æ³•ã«å¯¾ã™ã‚‹æ˜ç¢ºãªå„ªä½æ€§ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€å®Ÿç”¨æ€§ã¯ã‚„ã‚„ä½ã‚ã¨è©•ä¾¡ã—ã¾ã—ãŸã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=MuxBO5f8mL)
- [PDF](https://openreview.net/pdf?id=MuxBO5f8mL)

---

### 9. Doodle to Detect: A Goofy but Powerful Approach to Skeleton-based Hand Gesture Recognition

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.599** |
| OpenReviewç·åˆ | 0.448 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.558 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.635 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.50/10 |

**è‘—è€…**: Sang Hoon Han, Seonho Lee, Hyeok Nam, Jae Hyeon Park, Min Hee Cha ä»–5å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Hand Gesture Recognition, Skeleton based Action Recognition, Online Recognition, Modality Transform, Vision Transformer

#### æ¦‚è¦

Skeleton-based hand gesture recognition plays a crucial role in enabling intuitive humanâ€“computer interaction. Traditional methods have primarily relied on hand-crafted featuresâ€”such as distances between joints or positional changes across framesâ€”to alleviate issues from viewpoint variation or body proportion differences. However, these hand-crafted features often fail to capture the full spatio-temporal information in raw skeleton data, exhibit poor interpretability, and depend heavily on dataset-specific preprocessing, limiting generalization. In addition, normalization strategies in traditional methods, which rely on training data, can introduce domain gaps between training and testing environments, further hindering robustness in diverse real-world settings. To overcome these challenges, we exclude traditional hand-crafted features and propose Skeleton Kinematics Extraction Through Coordinated grapH (SKETCH), a novel framework that directly utilizes raw four-dimensional (time, x, y, and z) skeleton sequences and transforms them into intuitive visual graph representations. The proposed framework incorporates a novel learnable Dynamic Range Embedding (DRE) to preserve axis-wise motion magnitudes lost during normalization and visual graph representations, enabling richer and more discriminative feature learning. This approach produces a graph image that richly captures the raw dataâ€™s inherent information and provides interpretable visual attention cues. Furthermore, SKETCH applies independent minâ€“max normalization on fixed-length temporal windows in real time, mitigating degradation from absolute coordinate fluctuations caused by varying sensor viewpoints or differences in individual body proportions. Through these designs, our approach becomes inherently topology-agnostic, avoiding fragile dependencies on dataset- or sensor-specific skeleton definitions. By leveraging pre-trained vision backbones, SKETCH achieves efficient convergence and superior recognition accuracy. Experimental results on SHRECâ€™19 and SHRECâ€™22 benchmarks show that it outperforms state-of-the-art methods in both robustness and generalization, establishing a new paradigm for skeleton-based hand gesture recognition. The code is available at https://github.com/capableofanything/SKETCH.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.448 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.558ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.635ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.75/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€Skeleton-based hand gesture recognitionã«ãŠã‘ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ãŠã‚Šã€ç‰¹ã«Graph Generationã«é–¢é€£ã™ã‚‹è¦–è¦šçš„ã‚°ãƒ©ãƒ•è¡¨ç¾ã‚’ç”¨ã„ã¦ã„ã‚‹ãŸã‚ã€é–¢é€£æ€§ãŒé«˜ã„ã§ã™ã€‚æ–°è¦æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€å¾“æ¥ã®æ‰‹æ³•ã¨ã®æ¯”è¼ƒãŒä¸ååˆ†ã§ã‚ã‚Šã€å®Ÿç”¨æ€§ã¯å®Ÿé¨“çµæœã«åŸºã¥ãã‚‚ã®ã®ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã‘ã‚‹èª²é¡ŒãŒæ®‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=u8SXX5ITE6)
- [PDF](https://openreview.net/pdf?id=u8SXX5ITE6)

---

### 10. Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.598** |
| OpenReviewç·åˆ | 0.445 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.546 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.637 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.25/10 |

**è‘—è€…**: Siwei Zhang, Yun Xiong, Yateng Tang, Jiarong Xu, Xi Chen ä»–4å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Temporal Text-attributed Graph, Large Language Models, Data Mining

#### æ¦‚è¦

Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement.
To tackle these issues, we present $\textbf{CROSS}$, a flexible framework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is designed by decomposing the TTAG modeling process into two phases: (i) temporal semantics extraction; and (ii) semantic-structural information unification. The key idea is to advance the large language models (LLMs) to $\textit{dynamically}$ extract the temporal semantics in text space and then generate $\textit{cohesive}$ representations unifying both semantics and structures.
Specifically, we propose a Temporal Semantics Extractor in the CROSS framework, which empowers LLMs to offer the temporal semantic understanding of node's evolving contexts of textual neighborhoods, facilitating semantic dynamics.
Subsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experiments show that CROSS achieves state-of-the-art results on four public datasets and one industrial dataset, with 24.7\% absolute MRR gain on average in temporal link prediction and 3.7\% AUC gain in node classification of industrial application.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.25/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.445 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.546ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.637ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯4.00/5ï¼ˆéå¸¸ã«é«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå±æ€§ã‚’æŒã¤æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯CROSSã¯ã€æ—¢å­˜ã®TGNNã‚’æ‹¡å¼µã™ã‚‹ç‚¹ã§æ–°è¦æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ç‰¹ã«å®Ÿç”¨æ€§ã«é–¢ã—ã¦ã¯ã€å®Ÿé¨“çµæœãŒç¤ºã™é€šã‚Šã®åŠ¹æœãŒã©ã®ç¨‹åº¦ã®ç¯„å›²ã§é©ç”¨å¯èƒ½ã‹ã¯ä¸æ˜ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=9env0BdcDV)
- [PDF](https://openreview.net/pdf?id=9env0BdcDV)

---

### 11. Learning to Plan Like the Human Brain via Visuospatial Perception and Semantic-Episodic Synergistic Decision-Making

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.598** |
| OpenReviewç·åˆ | 0.444 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.563 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.616 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.60/10 |

**è‘—è€…**: Tianyuan Jia, Ziyu Li, Qing Li, Xiuxing Li, Xiang Li ä»–3å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Brain-inspired learning; Motion planning; Graph neural networks;

#### æ¦‚è¦

Motion planning in high-dimensional continuous spaces remains challenging due to complex environments and computational constraints. Although learning-based planners, especially graph neural network (GNN)-based, have significantly improved planning performance, they still struggle with inaccurate graph construction and limited structural reasoning, constraining search efficiency and path quality. The human brain exhibits efficient planning through a two-stage Perception-Decision model. First, egocentric spatial representations from visual and proprioceptive input are constructed, and then semanticâ€“episodic synergy is leveraged to support decision-making in uncertainty scenarios. Inspired by this process, we propose NeuroMP, a brain-inspired planning framework that learns to plan like the human brain. NeuroMP integrates a Perceptive Segment Selector inspired by visuospatial perception to construct safer graphs, and a Global Alignment Heuristic guide search in weakly connected graphs by modeling semantic-episodic synergistic decision-making. Experimental results demonstrate that NeuroMP significantly outperforms existing planning methods in efficiency and quality while maintaining a high success rate.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯5ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.60/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.444 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.563ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.616ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.20/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ããƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç ”ç©¶èˆˆå‘³ã§ã‚ã‚‹ã‚°ãƒ©ãƒ•ç”Ÿæˆã«é–¢é€£ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„è„³ã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã‚‹ãŸã‚ã€æ–°è¦æ€§ã‚‚é«˜ã„ã§ã™ãŒã€å®Ÿç”¨æ€§ã¯ã¾ã å®Ÿé¨“çµæœã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚„ã‚„ä½ã‚ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=1KXST1ksJ2)
- [PDF](https://openreview.net/pdf?id=1KXST1ksJ2)

---

### 12. Reinforcement learning for one-shot DAG scheduling with comparability identification and dense reward

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.595** |
| OpenReviewç·åˆ | 0.436 |
| ã€€â”œ é–¢é€£æ€§ | 0.375 |
| ã€€â”œ æ–°è¦æ€§ | 0.338 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.617 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.25/10 |

**è‘—è€…**: Xumai Qi, Dongdong Zhang, Taotao Liu, Hongcheng Wang

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: DAG scheduling, graph theory, combinatorial optimization problem, reinforcement learning

#### æ¦‚è¦

In recent years, many studies proposed to generate solutions for Directed Acyclic Graph (DAG) scheduling problem in one shot by combining reinforcement learning and list scheduling heuristic. However, these existing methods suffer from biased estimation of sampling probabilities and inefficient guidance in training, due to redundant comparisons among node priorities and the sparse reward challenge. To address these issues, we analyze of the limitations of these existing methods, and propose a novel one-shot DAG scheduling method with comparability identification and dense reward signal, based on the policy gradient framework. In our method, a comparable antichain identification mechanism is proposed to eliminate the problem of redundant nodewise priority comparison. We also propose a dense reward signal for node level decision-making optimization in training, effectively addressing the sparse reward challenge. The experimental results show that the proposed method can yield superior results of scheduling objectives compared to other learning-based DAG scheduling methods.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.25/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.436 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.375ã€ æ–°è¦æ€§ 0.338ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.617ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.50/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯DAGã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹ãŸã‚ã€é–¢é€£æ€§ã¯é«˜ã„ã§ã™ã€‚æ–°è¦æ€§ã«ã¤ã„ã¦ã¯ã€å¾“æ¥ã®æ‰‹æ³•ã®é™ç•Œã‚’å…‹æœã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã‚‹ãŸã‚ã€ä¸€å®šã®æ–°è¦æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ—¢å­˜ã®ç ”ç©¶ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚å®Œç’§ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿç”¨æ€§ã¯ã€ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ãŒå®Ÿé¨“çµæœã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã‘ã‚‹å…·ä½“çš„ãªåˆ©ç‚¹ã‚„åˆ¶ç´„ãŒä¸æ˜ãªãŸã‚ã€ã‚„ã‚„ä½ã‚ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=KDKddNgeKo)
- [PDF](https://openreview.net/pdf?id=KDKddNgeKo)

---

### 13. Differentially Private Gomory-Hu Trees

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.594** |
| OpenReviewç·åˆ | 0.436 |
| ã€€â”œ é–¢é€£æ€§ | 0.312 |
| ã€€â”œ æ–°è¦æ€§ | 0.383 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.653 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.33/10 |

**è‘—è€…**: Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan MitroviÄ‡, Yuriy Nevmyvaka ä»–2å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: differential privacy, graph algorithms, cuts

#### æ¦‚è¦

Given an undirected, weighted $n$-vertex graph $G = (V, E, w)$, a Gomory-Hu tree $T$ is a weighted tree on $V$ that preserves the Min-$s$-$t$-Cut between any pair of vertices $s, t \in V$. Finding cuts in graphs is a key primitive in problems such as bipartite matching, spectral and correlation clustering, and community detection. We design a differentially private (DP) algorithm that computes an approximate Gomory-Hu tree. Our algorithm is $\varepsilon$-DP, runs in polynomial time, and can be used to compute $s$-$t$ cuts that are $\tilde{O}(n/\varepsilon)$-additive approximations of the Min-$s$-$t$-Cuts in $G$ for all distinct $s, t \in V$ with high probability. Our error bound is essentially optimal, since [Dalirrooyfard, Mitrovic and Nevmyvaka, Neurips 2023] showed that privately outputting a single Min-$s$-$t$-Cut requires $\Omega(n)$ additive error even with $(\varepsilon, \delta)$-DP and allowing for multiplicative error. Prior to our work, the best additive error bounds for approximate all-pairs Min-$s$-$t$-Cuts were $O(n^{3/2}/\varepsilon)$ for $\varepsilon$-DP [Gupta, Roth, Ullman, TCC 2009] and $\tilde{O}(\sqrt{mn}/ \varepsilon)$  for $(\varepsilon, \delta)$-DP [Liu, Upadhyay and Zou, SODA 2024], both achieved by DP algorithms that preserve all cuts in the graph. To achieve our result, we develop an $\varepsilon$-DP algorithm for the Minimum Isolating Cuts problem with near-linear error, and introduce a novel privacy composition technique combining elements of both parallel and basic composition to handle `bounded overlap' computational branches in recursive algorithms, which maybe of independent interest.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯3ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.33/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.436 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.312ã€ æ–°è¦æ€§ 0.383ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.653ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯4.33/5ï¼ˆéå¸¸ã«é«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã‚°ãƒ©ãƒ•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢é€£ã—ã¦ãŠã‚Šã€ç‰¹ã«Gomory-Huæœ¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€Graph Generationã«èˆˆå‘³ãŒã‚ã‚‹ç ”ç©¶è€…ã«ã¨ã£ã¦é–¢é€£æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ–°è¦æ€§ã«ã¤ã„ã¦ã¯ã€å·®åˆ†ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚’è€ƒæ…®ã—ãŸæ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¦ãŠã‚Šã€ç‰¹ã«ã‚¨ãƒ©ãƒ¼å¢ƒç•Œã®æœ€é©æ€§ãŒå¼·èª¿ã•ã‚Œã¦ã„ã¾ã™ãŒã€å®Ÿç”¨æ€§ã¯ç†è«–çš„ãªçµæœã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚„ã‚„ä½ã‚ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=7rBeyE4nie)
- [PDF](https://openreview.net/pdf?id=7rBeyE4nie)

---

### 14. Low-degree evidence for computational transition of recovery rate in stochastic block model

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.586** |
| OpenReviewç·åˆ | 0.466 |
| ã€€â”œ é–¢é€£æ€§ | 0.213 |
| ã€€â”œ æ–°è¦æ€§ | 0.500 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.770 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.500 |
| OpenReviewè©•ä¾¡ | 5.00/10 |

**è‘—è€…**: Jingqiu Ding, Yiding Hua, Lucas Slot, David Steurer

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: low-degree lower bound, stochastic block model, computational complexity

#### æ¦‚è¦

We investigate implications of the (extended) low-degree conjecture (recently formalized in [moitra et al2023]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve $n^{-0.49}$ correlation with the true communities. 
Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability  [massoulie et al 2014,abbe et al 2015]. 

To our knowledge, we provide the first rigorous evidence for such sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. 
Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. 
Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models.

In contrast, prior work either (i) rules out polynomial-time algorithms with $1 - o(1)$ success probability [Hopkins 18, bandeira et al 2021] under the low-degree conjecture, or (ii) degree-$\text{poly}(k)$ polynomials for learning the stochastic block model [Luo et al 2023].

For this, we design a hypothesis test which succeeeds with constant probability under symmetric stochastic block model, and $1-o(1)$ probability under the distribution of \Erdos \Renyi random graphs.
Our proof combines low-degree lower bounds from [Hopkins 18, bandeira et al 2021]  with graph splitting and cross-validation techniques. 
In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [Hopkins et al 17].

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡5.00/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (spotlight)ã€ã§ã€ç‰¹ã«é«˜ãè©•ä¾¡ã•ã‚Œã¦ã„ã¾ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.466 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.213ã€ æ–°è¦æ€§ 0.500ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.770ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.00/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ç¢ºç‡çš„ãƒ–ãƒ­ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹è¨ˆç®—çš„é·ç§»ã«é–¢ã™ã‚‹æ–°ã—ã„è¨¼æ‹ ã‚’æä¾›ã—ã¦ãŠã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹ç ”ç©¶ã«ã¨ã£ã¦é‡è¦ã§ã™ã€‚æ–°è¦æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€å®Ÿç”¨æ€§ã¯é™ã‚‰ã‚Œã¦ãŠã‚Šã€ç†è«–çš„ãªçµæœãŒå®Ÿéš›ã®å¿œç”¨ã«ã©ã®ã‚ˆã†ã«çµã³ã¤ãã‹ã¯ä¸æ˜ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=fBNaGVMDD9)
- [PDF](https://openreview.net/pdf?id=fBNaGVMDD9)

---

### 15. Venus-MAXWELL: Efficient Learning of Protein-Mutation Stability Landscapes using Protein Language Models

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.586** |
| OpenReviewç·åˆ | 0.415 |
| ã€€â”œ é–¢é€£æ€§ | 0.213 |
| ã€€â”œ æ–°è¦æ€§ | 0.487 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.612 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.600 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.800 |
| OpenReviewè©•ä¾¡ | 4.75/10 |

**è‘—è€…**: Yuanxi Yu, Fan Jiang, Xinzhu Ma, Liang Zhang, Bozitao Zhong ä»–5å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Deep learning, Protein language model, Protein stability, Protein engineering

#### æ¦‚è¦

In-silico prediction of protein mutant stability, measured by the difference in Gibbs free energy change ($\Delta \Delta G$), is fundamental for protein engineering.
Current sequence-to-label methods typically employ two-stage pipelines: (i) encoding mutant sequences using neural networks (e.g., transformers), followed by (ii) the  $\Delta \Delta G$ regression from the latent representations.
Although these methods have demonstrated promising performance, their dependence on specialized neural network encoders significantly increases the complexity.
Additionally, the requirement to compute latent representations individually for each mutant sequence negatively impacts computational efficiency and poses the risk of overfitting.
This work proposes the Venus-MAXWELL framework, which reformulates mutation $\Delta \Delta G$ prediction as a sequence-to-landscape task.
In Venus-MAXWELL, mutations of a protein and their corresponding $\Delta \Delta G$ values are organized into a landscape matrix, allowing our framework to learn the $\Delta \Delta G$ landscape of a protein with a single forward and backward pass during training. To this end, we curated a new  $\Delta \Delta G$ benchmark dataset with strict controls on data leakage and redundancy to ensure robust evaluation.
Leveraging the zero-shot scoring capability of protein language models (PLMs), Venus-MAXWELL effectively utilizes the evolutionary patterns learned by PLMs during pre-training.
More importantly, Venus-MAXWELL is compatible with multiple protein language models.
For example, when integrated with the ESM-IF, Venus-MAXWELL achieves higher accuracy than ThermoMPNN with 10$\times$ faster in inference speed (despite having 50$\times$ more parameters than ThermoMPNN).
The training codes, model weights, and datasets are publicly available at https://github.com/ai4protein/Venus-MAXWELL.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.75/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.415 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.213ã€ æ–°è¦æ€§ 0.487ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.612ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.00/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®å¤‰ç•°å®‰å®šæ€§äºˆæ¸¬ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«ç›´æ¥é–¢é€£ã™ã‚‹å†…å®¹ã§ã¯ãªã„ãŒã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®æ§‹é€ ã‚„å¤‰ç•°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æ‰±ã£ã¦ã„ã‚‹ãŸã‚ã€ä¸€å®šã®é–¢é€£æ€§ãŒã‚ã‚‹ã€‚æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ãŠã‚Šã€å¾“æ¥ã®æ‰‹æ³•ã«æ¯”ã¹ã¦åŠ¹ç‡çš„ã§ã‚ã‚‹ãŸã‚ã€æ–°è¦æ€§ãŒé«˜ã„ã€‚å®Ÿç”¨æ€§ã‚‚é«˜ãã€å…¬é–‹ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚Šã€ä»–ã®ç ”ç©¶è€…ãŒå®¹æ˜“ã«åˆ©ç”¨ã§ãã‚‹ç‚¹ãŒè©•ä¾¡ã•ã‚Œã‚‹ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=w7hiWakSAq)
- [PDF](https://openreview.net/pdf?id=w7hiWakSAq)

---

### 16. Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.580** |
| OpenReviewç·åˆ | 0.400 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.425 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.607 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.25/10 |

**è‘—è€…**: Ling Zhan, Junjie Huang, Xiaoyao Yu, Wenyu Chen, Tao Jia

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: functional brain networks, high-order interactions, graph structure learning, pairwise network modeling

#### æ¦‚è¦

Functional brain network (FBN) modeling often relies on local pairwise interactions, whose limitation in capturing high-order dependencies is theoretically analyzed in this paper. Meanwhile, the computational burden and heuristic nature of current hypergraph modeling approaches hinder end-to-end learning of FBN structures directly from data distributions. To address this, we propose to extract high-order FBN structures under global constraints, and implement this as a Global Constraints oriented Multi-resolution (GCM) FBN structure learning framework. It incorporates 4 types of global constraint (signal synchronization, subject identity, expected edge numbers, and data labels) to enable learning FBN structures for 4 distinct levels (sample/subject/group/project) of modeling resolution. Experimental results demonstrate that GCM achieves up to a 30.6% improvement in relative accuracy and a 96.3% reduction in computational time across 5 datasets and 2 task settings, compared to 9 baselines and 10 state-of-the-art methods. Extensive experiments validate the contributions of individual components and highlight the interpretability of GCM. This work offers a novel perspective on FBN structure learning and provides a foundation for interdisciplinary applications in cognitive neuroscience. Code is publicly available on https://github.com/lzhan94swu/GCM.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.25/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.400 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.425ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.607ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.25/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯é«˜æ¬¡ã®æ©Ÿèƒ½çš„è„³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®æŠ½å‡ºã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚æ–°è¦æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€æ—¢å­˜ã®æ‰‹æ³•ã¨ã®æ¯”è¼ƒãŒå¤šã„ãŸã‚ã€å®Œå…¨ã«ç‹¬è‡ªã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚ã¾ãŸã€å®Ÿç”¨æ€§ã¯æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã‚‚ã®ã®ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã‘ã‚‹å…·ä½“çš„ãªåˆ©ç‚¹ãŒä¸æ˜ç­ãªãŸã‚ã€ã‚„ã‚„ä½ã‚ã«è©•ä¾¡ã—ã¾ã—ãŸã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=ybH0avRV4n)
- [PDF](https://openreview.net/pdf?id=ybH0avRV4n)

---

### 17. GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.550** |
| OpenReviewç·åˆ | 0.324 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.175 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.605 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.800 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 3.50/10 |

**è‘—è€…**: Jialong Zhou, Lichao Wang, Xiao Yang

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: LLM-based agent, defense, safety

#### æ¦‚è¦

The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration faces critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization. The code is available at https://github.com/JialongZhou666/GUARDIAN.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡3.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.324 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.175ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.605ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.75/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã€ã‚°ãƒ©ãƒ•ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦LLMãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®‰å…¨æ€§ã‚’ç¢ºä¿ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã¦ãŠã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚æ–°è¦æ€§ã¯ã‚ã‚‹ã‚‚ã®ã®ã€æ—¢å­˜ã®æ‰‹æ³•ã¨ã®æ¯”è¼ƒãŒä¸æ˜ç­ã§ã‚ã‚Šã€å®Ÿç”¨æ€§ã¯å®Ÿé¨“çµæœã«ä¾å­˜ã™ã‚‹ãŸã‚ã‚„ã‚„ä½ã‚ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=6j9xJ9pBjm)
- [PDF](https://openreview.net/pdf?id=6j9xJ9pBjm)

---

### 18. Searching Latent Program Spaces

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.546** |
| OpenReviewç·åˆ | 0.466 |
| ã€€â”œ é–¢é€£æ€§ | 0.213 |
| ã€€â”œ æ–°è¦æ€§ | 0.487 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.782 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.500 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.600 |
| OpenReviewè©•ä¾¡ | 4.75/10 |

**è‘—è€…**: Matthew Macfarlane, ClÃ©ment Bonnet

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Test-Time Compute, Latent Search, Deep Learning, Meta-Learning

#### æ¦‚è¦

General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions.
Although program synthesis approaches have strong generalization power, they face scaling issues due to large combinatorial spaces that quickly make them impractical and require human-generated DSLs or pre-trained priors to narrow this search space.
On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning.
In this work, we propose the Latent Program Network (LPN), a new architecture that builds in test-time search directly into neural models.
LPN learns a latent space of implicit programs---neurally mapping inputs to outputs---through which it can search using gradients at test time.
LPN combines the adaptability of symbolic approaches and the scalability of neural methods.
It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages.
On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods.
Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks.
LPN doubles its performance on out-of-distribution tasks when test-time search is switched on.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.75/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (spotlight)ã€ã§ã€ç‰¹ã«é«˜ãè©•ä¾¡ã•ã‚Œã¦ã„ã¾ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.466 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.213ã€ æ–°è¦æ€§ 0.487ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.782ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.50/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆæˆã¨æ·±å±¤å­¦ç¿’ã®èåˆã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«ç›´æ¥é–¢é€£ã™ã‚‹å†…å®¹ã§ã¯ãªã„ãŸã‚ã€é–¢é€£æ€§ã¯ä¸­ç¨‹åº¦ã¨è©•ä¾¡ã—ã¾ã—ãŸã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹Latent Program Networkã¯ã€ãƒ†ã‚¹ãƒˆæ™‚ã®æ¤œç´¢ã‚’çµ„ã¿è¾¼ã‚€ã¨ã„ã†æ–°è¦æ€§ãŒã‚ã‚Šã€ç‰¹ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æœ‰æœ›ã§ã™ã€‚å®Ÿç”¨æ€§ã«ã¤ã„ã¦ã¯ã€ææ¡ˆæ‰‹æ³•ãŒç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½å‘ä¸Šã‚’ç¤ºã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€ä¸€èˆ¬çš„ãªå¿œç”¨å¯èƒ½æ€§ã¯ã¾ã ä¸æ˜ç¢ºã§ã‚ã‚‹ãŸã‚ã€ã‚„ã‚„æ§ãˆã‚ã«è©•ä¾¡ã—ã¾ã—ãŸã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=CsXKGIqZtr)
- [PDF](https://openreview.net/pdf?id=CsXKGIqZtr)

---

### 19. Association-Focused Path Aggregation for Graph Fraud Detection

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.542** |
| OpenReviewç·åˆ | 0.454 |
| ã€€â”œ é–¢é€£æ€§ | 0.213 |
| ã€€â”œ æ–°è¦æ€§ | 0.613 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.617 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.600 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.500 |
| OpenReviewè©•ä¾¡ | 4.25/10 |

**è‘—è€…**: Tian Qiu, Wenda Li, Zunlei Feng, Jie Lei, Tao Wang ä»–3å

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: deep learning, path aggregation, graph fraud detection

#### æ¦‚è¦

Fraudulent activities have caused substantial negative social impacts and are exhibiting emerging characteristics such as intelligence and industrialization, posing challenges of high-order interactions, intricate dependencies, and the sparse yet concealed nature of fraudulent entities. Existing graph fraud detectors are limited by their narrow "receptive fields", as they focus only on the relations between an entity and its neighbors while neglecting longer-range structural associations hidden between entities. To address this issue, we propose a novel fraud detector based on Graph Path Aggregation (GPA). It operates through variable-length path sampling, semantic-associated path encoding, path interaction and aggregation, and aggregation-enhanced fraud detection. To further facilitate interpretable association analysis, we synthesize G-Internet, the first benchmark dataset in the field of internet fraud detection. Extensive experiments across datasets in multiple fraud scenarios demonstrate that the proposed GPA outperforms mainstream fraud detectors by up to +15% in Average Precision (AP). Additionally, GPA exhibits enhanced robustness to noisy labels and provides excellent interpretability by uncovering implicit fraudulent patterns across broader contexts. Code is available at https://github.com/horrible-dong/GPA.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.25/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.454 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.213ã€ æ–°è¦æ€§ 0.613ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.617ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.50/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ã‚°ãƒ©ãƒ•ã«åŸºã¥ãè©æ¬ºæ¤œå‡ºã«é–¢ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€Graph Generationã«é–¢é€£ã™ã‚‹è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ãŒã€ç›´æ¥çš„ãªé–¢é€£æ€§ã¯ã‚„ã‚„ä½ã„ã§ã™ã€‚æ–°è¦æ€§ã«ã¤ã„ã¦ã¯ã€ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ãŒæ—¢å­˜ã®æ‰‹æ³•ã«å¯¾ã—ã¦æ”¹å–„ã‚’ç¤ºã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€ç‰¹ã«é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚å®Ÿç”¨æ€§ã¯ã€å®Ÿé¨“çµæœãŒç¤ºã™ã‚ˆã†ã«ä¸€å®šã®åŠ¹æœã‚’æŒã¤ã‚‚ã®ã®ã€å®Ÿéš›ã®å¿œç”¨ã«ãŠã„ã¦ã¯ã•ã‚‰ãªã‚‹æ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=TiE8aTc3Zg)
- [PDF](https://openreview.net/pdf?id=TiE8aTc3Zg)

---

### 20. Noisy Multi-Label Learning through Co-Occurrence-Aware Diffusion

#### ã‚¹ã‚³ã‚¢

| é …ç›® | ã‚¹ã‚³ã‚¢ |
|------|--------|
| **æœ€çµ‚ã‚¹ã‚³ã‚¢** | **0.537** |
| OpenReviewç·åˆ | 0.442 |
| ã€€â”œ é–¢é€£æ€§ | 0.225 |
| ã€€â”œ æ–°è¦æ€§ | 0.558 |
| ã€€â”” ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ | 0.615 |
| AIè©•ä¾¡ï¼ˆé–¢é€£æ€§ï¼‰ | 0.600 |
| AIè©•ä¾¡ï¼ˆæ–°è¦æ€§ï¼‰ | 0.700 |
| AIè©•ä¾¡ï¼ˆå®Ÿç”¨æ€§ï¼‰ | 0.500 |
| OpenReviewè©•ä¾¡ | 4.50/10 |

**è‘—è€…**: Senyu Hou, Yuru Ren, Gaoxia Jiang, Wenjian Wang

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Multi-label classification; Noisy multi-label learning; Diffusion model; Co-occurrence-aware

#### æ¦‚è¦

Noisy labels often compel models to overfit, especially in multi-label classification tasks. Existing methods for noisy multi-label learning (NML) primarily follow a discriminative paradigm, which relies on noise transition matrix estimation or small-loss strategies to correct noisy labels. However, they remain substantial optimization difficulties compared to noisy single-label learning. In this paper, we propose a Co-Occurrence-Aware Diffusion (CAD) model, which reformulates NML from a generative perspective. We treat features as conditions and multi-labels as diffusion targets, optimizing the diffusion model for multi-label learning with theoretical guarantees. Benefiting from the diffusion model's strength in capturing multi-object semantics and structured label matrix representation, we can effectively learn the posterior mapping from features to true multi-labels. To mitigate the interference of noisy labels in the forward process, we guide generation using pseudo-clean labels reconstructed from the latent neighborhood space, replacing original point-wise estimates with neighborhood-based proxies. In the reverse process, we further incorporate label co-occurrence constraints to enhance the model's awareness of incorrect generation directions, thereby promoting robust optimization. Extensive experiments on both synthetic (Pascal-VOC, MS-COCO) and real-world (NUS-WIDE) noisy datasets demonstrate that our approach outperforms state-of-the-art methods.

#### OpenReviewè©•ä¾¡

ã“ã®è«–æ–‡ã¯4ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å—ã‘ã€ å¹³å‡4.50/10ã®è©•ä¾¡ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ æ¡æŠåˆ¤å®šã¯ã€ŒAccept (poster)ã€ã§ã™ã€‚ 

ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®è©³ç´°ã€‘ ç·åˆã‚¹ã‚³ã‚¢ï¼š0.442 ï¼ˆå†…è¨³ï¼šé–¢é€£æ€§ 0.225ã€ æ–°è¦æ€§ 0.558ã€ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ 0.615ï¼‰ 
ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®ä¿¡é ¼åº¦ã¯3.25/5ï¼ˆé«˜ã„ï¼‰ã§ã™ã€‚

#### AIè©•ä¾¡ï¼ˆå†…å®¹åˆ†æï¼‰

ã“ã®è«–æ–‡ã¯ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡ã«ãŠã‘ã‚‹ãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’æ‰±ã£ã¦ãŠã‚Šã€ç‰¹ã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¦³ç‚¹ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¦ã„ã‚‹ãŸã‚ã€Graph Generationã«é–¢é€£ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€ç›´æ¥çš„ãªé–¢é€£æ€§ã¯è–„ãã€ä¸»ã«ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«å­¦ç¿’ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚æ–°è¦æ€§ã¯é«˜ãã€å¾“æ¥ã®æ‰‹æ³•ã¨ã¯ç•°ãªã‚‹è¦–ç‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ãŒã€å®Ÿç”¨æ€§ã¯å®Ÿé¨“çµæœã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ã‚„ã‚„ä½ã‚ã«è©•ä¾¡ã—ã¾ã—ãŸã€‚

**ğŸ”— ãƒªãƒ³ã‚¯**:
- [OpenReview](https://openreview.net/forum?id=zft0zTOFkN)
- [PDF](https://openreview.net/pdf?id=zft0zTOFkN)

---
